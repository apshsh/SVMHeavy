//FIXME: make c(x) part of the testlog plot - line 9181 - suspect mean includes non-valid results!

//FIXME: consider removing -tMpy etc (they aren't needed anymore)
//remove -tMpy... -tMexe... pyorexeeval

//FIXME: option to set prior on bayesopt models
//FIXME: with models that don't have variance, let them return sigma when asked for variance. Then you can plug them in for e.g. the weight
//       function in our method and they'll just be a trivial block.
//       TO DO THIS YOU NEED TO MAKE THE var TREE RETURN SOMETHING SENSIBLE IN THESE CASES!
//FIXME: mixed integer program. gridfile has nulls for continuous variabels. nulls are filled by direct, non-nulls are called by the
//       activation function to do a grid sweep.
//FIXME: if lower fidelity falls outside of range then never do higher fidelity (assumed surrogate accuracty bounds)
//FIXME: clean up multi-threaded mess!

//FIXME:in bayesopt postprocessing need to disgard if constraints not met

//FIXME: having added d = 2 to addTrainingVector functionality thoughout (gentype y), fix non-trivial versions to make sense (d = -4 as *default*?)

/*
TO DO:
- have -mu 3 mode where training data is added to prior ML and training goes to prior ML (like the BLK, but done right)
- extend prior to non-scalar types


FIXME: TEST NEW FIDELITY CODE WITH FIDELITY FEEDBACK!
FIXME: IS FIDELITY FEEDBACK BEING CORRECTLY ACCOUNTED FOR?
FIXME: IS FIDELITY COST BEING CORECTLY CALCULATED AND USED FOR TERMINATION?
*/

/*REALLY BIG GRID TOO MUCH FOR MEMORY HOW TO DO IT?
fidelity on constraints
mixed integer programming

if x feedback changes fidelity (or other high-level "stuff")... how do we do that? How do we feed back "actual" fidelity? This needs to be done!
could you code sparsevectors as gentype vectors by coding things like :, ::, ~ etc as strings and then reinterpretting into sparse format? In gentype
streaming you might need a bit of code to treat :, ::, ~ etc on their own as strings!

:
::
:::
::::
~

could you have xreplace in gridopt?


subfact - the notation for this is wrong, so get rid of :a notation
*/


/*
    output << ( (          advanced ) ? "             { y v [ce1..cen] [cg1..cgn] [xx1..xxn] xf [xf1..xfn] xff xf3 t } \n" : "" );
ADD FIDELITY FEEDBACK OPTION HERE
ACTUALLY ALLOW BASICALLY ALL OF THE INTERACTIVE STUFF VIA THIS INPUT


varadd: maybe have varOVER-RIDE instead?
Let the user over-ride x (including fidelity).


tuneceq
tunecgt

mu...

ismoo
isceq
iscgt


bayesopt: grab inequality vectors (but ignore if y null)
          add them to the relevant models
          build the inequality constraints into the cost function as per Julien code

mlinter: make all the inequality stuff accessible


DO REVIEWS
*/

//FIXME: 1.33c is supposed to be nominally constant in gentype, which should then pass back and prevent tuneKernel from tuning this term.
//       gentype can parse the c, but it currently isn't stored. Finish this.
//FIXME: should be using fewer decimal places when reporting in globalopt, smboopt and bayesopt (ie logging to cout at human readable accuracy)

//in sparsevector.hpp:
//sv/set should work in the sparse case as well (altcontentsp)

//For some reason, when plotml evaluates [0] for blk_usrfnb with 4*(x_0-0.5)^2 it gives 10???	














//fpareto, fgrid etc need to be negated for analysis with multi-objective!
//the optimisation code seems to be working fine, but the *recording* code is negative to what it should be.



/*
ADD ALONG WITH -SA, -SAi, -SAA

    virtual int setaltMLidsKB(const Vector<int>    &nv) { KBaltMLids = nv; return 1; }
    virtual int setMLweightKB(const Vector<double> &nv) { KBMLweight = nv; return 1; }

Optimisation options:
    virtual int setminstepKB(double nv) { KBminstep = nv; return 1; }
    virtual int setmaxiterKB(int    nv) { KBmaxiter = nv; return 1; }
    virtual int setlrKB     (double nv) { KBlr = nv;      return 1; }

Consider integrating into xferml.h
*/


//
// SVMHeavyv7 abstracted interface
//
// Version: 6
// Date: 05/12/2014
// Written by: Alistair Shilton (AlShilton@gmail.com)
// Copyright: all rights reserved
//

#include "mlinter.hpp"

#include <iostream>
#include <sstream>
#include <iomanip>
#include <fstream>
#include <time.h>
#include <ctype.h>
#include <string.h>
#include <string>
#include <math.h>
#include "hillclimb.hpp"
#include "fuzzyml.hpp"
#include "errortest.hpp"
#include "addData.hpp"
#include "analyseAnomaly.hpp"
#include "balc.hpp"
#include "gridopt.hpp"
#include "directopt.hpp"
#include "nelderopt.hpp"
#include "bayesopt.hpp"
#include "globalopt.hpp"
#include "opttest.hpp"
#include "paretotest.hpp"
#include "xferml.hpp"
#include "plotml.hpp"
#include "matrix.hpp"
#include "randfun.hpp"
#ifdef ENABLE_THREADS
#include <mutex>
#endif

// uncomment to process underscores in input as spaces
//#define MANGLE_UNDERSCORES


// Argument extraction
// ===================
//
// grabnextarg: extracts the next argument (string) from the relevant input
//              source as defined by streamactive (see runsvm documentation),
//              updates any relevant counters etc as required, and places the
//              result into currentarg.  Returns 0 on success, 1 if no argument
//              can be extracted.  stdcinin is used in leui of cin
//
// grabargs: extracts the next num arguments (strings) as per grabnextarg.
//           The resulting arguments are placed in destlist (currentarg
//           contains the very last argument extracted).   If allowvector is
//           set then "vectorial" arguments of the form eg:
//
//           [ blah bah ... ]
//
//           are treated as on argument (eg "[ blah bah ... ]") rather than a
//           list of individual arguments separated by whitespace (eg
//           "[","blah","bah","...","]").  Returns 0 on success, 1 on fail.
//
// safeatoi: a "safe" version of the standard atoi function.  Reads input
//           string src into an gentype variable, substitutes argvariables into
//           this, then attempts to cast the result as an integer.  Thus you
//           could have an expression like "1+var(60,6)^4" will substitute
//           the current SVM's m value (typically 2) into var(60,6), evaluate
//           the equation, and convert the result to an integer.  On success
//           this will return the integer so obtained.  On failure it will
//           throw an exception.
//
// safeatof: a "safe" version of the standard atof function.   Operation is
//           essentially the same as safeatoi, but for floats.
//
// safeatof: a "safe" version of ascii to gentype

int grabnextarg(Stack<awarestream *> &commstack, std::string &currentarg);
int grabargs(int num, Vector<Vector<std::string> > &destlist, Stack<awarestream *> &commstack, std::string &currentarg, int allowvetor = 1);
int puttylump(const std::string &src, Stack<awarestream *> &commstack);

int     safeatoi(const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);
double  safeatof(const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);
gentype safeatog(const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);

                   int             &safeatowhatever(int &res,             const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);
                   double          &safeatowhatever(double &res,          const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);
                   gentype         &safeatowhatever(gentype &res,         const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);
template <class T> Vector<T>       &safeatowhatever(Vector<T> &res,       const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);
template <class T> Matrix<T>       &safeatowhatever(Matrix<T> &res,       const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);
template <class T> SparseVector<T> &safeatowhatever(SparseVector<T> &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables);

void stripcurlybrackets(std::string &evalargs);
void stripquotes(std::string &evalargs);

// Data File Suffix Decoder
// ========================
//
// Decodes the standard suffix string on -AA, -tf etc flags.  These have the
// form:
//
// -AA... {i j {k}} filename/number {n}
//
// where ... is the subcom, and currcommand is the vector:
//
// [ -AA...          ]
// [ {i}             ] - present if isANtype is set
// [ {j}             ] - present if isANtype is set
// [ {{k}}           ] - present if isANtype and setibase is set
// [ filename/number ] - located at position fileargpos in vector
// [ {n}             ] - present if coercefromsingle is set
//
// The following vectors are then set:
//
// int reverse           - set 1 if e present in suffix string (target-at-end format)
// int ignoreStart       - set i, or -1 if i not present
// int imax              - set j, or -1 if j not present
// int ibase             - set k, or -1 if k not present
// int uselinesvector    - set 1 if i or r present in suffix string (data taken from open file, lines in linesread)
//                       - set 2 if I or R present in suffix string (data taken from open file, lines in linesread, but linesread returned to original state after use so lines can be reused)
// int israw             - set 1 if B present in suffix string (save output in raw format)
// int startpoint        - set 1 if z present in suffix string (reset SVM for each iteration in LOO etc)
// int coercetosingle    - set 1 if u present in suffix string (read class/target but disgard)
// int coercefromsingle  - set 1 if l present in suffix string (don't read class/target but substitute given target n)
// double fromsingletar  - regression target n
// int fromsingletarget  - regression target (int) n
// std::string trainfile - filename
// Vector<int> linesread - numbers to be used if data taken from open file.

void xlateDataSourceSuffixes(int isANtype, int fileargpos, int setibase, const Vector<std::string> &currcommand, const std::string &subcom, SparseVector<SparseVector<gentype> > &argvariables, SparseVector<ofiletype> &filevariables,
     int &reverse, int &ignoreStart, int &imax, int &ibase, int &uselinesvector, int &israw, int &startpoint, int &coercetosingle, int &coercefromsingle, gentype &fromsingletarget, std::string &trainfile, Vector<int> &linesread);

// Data File Line Selection
// ========================
//
// preExtractLinesFromFile: removes unused lines from file filelines and puts
//                          them in linesread.  That is, the argument filelines
//                          keeps track of which lines in a given file have not
//                          yet been used.  This function takes unused lines out
//                          of filelines and puts them into linesread so that
//                          some other function can then extract the lines
//                          themselves from the file and do something with them.
//
// Operation depends on ignoreStart setting:
//
// ignoreStart = -1: extract maxadd (or the number of lines available if this
//                   is lower) lines from filelines, at random, and put them
//                   into the linesread vector.
// ignoreStart >= 0: extract imax (or the number of lines available, not
//                   including the first ignoreStart lines, if this is lower)
//                   lines from filelines, skipping the first ignoreStart
//                   points, and put them into the linesread vector.
//
// - The string trainfile is set to the filename contained in filelines.
// - The number of lines left in filelines after extraction is written to
//   linesleft after extraction has finished.

void preExtractLinesFromFile(ofiletype &filelines, gentype &linesleft, std::string &trainfile, int ignoreStart, int imax, Vector<int> &linesread, int randorder = 0);

// Process kernel command
//
// ktype: 0 - standard kernel
//        1 - output kernel
//        2 - kernel not associated with an ML
//        3 - RFF feature kernel
// currcommandis overwrites currcomand(0)

void processKernel(ML_Base &kernML, MercerKernel &theKern, const std::string &currcommandis, const Vector<std::string> &currcomand, int ktype, SparseVector<SparseVector<gentype> > &argvariables, int &kernnum, int firstcall,
                   SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext);


// SVM testing functions
// =====================
//
// Function arguments follow from the relevant test operation in the SVM clss,
// with the additional arguments:
//
// logfile: the name of the logfile that the function writes results to.
// svmbase: the SVM being tested.
// firstsum: if 1 then the logfile is opened for writing and firstsum is set
//           to 0.  Otherwise the logfile is appended to.
// finalresult: the result of the test is written here.
// resfilter: this function is used to calculate finalresult, given
//            argvariables and additionally var(0,0) = result from test,
//            var(0,1) = class count vector, var(0,2) = confusion matrix.
// argvariables: general argvariables
//
// Arguments in the testFileVectors form copy that of addtrainingData.

void testLOO        (                                   std::string &logfile, const ML_Mutable &svmbase,                                                                            int &firstsum, int startpoint,                                                            gentype &finalresult,                                                                                                                                                        const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int logres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num), int useThreads);
void testRecall     (                                   std::string &logfile, const ML_Mutable &svmbase,                                                                            int &firstsum,                                                                            gentype &finalresult,                                                                                                                                                        const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int logres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num), int useThreads);
void testCross      (                                   std::string &logfile, const ML_Mutable &svmbase,                                                                            int &firstsum, int numreps, int startpoint, int randcross, int numfolds,                  gentype &finalresult,                                                                                                                                                        const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int logres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num), int useThreads);
void testSparSens   (                                   std::string &logfile, const ML_Mutable &svmbase,                                                                            int &firstsum, int minbad, int maxbad, double noisemean, double noisevar, int startpoint, gentype &finalresult,                                                                                                                                                        const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int logres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num), int useThreads);
void testFileVectors(int binaryRelabel, int singleDrop, std::string &logfile, const ML_Mutable &svmbase, std::string &tfilename, int reverse, int ignoreStart, int imax,            int &firstsum, int coercetosingle, int coercefromsingle, const gentype &fromsingletarget, gentype &finalresult, int uselinesvector, Vector<int> &linesread,                                                                                                            const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int logres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num),                const SparseVector<gentype> &xtemplate);
void testTest       (                                   std::string &logfile, const ML_Mutable &svmbase, const Vector<SparseVector<gentype> > &xtest, const Vector<gentype> &ytest, int &firstsum,                                                                            gentype &finalresult,                                                                                                                                                        const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int logres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num), int startpoint, int suppressfb, int useThreads);
void testnegloglike (                                   std::string &logfile, const ML_Mutable &svmbase,                                                                            int &firstsum,                                                                            gentype &finalresult,                                                                                                                                                        const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables,                                            int (*getsetExtVar)(gentype &res, const gentype &src, int num), int useThreads);
void testmaxinfogain(                                   std::string &logfile, const ML_Mutable &svmbase,                                                                            int &firstsum,                                                                            gentype &finalresult,                                                                                                                                                        const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables,                                            int (*getsetExtVar)(gentype &res, const gentype &src, int num), int useThreads);
void testRKHSnorm   (                                   std::string &logfile, const ML_Mutable &svmbase,                                                                            int &firstsum,                                                                            gentype &finalresult,                                                                                                                                                        const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables,                                            int (*getsetExtVar)(gentype &res, const gentype &src, int num), int useThreads);



int loadDataFromMatlab(const std::string &xmatname, const std::string &ymatname, Vector<SparseVector<gentype> > &x, Vector<gentype> &dz, char targtype, int (*getsetExtVar)(gentype &res, const gentype &src, int num));

// Funciton to evaluate f(x) using external python script or exe
// =============================================================
//
// isscalar = nz if scalar
// isvector = nz if vector
// ispy     = 0 if exe
//          = 1 if python
// evalname = python/exe to be called
// sf       = scalar or function to be evaluated
// v        = vector

int pyorexeeval(int isscalar, int isvector, int ispy, const std::string &evalname, const gentype &sf, const Vector<gentype> &v, gentype &finalresult);

// Help function
// =============
//
// Prints help to stream given

void printhelp(std::ostream &output, int basic = 1, int advanced = 0);
void printhelpkernel(std::ostream &output, int basic = 1, int advanced = 0);
void printhelpvars(std::ostream &output, int basic = 1, int advanced = 0);
void printhelpgentype(std::ostream &output, int basic = 1, int advanced = 0);

void emptycommstack(Stack<awarestream *> &commstack);

void gridelmrun(gentype &res, Vector<gentype> &x, void *arg);
int gridelmMLreg(int ind, ML_Mutable *MLreg, void *arg);


// Global gentype ML_Base access function

const ML_Mutable &getMLrefconst(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext);

int getparamfull(int svmInd, int fnind, gentype &val, const gentype &xa, int ia, const gentype &xb, int ib, 
              SparseVector<int> *svmThreadOwner = nullptr,
              SparseVector<ML_Mutable *> *svmbase = nullptr,
              int *threadInd = nullptr,
              SparseVector<SVMThreadContext *> *svmContext = nullptr);
int getparamfull(int svmInd, int fnind, gentype &val, const gentype &xa, int ia, const gentype &xb, int ib, 
              SparseVector<int> *xsvmThreadOwner,
              SparseVector<ML_Mutable *> *xsvmbase,
              int *xthreadInd,
              SparseVector<SVMThreadContext *> *xsvmContext)
{
    static SparseVector<int> *svmThreadOwner = nullptr;
    static SparseVector<ML_Mutable *> *svmbase = nullptr;
    static int *threadInd = nullptr;
    static SparseVector<SVMThreadContext *> *svmContext = nullptr;

    svmThreadOwner = xsvmThreadOwner ? xsvmThreadOwner : svmThreadOwner;
    svmbase        = xsvmbase        ? xsvmbase        : svmbase;
    threadInd      = xthreadInd      ? xthreadInd      : threadInd;
    svmContext     = xsvmContext     ? xsvmContext     : svmContext;

    NiceAssert( svmThreadOwner );
    NiceAssert( svmbase );
    NiceAssert( threadInd );
    NiceAssert( svmContext );

    if ( svmInd >= 0 )
    {
        const char *dummy = "";
        return getMLrefconst(*svmThreadOwner,*svmbase,*threadInd,svmInd,*svmContext).getparam(fnind,val,xa,ia,xb,ib,dummy);
    }

    else if ( ( svmInd == -1 ) && ( ia == 0 ) )
    {
        const Vector<double> &xxx = (const Vector<double> &) xa;

        evalTestFn(fnind,val.force_double(),xxx);

        return 0;
    }

    else if ( ( svmInd == -2 ) && ( ia == 0 ) && ( ib == 0 ) )
    {
        const Vector<double> &xxx = (const Vector<double> &) xa;
        const Matrix<double> &aaa = (const Matrix<double> &) xb;

        evalTestFn(fnind,val.force_double(),xxx,&aaa);

        return 0;
    }

    else if ( ( svmInd == -3 ) && ( ia == 0 ) && ( ib == 0 ) )
    {
        const Vector<double> &xxx = (const Vector<double> &) xa;
        int MM = (int) xb;

        double mooalpha = DEFAULT_MOOALPHA; // FIXME should be adjustable

        Vector<double> rrr(MM);

        evalTestFn(fnind,xxx.size(),MM,rrr,xxx,mooalpha);

        val = rrr;

        return 0;
    }

    return 1;
}

int egetparamfull(int svmInd, int fnind, Vector<gentype> &val, const Vector<gentype> &xa, int ia, const Vector<gentype> &xb, int ib, 
              SparseVector<int> *svmThreadOwner = nullptr,
              SparseVector<ML_Mutable *> *svmbase = nullptr,
              int *threadInd = nullptr,
              SparseVector<SVMThreadContext *> *svmContext = nullptr);
int egetparamfull(int svmInd, int fnind, Vector<gentype> &val, const Vector<gentype> &xa, int ia, const Vector<gentype> &xb, int ib, 
              SparseVector<int> *xsvmThreadOwner,
              SparseVector<ML_Mutable *> *xsvmbase,
              int *xthreadInd,
              SparseVector<SVMThreadContext *> *xsvmContext)
{
    static SparseVector<int> *svmThreadOwner = nullptr;
    static SparseVector<ML_Mutable *> *svmbase = nullptr;
    static int *threadInd = nullptr;
    static SparseVector<SVMThreadContext *> *svmContext = nullptr;

    svmThreadOwner = xsvmThreadOwner ? xsvmThreadOwner : svmThreadOwner;
    svmbase        = xsvmbase        ? xsvmbase        : svmbase;
    threadInd      = xthreadInd      ? xthreadInd      : threadInd;
    svmContext     = xsvmContext     ? xsvmContext     : svmContext;

    NiceAssert( svmThreadOwner );
    NiceAssert( svmbase );
    NiceAssert( threadInd );
    NiceAssert( svmContext );

    if ( svmInd >= 0 )
    {
        return getMLrefconst(*svmThreadOwner,*svmbase,*threadInd,svmInd,*svmContext).egetparam(fnind,val,xa,ia,xb,ib);
    }

    else if ( ( svmInd == -1 ) && ( ia == 0 ) )
    {
        int k;

        NiceAssert( xa.size() == xb.size() );

        val.resize(xa.size());

        for ( k = 0 ; k < xa.size() ; ++k )
        {
            const Vector<double> &xxx = (const Vector<double> &) xa(k);

            evalTestFn(fnind,val("&",k).force_double(),xxx);
        }

        return 0;
    }

    else if ( ( svmInd == -2 ) && ( ia == 0 ) && ( ib == 0 ) )
    {
        int k;

        NiceAssert( xa.size() == xb.size() );

        val.resize(xa.size());

        for ( k = 0 ; k < xa.size() ; ++k )
        {
            const Vector<double> &xxx = (const Vector<double> &) xa(k);
            const Matrix<double> &aaa = (const Matrix<double> &) xb(k);

            evalTestFn(fnind,val("&",k).force_double(),xxx,&aaa);
        }

        return 0;
    }

    else if ( ( svmInd == -3 ) && ( ia == 0 ) && ( ib == 0 ) )
    {
        int k;

        NiceAssert( xa.size() == xb.size() );

        val.resize(xa.size());

        for ( k = 0 ; k < xa.size() ; ++k )
        {
            const Vector<double> &xxx = (const Vector<double> &) xa(k);
            int MM = (int) xb(k);

            double mooalpha = DEFAULT_MOOALPHA; // FIXME should be adjustable

            Vector<double> rrr(MM);

            evalTestFn(fnind,xxx.size(),MM,rrr.resize(MM),xxx,mooalpha);

            val.castassign(rrr);
        }

        return 0;
    }

    return 1;
}

void getparam(int svmind, int fnind, gentype &val, const gentype &xa, int ia, const gentype &xb, int ib);
void getparam(int svmind, int fnind, gentype &val, const gentype &xa, int ia, const gentype &xb, int ib)
{
    if ( getparamfull(svmind,fnind,val,xa,ia,xb,ib) )
    {
        if ( !ia && !ib )
        {
            gentype tempval("fnC(x,y,z,var(0,3))");

            gentype gsvmind(svmind);
            gentype gfnind(fnind);

            val = tempval(gsvmind,gfnind,xa,xb);
        }

        else
        {
            gentype tempval("dfnC(x,y,z,var(0,3),var(0,4),var(0,5))");

            gentype gsvmind(svmind);
            gentype gfnind(fnind);
            gentype gia(ia);
            gentype gib(ib);

            val = tempval(gsvmind,gfnind,xa,gia,xb,gib);
        }
    }

    return;
}

void egetparam(int svmind, int fnind, Vector<gentype> &val, const Vector<gentype> &xa, int ia, const Vector<gentype> &xb, int ib);
void egetparam(int svmind, int fnind, Vector<gentype> &val, const Vector<gentype> &xa, int ia, const Vector<gentype> &xb, int ib)
{
    if ( egetparamfull(svmind,fnind,val,xa,ia,xb,ib) )
    {
        if ( !( ia == 0 ) && !( ib == 0 ) )
        {
            gentype tempval("efnC(x,y,z,var(0,3))");

            gentype gsvmind(svmind);
            gentype gfnind(fnind);

            gentype xxa(xa);
            gentype xxb(xb);

            val = tempval(gsvmind,gfnind,xxa,xxb);
        }

        else
        {
            gentype tempval("edfnC(x,y,z,var(0,3),var(0,4),var(0,5))");

            gentype gsvmind(svmind);
            gentype gfnind(fnind);
            gentype gia(ia);
            gentype gib(ib);

            gentype xxa(xa);
            gentype xxb(xb);

            val = tempval(gsvmind,gfnind,xxa,gia,xxb,gib);
        }
    }

    return;
}

int runsvmint(int threadInd,
           SparseVector<SVMThreadContext *> &svmContext,
           SparseVector<ML_Mutable *> &svmbase,
           SparseVector<int> &svmThreadOwner,
           Stack<awarestream *> *xxxcommstack,
           svmvolatile SparseVector<SparseVector<gentype> > &globargvariables,
           int (*getsetExtVar)(gentype &res, const gentype &src, int num),
           SparseVector<SparseVector<int> > &returntag);

int runsvm(int threadInd,
           SparseVector<SVMThreadContext *> &svmContext,
           SparseVector<ML_Mutable *> &svmbase,
           SparseVector<int> &svmThreadOwner,
           Stack<awarestream *> *xxxcommstack,
           svmvolatile SparseVector<SparseVector<gentype> > &globargvariables,
           int (*getsetExtVar)(gentype &res, const gentype &src, int num),
           SparseVector<SparseVector<int> > &returntag)
{
    // Install gentype global function

    gentype temp;
    Vector<gentype> etemp;

    getparamfull(0,0,temp,temp,0,temp,0,&svmThreadOwner,&svmbase,&threadInd,&svmContext);
    egetparamfull(0,0,etemp,etemp,0,etemp,0,&svmThreadOwner,&svmbase,&threadInd,&svmContext);

    setGenFunc(getparam);
    seteGenFunc(egetparam);

//FIXME
    return runsvmint(threadInd,svmContext,svmbase,svmThreadOwner,xxxcommstack,globargvariables,getsetExtVar,returntag);
}









void theRoundFile(char c);
void theRoundFile(char c)
{
    (void) c;
    return;
}











// Write to both file and log

#define LOGVARPREFIX "svmh_"

template <class T>
int writeLog(const Vector<T> &resy, const Vector<Vector<T> > &resx, const std::string &resfilename); // just to file for this one
template <class T>
int writeLog(const Vector<T> &resy, const Vector<T> &rest, const Vector<Vector<T> > &resx, const std::string &resfilename); // just to file for this one
template <class T>
int writeLog(const Vector<T> &res, const std::string &resfilename, int (*getsetExtVar)(gentype &res, const gentype &src, int num));
template <>
int writeLog(const Vector<Vector<gentype> > &res, const std::string &resfilename, int (*getsetExtVar)(gentype &res, const gentype &src, int num));
template <class T>
int writeLog(const Matrix<T> &res, const std::string &resfilename, int (*getsetExtVar)(gentype &res, const gentype &src, int num));

template <class T>
int writeLog(const Vector<T> &res, const std::string &resfilename, int (*getsetExtVar)(gentype &res, const gentype &src, int num))
{
    (void) getsetExtVar;

    int i;
    int ires = 0;

    // Always write to logfiles
    {
        std::ofstream resfile;
        resfile.open(resfilename.c_str(),std::ofstream::out);

        if ( !resfile.is_open() )
        {
            STRTHROW("Unable to open result gridfile "+resfilename);
        }

        if ( res.size() )
        {
            for ( i = 0 ; i < res.size() ; ++i )
            {
                resfile << res(i) << "\n";
            }
        }

        resfile.close();
    }

    //  Also log to external variables if option available

    gentype dummya;
    gentype dummyb;

    if ( !(*getsetExtVar)(dummya,dummyb,-4) )
    {
        std::string extvarname(LOGVARPREFIX);
        extvarname += resfilename;

        if ( extvarname.length() )
        {
            for ( i = 0 ; i < (int) extvarname.length() ; ++i )
            {
                extvarname[i] = ( extvarname[i] == '.' ) ? '_' : extvarname[i];
            }
        }

        gentype resalt;

        resalt.force_vector().resize(res.size());

        if ( res.size() )
        {
            for ( i = 0 ; i < res.size() ; ++i )
            {
                resalt("&",i) = res(i);
            }
        }

        gentype extvar;

        extvar.makeString(extvarname);

        ires = (*getsetExtVar)(extvar,resalt,-2);
    }

    return ires;
}

template <>
int writeLog(const Vector<Vector<gentype> > &resx, const std::string &resfilename, int (*getsetExtVar)(gentype &res, const gentype &src, int num))
{
    (void) getsetExtVar;

    int i;
    int ires = 0;

    // Always write to logfiles
    {
        std::ofstream resfile;
        resfile.open(resfilename.c_str(),std::ofstream::out);

        if ( !resfile.is_open() )
        {
            STRTHROW("Unable to open result gridfile "+resfilename);
        }

        {
            for ( i = 0 ; i < resx.size() ; ++i )
            {
                printoneline(resfile,resx(i));
                resfile << "\n";
            }
        }

        resfile.close();
    }

    //  Also log to external variables if option available

    gentype dummya;
    gentype dummyb;

    if ( !(*getsetExtVar)(dummya,dummyb,-4) )
    {
        std::string extvarname(LOGVARPREFIX);
        extvarname += resfilename;

        if ( extvarname.length() )
        {
            for ( i = 0 ; i < (int) extvarname.length() ; ++i )
            {
                extvarname[i] = ( extvarname[i] == '.' ) ? '_' : extvarname[i];
            }
        }

        gentype resalt;

        resalt.force_vector().resize(resx.size());

        if ( resx.size() )
        {
            for ( i = 0 ; i < resx.size() ; ++i )
            {
                resalt("&",i) = resx(i);
            }
        }

        gentype extvar;

        extvar.makeString(extvarname);

        ires = (*getsetExtVar)(extvar,resalt,-2);
    }

    return ires;
}

template <class T>
int writeLog(const Vector<T> &resy, const Vector<Vector<T> > &resx, const std::string &resfilename)
{
    NiceAssert( resy.size() == resx.size() );

    int i,j;
    int ires = 0;

    // Always write to logfiles
    {
        std::ofstream resfile;
        resfile.open(resfilename.c_str(),std::ofstream::out);

        if ( !resfile.is_open() )
        {
            STRTHROW("Unable to open result gridfile "+resfilename);
        }

        if ( resy.size() )
        {
            for ( i = 0 ; i < resy.size() ; ++i )
            {
                resfile << resy(i);

                for ( j = 0 ; j < resx(i).size() ; ++j )
                {
                    resfile << "\t" << resx(i)(j);
                }

                resfile << "\n";
            }
        }

        resfile.close();
    }

    // Don't log external in this case.

    return ires;
}

#define MINVARREC 1e-12

template <class T>
int writeLog(const Vector<T> &resy, const Vector<T> &ress, const Vector<Vector<T> > &resx, const std::string &resfilename)
{
    NiceAssert( resy.size() == resx.size() );
    NiceAssert( ress.size() == resx.size() );

    int i,j;
    int ires = 0;

    // Always write to logfiles
    {
        std::ofstream resfile;
        resfile.open(resfilename.c_str(),std::ofstream::out);

        if ( !resfile.is_open() )
        {
            STRTHROW("Unable to open result gridfile "+resfilename);
        }

        if ( resy.size() )
        {
            for ( i = 0 ; i < resy.size() ; ++i )
            {
                double tempsup = ( ress(i).isCastableToRealWithoutLoss() ? ((double) ress(i)) : 1.0 );

                resfile << resy(i) << "\ts" << tempsup;

                for ( j = 0 ; j < resx(i).size() ; ++j )
                {
                    resfile << "\t" << resx(i)(j);
                }

                resfile << "\n";
            }
        }

        resfile.close();
    }

    // Don't log external in this case.

    return ires;
}

template <class T>
int writeLog(const Matrix<T> &res, const std::string &resfilename, int (*getsetExtVar)(gentype &res, const gentype &src, int num))
{
    (void) getsetExtVar;

    int ires = 0;

    {
        std::ofstream resfile;
        resfile.open(resfilename.c_str(),std::ofstream::out);

        if ( !resfile.is_open() )
        {
            STRTHROW("Unable to open result gridfile "+resfilename);
        }

        resfile << res << "\n";

        resfile.close();
    }

    gentype dummya;
    gentype dummyb;

    if ( !(*getsetExtVar)(dummya,dummyb,-4) )
    {
        std::string extvarname(LOGVARPREFIX);
        extvarname += resfilename;

        int i,j;

        if ( extvarname.length() )
        {
            for ( i = 0 ; i < (int) extvarname.length() ; ++i )
            {
                extvarname[i] = ( extvarname[i] == '.' ) ? '_' : extvarname[i];
            }
        }

        gentype resalt;

        resalt.force_matrix().resize(res.numRows(),res.numCols());

        if ( res.numRows() && res.numCols() )
        {
            for ( i = 0 ; i < res.numRows() ; ++i )
            {
                for ( j = 0 ; j < res.numCols() ; ++j )
                {
                    resalt("&",i,j) = res(i,j);
                }
            }
        }

        gentype extvar;

        extvar.makeString(extvarname);

        ires = (*getsetExtVar)(extvar,resalt,-2);
    }

    return ires;
}
























// Background training functionality

class bgTrainData;
class bgTrainData
{
public:
    ML_Mutable &svmbase;
    svmvolatile int &trainKillSwitch;
    svmvolatile int &traincontrolThreadInd;
};

void *brTrainRun(void *svmBGContext);
void *brTrainRun(void *svmBGContext)
{
    // Function called while waiting for input to train the SVM in a
    // background thread.

    ML_Mutable &svmbase = ((*((bgTrainData *) svmBGContext)).svmbase);
    svmvolatile int &trainKillSwitch = (*((bgTrainData *) svmBGContext)).trainKillSwitch;
    svmvolatile int &traincontrolThreadInd = (*((bgTrainData *) svmBGContext)).traincontrolThreadInd;

    // Train the SVM, with killswitch referred back to other thread. 

    errstream() << "\n\n*** Background training commenced in sub-thread " << traincontrolThreadInd << ".\n\n";

    int resdummy = 0;
    svmbase.train(resdummy,trainKillSwitch);

    if ( svmbase.isTrained() )
    {
        errstream() << "\n\n~~~ Background training successful in sub-thread " << traincontrolThreadInd << ".\n\n";
    }

    else
    {
        errstream() << "\n\n### Background training interrupted in sub-thread " << traincontrolThreadInd << ".\n\n";
    }

    // Signal training complete

    traincontrolThreadInd = -1;

    return nullptr;
}

// grabsvm: get SVM with relevant index and *own* it in thread etc
// regsvm: register SVM whattoreg at first available index >= svmInd
//         returns index where registered

void grabsvm(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext);
int regsvm(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext, ML_Mutable &whattoreg);
//const ML_Mutable &getMLrefconst(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext);
ML_Mutable &getMLref(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext);

void killallthreads(SparseVector<SVMThreadContext *> &svmContext, int killmain)
{
#ifdef ENABLE_THREADS
    static std::mutex eyelock;
    eyelock.lock();
#endif

    // First need to set killswitch on potentially blocking shared stack
    // inter-thread comms

    awarestream dummy;

    dummy.killfifo();

    // Now continue with thread destruction

    if ( svmContext.indsize() )
    {
        int i,threadInd;

        for ( i = svmContext.indsize()-1 ; i >= 0 ; --i )
        {
            threadInd = svmContext.ind(i);

            svmvolatile int &killswitch       = (*(svmContext.direref(i))).killswitch;
            svmvolatile int &controlThreadInd = (*(svmContext.direref(i))).controlThreadInd;

            killswitch = 1;

            while ( controlThreadInd != -1 )
            {
                svm_msleep(10);
            }

            if ( threadInd || killmain )
            {
                MEMDEL(svmContext.direref(i));
                svmContext.zero(threadInd);
            }
        }
    }

#ifdef ENABLE_THREADS
    eyelock.unlock();
#endif

    return;
}

void deleteMLs(SparseVector<ML_Mutable *> &svmbase)
{
#ifdef ENABLE_THREADS
    static std::mutex eyelock;
    eyelock.lock();
#endif

    if ( svmbase.indsize() )
    {
        int i,svmInd;

        for ( i = svmbase.indsize()-1 ; i >= 0 ; --i )
        {
            svmInd = svmbase.ind(i);

            MEMDEL(svmbase.direref(i));
            svmbase.zero(svmInd);
        }
    }

#ifdef ENABLE_THREADS
    eyelock.unlock();
#endif

    return;
}

void grabsvm(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext)
{
#ifdef ENABLE_THREADS
    static std::mutex eyelock;
    eyelock.lock();
#endif

    // If svmInd does not exist we need to make it and own it

    if ( svmbase(svmInd) == nullptr ) 
    { 
        MEMNEW(const_cast<svmvolatile ML_Mutable *&>(svmbase("&",svmInd)),ML_Mutable); 
    }

    if ( !svmThreadOwner.isindpresent(svmInd) )
    {
        const_cast<svmvolatile int &>(svmThreadOwner("&",svmInd)) = threadInd;
    }

    int threadOwner = svmThreadOwner("&",svmInd);

    // If svmInd is owned be a dead thread we need to claim it

    if ( ( threadOwner == -1 ) || !svmContext.isindpresent(threadOwner) )
    {
        const_cast<svmvolatile int &>(svmThreadOwner("&",svmInd)) = threadInd;
        threadOwner = threadInd;
    }

    // If another thread owns svmInd we need to kill that thread (or wait for it to die) and claim it

    if ( threadOwner != threadInd )
    {
        svmvolatile int &killswitch       = (*(svmContext("&",threadOwner))).killswitch;
        svmvolatile int &controlThreadInd = (*(svmContext("&",threadOwner))).controlThreadInd;

        int killmethod = (*(svmContext("&",threadInd))).killmethod;
    
        // Cannot proceed until we own this thread.

        int oldkillswitch = killswitch;

//FIXME add option to just wait
        killswitch = killmethod;

        while ( controlThreadInd != -1 )
        {
            svm_msleep(10);
        }

        killswitch = oldkillswitch;

        if ( threadOwner )
        {
            // Kill all but the main thread

            MEMDEL(svmContext("&",threadOwner));
            svmContext.zero(threadOwner);
        }

        const_cast<svmvolatile int &>(svmThreadOwner("&",svmInd)) = threadInd;
        threadOwner = threadInd;
    }

#ifdef ENABLE_THREADS
    eyelock.unlock();
#endif

    return;
}

int regsvm(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext, ML_Mutable *whattoreg)
{
#ifdef ENABLE_THREADS
    static std::mutex eyelock;
    eyelock.lock();
#endif

    // Put whattoreg at nearest available ind

    while ( svmbase(svmInd) != nullptr )
    {
        ++svmInd;
    }

    const_cast<svmvolatile ML_Mutable *&>(svmbase("&",svmInd)) = whattoreg;

    // The rest is basically the same as grabsvm

    if ( !svmThreadOwner.isindpresent(svmInd) )
    {
        const_cast<svmvolatile int &>(svmThreadOwner("&",svmInd)) = threadInd;
    }

    int threadOwner = svmThreadOwner("&",svmInd);

    // If svmInd is owned be a dead thread we need to claim it

    if ( ( threadOwner == -1 ) || !svmContext.isindpresent(threadOwner) )
    {
        const_cast<svmvolatile int &>(svmThreadOwner("&",svmInd)) = threadInd;
        threadOwner = threadInd;
    }

    // If another thread owns svmInd we need to kill that thread (or wait for it to die) and claim it

    if ( threadOwner != threadInd )
    {
        svmvolatile int &killswitch       = (*(svmContext("&",threadOwner))).killswitch;
        svmvolatile int &controlThreadInd = (*(svmContext("&",threadOwner))).controlThreadInd;

        int killmethod = (*(svmContext("&",threadInd))).killmethod;
    
        // Cannot proceed until we own this thread.

        int oldkillswitch = killswitch;

//FIXME add option to just wait
        killswitch = killmethod;

        while ( controlThreadInd != -1 )
        {
            svm_msleep(10);
        }

        killswitch = oldkillswitch;

        if ( threadOwner )
        {
            // Kill all but the main thread

            MEMDEL(svmContext("&",threadOwner));
            svmContext.zero(threadOwner);
        }

        const_cast<svmvolatile int &>(svmThreadOwner("&",svmInd)) = threadInd;
        threadOwner = threadInd;
    }

#ifdef ENABLE_THREADS
    eyelock.unlock();
#endif

    return svmInd;
}

const ML_Mutable &getMLrefconst(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext)
{
    grabsvm(svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

    const ML_Mutable &res = *((svmbase)(svmInd));

    return res;
}

ML_Mutable &getMLref(SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext)
{
    grabsvm(svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

    return *((svmbase)("&",svmInd));
}























// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================
// ============================================================================

#define FILETESTRATE 10
#define DEFAULTLOGFILE "logfile"

// callsvm: gridargvars should be a copy of argvariables with any additional
// variables only required for the call written over.

int callsvm(int threadInd,
           SparseVector<SVMThreadContext *> &svmContext,
           SparseVector<ML_Mutable *> &svmbase,
           SparseVector<int> &svmThreadOwner,
           Stack<awarestream *> *xxxgridcommstack,
           svmvolatile SparseVector<SparseVector<gentype> > &globargvariables,
           int (*getsetExtVar)(gentype &res, const gentype &src, int num),
           SparseVector<SparseVector<gentype> > &gridargvars,
           int locverblevel,
           gentype &locfinalresult,
           std::string &loclogfile);

int callsvm(int threadInd,
           SparseVector<SVMThreadContext *> &svmContext,
           SparseVector<ML_Mutable *> &svmbase,
           SparseVector<int> &svmThreadOwner,
           Stack<awarestream *> *xxxgridcommstack,
           svmvolatile SparseVector<SparseVector<gentype> > &globargvariables,
           int (*getsetExtVar)(gentype &res, const gentype &src, int num),
           SparseVector<SparseVector<gentype> > &gridargvars,
           int locverblevel,
           gentype &locfinalresult,
           std::string &loclogfile)
{
    // Get variables

    Stack<awarestream *> &gridcommstack = *xxxgridcommstack;

    int                                  &verblevel         = (*(svmContext("&",threadInd))).verblevel;
    gentype                              &finalresult       = (*(svmContext("&",threadInd))).finalresult;
  //int                                  &svmInd            = (*(svmContext("&",threadInd))).svmInd;
  //gentype                              &biasdefault       = (*(svmContext("&",threadInd))).biasdefault;
  //SparseVector<gentype>                &xtemplate         = (*(svmContext("&",threadInd))).xtemplate;
    SparseVector<SparseVector<gentype> > &argvariables      = (*(svmContext("&",threadInd))).argvariables;
  //SparseVector<ofiletype>              &filevariables     = (*(svmContext("&",threadInd))).filevariables;
    int                                  &depthin           = (*(svmContext("&",threadInd))).depthin;
  //int                                  &bgTrainOn         = (*(svmContext("&",threadInd))).bgTrainOn;
    std::string                          &logfile           = (*(svmContext("&",threadInd))).logfile;
  //int                                  &binaryRelabel     = (*(svmContext("&",threadInd))).binaryRelabel;
  //int                                  &singleDrop        = (*(svmContext("&",threadInd))).singleDrop;
  //int                                  &updateargvars     = (*(svmContext("&",threadInd))).updateargvars;
  //int                                  &killmethod        = (*(svmContext("&",threadInd))).killmethod;
  //Stack<int>                           &MLindstack        = (*(svmContext("&",threadInd))).MLindstack;
  //svmvolatile int                      &thread_killswitch = (*(svmContext("&",threadInd))).killswitch;
  //svmvolatile int                      &controlThreadInd  = (*(svmContext("&",threadInd))).controlThreadInd;

    // Save copy of variables

    int tempverblevel = verblevel;
    gentype tempfinalresult = finalresult;

    verblevel = locverblevel;
    finalresult = 0.0;

    ++depthin;

    qswap(argvariables,gridargvars);  // temporarily swap argvariables (which we want to preserve) and gridargvars (which will be used by the call)
    qswap(logfile,loclogfile);

    SparseVector<SparseVector<int> > returntag;

    int res = runsvmint(threadInd,svmContext,svmbase,svmThreadOwner,&gridcommstack,globargvariables,getsetExtVar,returntag);

    locfinalresult = finalresult;

    qswap(logfile,loclogfile);
    qswap(argvariables,gridargvars); // swap them back

    --depthin;

    verblevel = tempverblevel;
    finalresult = tempfinalresult;

    // Retain variables that have been returned

    if ( returntag.size() )
    {
        int i,j,ii,jj;

        for ( i = 0 ; i < returntag.indsize() ; ++i )
        {
            ii = returntag.ind(i);

            if ( returntag(ii).size() )
            {
                for ( j = 0 ; j < returntag(ii).indsize() ; ++j )
                {
                    jj = returntag(ii).ind(j);
                    argvariables("&",ii)("&",jj) = gridargvars(ii)(jj);
                }
            }
        }
    }

    return res;
}

#define WAIT_BASESLEEP         100000
#define WAIT_ADDSLEEP_RAND     10000

int runsvmint(int threadInd,
           SparseVector<SVMThreadContext *> &svmContext,
           SparseVector<ML_Mutable *> &svmbase,
           SparseVector<int> &svmThreadOwner,
           Stack<awarestream *> *xxxcommstack,
           svmvolatile SparseVector<SparseVector<gentype> > &globargvariables,
           int (*getsetExtVar)(gentype &res, const gentype &src, int num),
           SparseVector<SparseVector<int> > &returntag)
{
    BLK_Generic::getsetExtVar = getsetExtVar;

    int doopt = 1;

    // Get variables

    Stack<awarestream *> &commstack = *xxxcommstack;

    int                                  &verblevel         = (*(svmContext("&",threadInd))).verblevel;
    gentype                              &finalresult       = (*(svmContext("&",threadInd))).finalresult;
    int                                  &svmInd            = (*(svmContext("&",threadInd))).svmInd;
    gentype                              &biasdefault       = (*(svmContext("&",threadInd))).biasdefault;
    SparseVector<gentype>                &xtemplate         = (*(svmContext("&",threadInd))).xtemplate;
    SparseVector<SparseVector<gentype> > &argvariables      = (*(svmContext("&",threadInd))).argvariables;
    SparseVector<ofiletype>              &filevariables     = (*(svmContext("&",threadInd))).filevariables;
    int                                  &depthin           = (*(svmContext("&",threadInd))).depthin;
    int                                  &bgTrainOn         = (*(svmContext("&",threadInd))).bgTrainOn;
    std::string                          &logfile           = (*(svmContext("&",threadInd))).logfile;
    int                                  &binaryRelabel     = (*(svmContext("&",threadInd))).binaryRelabel;
    int                                  &singleDrop        = (*(svmContext("&",threadInd))).singleDrop;
    int                                  &updateargvars     = (*(svmContext("&",threadInd))).updateargvars;
    int                                  &killmethod        = (*(svmContext("&",threadInd))).killmethod;
    Stack<int>                           &MLindstack        = (*(svmContext("&",threadInd))).MLindstack;
    svmvolatile int                      &thread_killswitch = (*(svmContext("&",threadInd))).killswitch;
    svmvolatile int                      &controlThreadInd  = (*(svmContext("&",threadInd))).controlThreadInd;

#ifdef ENABLE_THREADS
    static std::mutex globargvariableslock;
#endif

    // Take control of this thread

    controlThreadInd = threadInd;

    // ...and go

    int iscomment = 0; // set to indicate processing comment /* ... */
    int stopnow   = 0; // set to cause exit
    int retval    = 0; // return value on exit
    std::string currentarg;
    int i;

    // Increment "depth" counter

    ++depthin;

    // Time recording variables

    timediffunits loggingtime     = 0;
    timediffunits multiruntime    = 0;
    timediffunits svmsetuptime    = 0;
    timediffunits svmpresetuptime = 0;
    timediffunits preloadtime     = 0;
    timediffunits loadtime        = 0;
    timediffunits postloadtime    = 0;
    timediffunits learningtime    = 0;
    timediffunits kerneltime      = 0;
    timediffunits tuningtime      = 0;
    timediffunits gridtime        = 0;
    timediffunits xfertime        = 0;
    timediffunits featuretime     = 0;
    timediffunits fuzzytime       = 0;
    timediffunits boottime        = 0;
    timediffunits optimtime       = 0;
    timediffunits performtime     = 0;
    timediffunits reporttime      = 0;

    // Command processing vectors

    Vector<Vector<std::string> > loggingopt;
    Vector<Vector<std::string> > multirunopt;
    Vector<Vector<std::string> > svmsetupopt;
    Vector<Vector<std::string> > svmpresetupopt;
    Vector<Vector<std::string> > preloadopt;
    Vector<Vector<std::string> > loadopt;
    Vector<Vector<std::string> > postloadopt;
    Vector<Vector<std::string> > learningopt;
    Vector<Vector<std::string> > kernelopt;
    Vector<Vector<std::string> > tuningopt;
    Vector<Vector<std::string> > gridopt;
    Vector<Vector<std::string> > xferopt;
    Vector<Vector<std::string> > featureopt;
    Vector<Vector<std::string> > fuzzyopt;
    Vector<Vector<std::string> > bootopt;
    Vector<Vector<std::string> > optimopt;
    Vector<Vector<std::string> > performopt;
    Vector<Vector<std::string> > reportopt;

    int argbatchsize = 0;
    int skipon = 0;

    // Loop until stop or kill encountered

    while ( !stopnow && !thread_killswitch )
    {
        skipon = 0;

        // Set variables

        if ( updateargvars && !argbatchsize )
        {
            argvariables("&",42)("&",42) = svmInd;
            updateargvars = 0;
        }

        // Train the ML in the background if background training is on, no arguments have
        // yet been processed in this block, this is a top-level function, and ML isn't
        // trained already

        if ( !argbatchsize && ( depthin == 1 ) && !(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).isTrained()) && bgTrainOn )
        {
            int newThreadInd = 10000; // default offset for non-user threads

            // Enter locked segment to prevent clashes in background thread assignment

#ifdef ENABLE_THREADS
            static std::mutex eyelock;
            eyelock.lock();
#endif

            // Find first free index to create new thread

            while ( svmContext.isindpresent(newThreadInd) )
            {
                ++newThreadInd;
            }

            // Create thread data

            MEMNEW(const_cast<svmvolatile SVMThreadContext *&>((svmContext)("&",newThreadInd)),SVMThreadContext(*svmContext(threadInd),newThreadInd));

            // Have claimed thread handle, so can leave locked segment safely

#ifdef ENABLE_THREADS
            eyelock.unlock();
#endif

            // Save some typing

            svmvolatile SVMThreadContext &locContext = (*(const_cast<svmvolatile SVMThreadContext *>((svmContext)("&",newThreadInd))));

            // Set up background training structure, lock the SVM to the new thread

            bgTrainData svmTrainOn = { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),locContext.killswitch,locContext.controlThreadInd};
            svmThreadOwner("&",svmInd) = newThreadInd;

            // Start thread running training function.  When completed controlThreadInd will
            // be set to -1.  To exit early use killswitch.

            svm_pthread_t optthread;

            if ( svm_pthread_create(&optthread,&brTrainRun,(void *) &svmTrainOn) )
            {
                errstream() << "ERROR: Thread creation failed, background training not available\n";
                retval  = 101;
                stopnow = 1;
            }

            // May now continue as per usual.  Any attempt to regrab
            // the SVM will result in appropriate action to terminate
            // the new thread.
        }

        // Get next command

        stopnow = grabnextarg(commstack,currentarg);

        ++argbatchsize;

        // Only continue if not stopped

        if ( !stopnow )
        {
            // Process input, store for later use if necessary
            //
            // Note that asynchronous options are processed here

            int preelse = 0; // visual studio has annoying limits on the number of "else if's" you can next

            if ( iscomment )
            {
                preelse = 1;

                // If in comment mode then wait for comment close marker

                if ( currentarg == "*/" )
                {
                    iscomment = 0;
                }
            }

            else if ( currentarg == "/*"         ) { preelse = 1; iscomment = 1; /* syntax highlighting ok */ }
            else if ( currentarg == "-bike"      ) { preelse = 1; ; /* -bike used to be an accelerator option, now does nothing, but still in a few scripts */ }
            else if ( currentarg == "-?"         ) { preelse = 1; printhelp(outstream()); }
            else if ( currentarg == "-??"        ) { preelse = 1; printhelp(outstream(),1,1); }
            else if ( currentarg == "-??k"       ) { preelse = 1; printhelpkernel(outstream(),1,1); }
            else if ( currentarg == "-??v"       ) { preelse = 1; printhelpvars(outstream(),1,1); }
            else if ( currentarg == "-??g"       ) { preelse = 1; printhelpgentype(outstream(),1,1); }
            else if ( currentarg == "-???"       ) { preelse = 1; errstream() << "\n\n\n\n\n\n\n\n\n\n"; }
            else if ( currentarg == "-Zinteract" ) { preelse = 1; kbquitdet("Root",nullptr,nullptr,nullptr,1); }
            else if ( currentarg == "-Zgod"      ) { preelse = 1; enablekbquitdet(); }
            else if ( currentarg == "-Zdawkins"  ) { preelse = 1; disablekbquitdet(); }
            else if ( currentarg == "-Zx"        ) { preelse = 1; skipon = 1; goto processnow; }
            else if ( currentarg == ";"          ) { preelse = 1; skipon = 1; goto processnow; }
            else if ( currentarg == "-ZZ"        ) { preelse = 1; ZZeval: stopnow = 1; skipon = 1; goto processnow; }
            else if ( currentarg == "end"        ) { preelse = 1; stopnow = 1; skipon = 1; goto processnow; }
            else if ( currentarg == "-ZZZZ"      ) { preelse = 1; ZZZZeval: errstream() << "Halt encountered: exitting now.\n"; stopnow = 1; retval = -1; }
            else if ( currentarg == "exit"       ) { preelse = 1; errstream() << "Halt encountered: exitting now.\n"; stopnow = 1; retval = -1; }
            else if ( currentarg == "-Zob"       ) { preelse = 1; bgTrainOn = 0; }
            else if ( currentarg == "-ZoB"       ) { preelse = 1; bgTrainOn = 1; killmethod = 1; }
            else if ( currentarg == "-ZoBB"      ) { preelse = 1; bgTrainOn = 1; killmethod = 0; }
            else if ( currentarg == "-Zmute"     ) { preelse = 1;   suppresserrstreamcout();  }
            else if ( currentarg == "-Zunmute"   ) { preelse = 1; unsuppresserrstreamcout();  }
            else if ( currentarg == "-ZMute"     ) { preelse = 1;   suppressoutstreamcout();  }
            else if ( currentarg == "-ZunMute"   ) { preelse = 1; unsuppressoutstreamcout();  }
            else if ( currentarg == "-ZMUTE"     ) { preelse = 1;   suppressallstreamcout(); }
            else if ( currentarg == "-ZunMUTE"   ) { preelse = 1; unsuppressallstreamcout(); }

            else if ( currentarg == "-Zs" )
            {
                preelse = 1;

                // Seed random number generator

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    int sval = -2;

                    if ( currentarg != "time" )
                    {
                        sval = safeatoi(currentarg,argvariables);
                    }

                    else
                    {
                        //sval = -2;
                        sval = (int) time(nullptr);
                    }

                    //svm_srand(sval);
                    srand(sval);
                    double dodum = 0;
                    randfill(dodum,'S',sval);
                }

                else
                {
                    errstream() << "Syntax error: -Zs requires 1 argument (seed value or rand)\n";
                    retval  = 1;
                    stopnow = 1;
                }
            }

            else if ( currentarg == "-Zrep" )
            {
                preelse = 1;

                // Reached rep command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(3,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -Zrep requires 2 argument\n";
                    retval  = 16;
                    stopnow = 1;
                }

                else
                {
                    // Strip curly braces off command

                    std::string evalarg = zifargs(0)(2);

                    stripcurlybrackets(evalarg);

                    // want to repeat b times

                    int numreps = 0;

                    try
                    {
                        numreps = safeatoi(zifargs(0)(1),argvariables);
                    }

                    catch ( ... )
                    {
                        errstream() << "Syntax error: -Zrep argument 1 must evaluate to integer\n";
                        retval  = 17;
                        stopnow = 1;
                    }

                    int iii;

                    for ( iii = 0 ; iii < numreps ; ++iii )
                    {
                        // Setup environment and run command

                        errstream() << "Running command " << evalarg << "\n";

                        std::stringstream *tmpcommand;
                        MEMNEW(tmpcommand,std::stringstream);
                        *tmpcommand << "-fV 1000 " << iii << " -Zx " << evalarg;
                        awarestream *gridbox;
                        MEMNEW(gridbox,awarestream(tmpcommand,1));
                        Stack<awarestream *> *gridcommstack;
                        MEMNEW(gridcommstack,Stack<awarestream *>);
                        gridcommstack->push(gridbox);

                        runsvmint(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,returntag);

                        errstream() << "Finished running command " << evalarg << "\n";
                    }
                }
            }

            else if ( ( currentarg == "-ZZif" ) || ( currentarg == "endif" ) )
            {
                preelse = 1;

                // Reached if command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(2,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -ZZif requires 1 argument\n";
                    retval  = 18;
                    stopnow = 1;
                }

                else
                {
                    // Exit if test evaluated true

                    int iftestres = 0;

                    try
                    {
                        iftestres = safeatoi(zifargs(0)(1),argvariables);
                    }

                    catch ( ... )
                    {
                        errstream() << "Syntax error: -ZZif argument 1 must evaluate to logical (integer)\n";
                        retval  = 19;
                        stopnow = 1;
                    }

                    if ( iftestres )
                    {
                        goto ZZeval;
                    }
                }
            }

            else if ( ( currentarg == "-ZZZZif" ) || ( currentarg == "exitif" ) )
            {
                preelse = 1;

                // Reached if command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(2,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -ZZif requires 1 argument\n";
                    retval  = 20;
                    stopnow = 1;
                }

                else
                {
                    // Exit if test evaluated true

                    int iftestres = 0;

                    try
                    {
                        iftestres = safeatoi(zifargs(0)(1),argvariables);
                    }

                    catch ( ... )
                    {
                        errstream() << "Syntax error: -ZZif argument 1 must evaluate to logical (integer)\n";
                        retval  = 21;
                        stopnow = 1;
                    }

                    if ( iftestres )
                    {
                        goto ZZZZeval;
                    }
                }
            }

            else if ( currentarg == "-Zifelse" )
            {
                preelse = 1;

                // Reached if command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(4,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -Zif requires 2 argument\n";
                    retval  = 22;
                    stopnow = 1;
                }

                else
                {
                    // Strip curly braces off command

                    std::string evalargtrue = zifargs(0)(2);

                    stripcurlybrackets(evalargtrue);

                    // Strip curly braces off command

                    std::string evalargfalse = zifargs(0)(2);

                    stripcurlybrackets(evalargfalse);

                    // Only proceed if test evaluated true

                    int iftestres = 0;

                    try
                    {
                        iftestres = safeatoi(zifargs(0)(1),argvariables);
                    }

                    catch ( ... )
                    {
                        errstream() << "Syntax error: -Zif argument 1 must evaluate to logical (integer)\n";
                        retval  = 23;
                        stopnow = 1;
                    }

                    if ( !stopnow && iftestres )
                    {
                        // Setup environment and run command

                        errstream() << "Running command " << evalargtrue << "\n";

                        std::stringstream *tmpcommand;
                        MEMNEW(tmpcommand,std::stringstream(evalargtrue));
                        awarestream *gridbox;
                        MEMNEW(gridbox,awarestream(tmpcommand,1));
                        Stack<awarestream *> *gridcommstack;
                        MEMNEW(gridcommstack,Stack<awarestream *>);
                        gridcommstack->push(gridbox);

                        runsvmint(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,returntag);

                        errstream() << "Finished running command " << evalargtrue << "\n";
                    }

                    if ( !stopnow && !iftestres )
                    {
                        // Setup environment and run command

                        errstream() << "Running command " << evalargfalse << "\n";

                        std::stringstream *tmpcommand;
                        MEMNEW(tmpcommand,std::stringstream(evalargfalse));
                        awarestream *gridbox;
                        MEMNEW(gridbox,awarestream(tmpcommand,1));
                        Stack<awarestream *> *gridcommstack;
                        MEMNEW(gridcommstack,Stack<awarestream *>);
                        gridcommstack->push(gridbox);

                        runsvmint(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,returntag);

                        errstream() << "Finished running command " << evalargfalse << "\n";
                    }
                }
            }

            else if ( currentarg == "-Zwhile" )
            {
                preelse = 1;

                // Reached while command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(3,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -Zwhile requires 2 argument\n";
                    retval  = 24;
                    stopnow = 1;
                }

                else
                {
                    // Strip curly braces off command

                    std::string evalarg = zifargs(0)(2);

                    stripcurlybrackets(evalarg);

                    // Only proceed if test evaluated true

                  try
                  {
                    errstream() << "While loop... " << evalarg << "...";

                    while ( !stopnow && safeatoi(zifargs(0)(1),argvariables) )
                    {
                        errstream() << "@";

                        // Setup environment and run command

                        std::stringstream *tmpcommand;
                        MEMNEW(tmpcommand,std::stringstream(evalarg));
                        awarestream *gridbox;
                        MEMNEW(gridbox,awarestream(tmpcommand,1));
                        Stack<awarestream *> *gridcommstack;
                        MEMNEW(gridcommstack,Stack<awarestream *>);
                        gridcommstack->push(gridbox);

                        runsvmint(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,returntag);
                    }

                    errstream() << "Finished while loop... " << evalarg << "\n";
                  }

                  catch ( ... )
                  {
                    errstream() << "Syntax error: -Zwhile argument 1 must evaluate to logical (integer)\n";
                    retval  = 25;
                    stopnow = 1;
                  }
                }
            }

            else if ( currentarg == "-Zwait" )
            {
                preelse = 1;

                // Reached wait command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(2,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -Zwait requires 1 argument\n";
                    retval  = 24;
                    stopnow = 1;
                }

                else
                {
                  try
                  {
                    errstream() << "Wait loop...";

                    while ( !stopnow && !safeatoi(zifargs(0)(1),argvariables) )
                    {
                        //svm_usleep(WAIT_BASESLEEP+(svm_rand()%WAIT_ADDSLEEP_RAND));
                        svm_usleep(WAIT_BASESLEEP+(rand()%WAIT_ADDSLEEP_RAND));
                    }

                    errstream() << "Finished wait loop...\n";
                  }

                  catch ( ... )
                  {
                    errstream() << "Syntax error: -Zwait argument 1 must evaluate to logical (integer)\n";
                    retval  = 25;
                    stopnow = 1;
                  }
                }
            }

            else if ( currentarg == "-Zusleep" )
            {
                preelse = 1;

                // Reached usleep command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(2,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -Zusleep requires 1 argument\n";
                    retval  = 24;
                    stopnow = 1;
                }

                else
                {
                    svm_usleep((int) safeatof(zifargs(0)(1),argvariables));
                }
            }

            else if ( currentarg == "-Zmsleep" )
            {
                preelse = 1;

                // Reached msleep command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(2,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -Zmsleep requires 1 argument\n";
                    retval  = 24;
                    stopnow = 1;
                }

                else
                {
                    svm_usleep((int) std::round(1000*safeatof(zifargs(0)(1),argvariables)));
                }
            }

            else if ( currentarg == "-Zsleep" )
            {
                preelse = 1;

                // Reached sleep command

                Vector<Vector<std::string> > zifargs;

                if ( grabargs(2,zifargs,commstack,currentarg) )
                {
                    errstream() << "Syntax error: -Zsleep requires 1 argument\n";
                    retval  = 24;
                    stopnow = 1;
                }

                else
                {
                    svm_usleep((int) std::round(1000000*safeatof(zifargs(0)(1),argvariables)));
                }
            }

            else if ( ( currentarg == "-Zw" ) || ( currentarg == "-Zaw" ) )
            {
                preelse = 1;

                // Switch input from file, no feedback

                int popflag = 0;

                if ( currentarg == "-Zaw" )
                {
                    // Pop element of command stack

                    popflag = 1;
                }

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    if ( popflag )
                    {
                        // Pop element of command stack

                        MEMDEL(commstack.accessTop());
                        commstack.pop();
                    }

                    std::ifstream *filein;

                    MEMNEW(filein,std::ifstream);

                    if ( filein )
                    {
                        filein->open(currentarg.c_str(),std::ifstream::in);

                        if ( filein->is_open() )
                        {
                            awarestream *fileinbox;

                            MEMNEW(fileinbox,awarestream(filein,1));

                            commstack.push(fileinbox);
                        }

                        else
                        {
                            errstream() << "Unable to open file -Zw " << currentarg << "\n";
                            retval  = 201;
                            stopnow = 1;
                        }
                    }

                    else
                    {
                        errstream() << "Unable to open file -Zw " << currentarg << "\n";
                        retval  = 202;
                        stopnow = 1;
                    }
                }

                else
                {
                    errstream() << "Syntax error: -Zw and -Zaw require 1 argument\n";
                    retval  = 26;
                    stopnow = 1;
                }
            }

            else if ( ( currentarg == "-Zk" ) || ( currentarg == "-Zak" ) )
            {
                preelse = 1;

                // Switch to cin without feedback

                if ( currentarg == "-Zak" )
                {
                    // Pop element of command stack

                    MEMDEL(commstack.accessTop());
                    commstack.pop();
                }

                awarestream *stdcinbox;

                MEMNEW(stdcinbox,awarestream(&instream(),0));

                commstack.push(stdcinbox);
            }

            else if ( ( currentarg == "-Zc" ) || ( currentarg == "-Zac" ) )
            {
                preelse = 1;

                // Switch to shared stream without feedback

                int popflag = 0;

                if ( currentarg == "-Zac" )
                {
                    // Pop element of command stack

                    popflag = 1;
                }

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    if ( popflag )
                    {
                        // Pop element of command stack

                        MEMDEL(commstack.accessTop());
                        commstack.pop();
                    }

                    awarestream *newsharestream;

                    MEMNEW(newsharestream,awarestream("&","&",safeatoi(currentarg,argvariables)));

                    commstack.push(newsharestream);
                }

                else
                {
                    errstream() << "Syntax error: -Zc, -Zu, -Zuf, -Zau and -Zauf require 1 argument\n";
                    retval  = 29;
                    stopnow = 1;
                }
            }

            else if ( ( currentarg == "-Zwf" ) || ( currentarg == "-Zawf" ) )
            {
                preelse = 1;

                // Switch input from file with feedback to file

                int popflag = 0;

                if ( currentarg == "-Zawf" )
                {
                    // Pop element of command stack

                    popflag = 1;
                }

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    std::ifstream *filein;

                    MEMNEW(filein,std::ifstream);

                    if ( filein )
                    {
                        filein->open(currentarg.c_str(),std::ifstream::in);

                        if ( filein->is_open() )
                        {
                            stopnow = grabnextarg(commstack,currentarg);

                            if ( !stopnow )
                            {
                                if ( popflag )
                                {
                                    // Pop element of command stack

                                    MEMDEL(commstack.accessTop());
                                    commstack.pop();
                                }

                                std::ofstream *fileout;

                                MEMNEW(fileout,std::ofstream);

                                if ( fileout )
                                {
                                    fileout->open(currentarg.c_str(),std::ofstream::out);

                                    if ( fileout->is_open() )
                                    {
                                        awarestream *fileiobox;

                                        MEMNEW(fileiobox,awarestream(filein,fileout,1,1));

                                        commstack.push(fileiobox);
                                    }

                                    else
                                    {
                                        errstream() << "Unable to open file -Zwf ... " << currentarg << "\n";
                                        retval  = 203;
                                        stopnow = 1;
                                    }
                                }

                                else
                                {
                                    errstream() << "Unable to open file -Zwf ... " << currentarg << "\n";
                                    retval  = 204;
                                    stopnow = 1;
                                }
                            }

                            else
                            {
                                errstream() << "Syntax error: -Zwf and -Zawf require 2 argument\n";
                                retval  = 27;
                                stopnow = 1;
                            }
                        }

                        else
                        {
                            errstream() << "Unable to open file -Zwf " << currentarg << " ...\n";
                            retval  = 205;
                            stopnow = 1;
                        }
                    }

                    else
                    {
                        errstream() << "Unable to open file -Zwf " << currentarg << " ...\n";
                        retval  = 206;
                        stopnow = 1;
                    }
                }

                else
                {
                    errstream() << "Syntax error: -Zwf and -Zawf require 2 argument\n";
                    retval  = 28;
                    stopnow = 1;
                }
            }

            else if ( ( currentarg == "-Zkf" ) || ( currentarg == "-Zakf" ) )
            {
                preelse = 1;

                // Switch input to cin with feedback to cout

                if ( currentarg == "-Zakf" )
                {
                    // Pop element of command stack

                    MEMDEL(commstack.accessTop());
                    commstack.pop();
                }

                awarestream *stdcinbox;

                MEMNEW(stdcinbox,awarestream(&instream(),&outstream(),0,0));

                commstack.push(stdcinbox);
            }

            else if ( currentarg == "-Za" )
            {
                preelse = 1;

                // Pop element of command stack

                MEMDEL(commstack.accessTop());
                commstack.pop();

                if ( commstack.size() == 0 )
                {
                    // No more input streams, exiting

                    stopnow = 1;
                }
            }

            else if ( currentarg == "-Zcw" )
            {
                preelse = 1;

                // push string onto shared stack

                Vector<Vector<std::string> > pushonopt;

                if ( grabargs(3,pushonopt,commstack,currentarg) )
                {
                    retval  = 39;
                    stopnow = 1;
                }

                int stacknum = safeatoi(pushonopt(0)(1),argvariables);

                static awarestream streamdummy("&","&",stacknum);

                streamdummy.vogon(pushonopt(0)(2));
            }

            else if ( currentarg == "-ZcW" )
            {
                preelse = 1;

                // push string onto shared stack

                int stacknum = 0;

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    stacknum = safeatoi(currentarg,argvariables);

                    stopnow = grabnextarg(commstack,currentarg);
                }

                if ( !stopnow )
                {
                    static awarestream streamdummy("&","&",stacknum);

                    gentype tmpg;

                    streamdummy.vogon(safeatowhatever(tmpg,currentarg,argvariables).cast_string(1));
                }

                else
                {
                    errstream() << "Syntax error: -Zcw and -ZcW require 2 argument\n";
                    retval  = 28;
                    stopnow = 1;
                }
            }

            else if ( currentarg == "-Zf" )
            {
                preelse = 1;

                // Create an empty flag file

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    std::ofstream flagfile;
                    flagfile.open(currentarg.c_str(),std::ofstream::out);

                    if ( flagfile.is_open() )
                    {
                        flagfile.close();
                    }

                    else
                    {
                        errstream() << "Unable to open file -Zf " << currentarg << "\n";
                        retval  = 207;
                        stopnow = 1;
                    }
                }

                else
                {
                    errstream() << "Syntax error: -Zf requires 1 argument\n";
                    retval  = 37;
                    stopnow = 1;
                }
            }

            else if ( currentarg == "-Zp" )
            {
                preelse = 1;

                // Wait for flag file to be created

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    errstream() << "Waiting for " << currentarg << " flagfile to be created...";

                    int ispresent = 0;

                    while ( !ispresent )
                    {
                        std::ifstream sndflagfile(currentarg.c_str());

                        if ( !(sndflagfile.is_open()) )
                        {
                            svm_usleep(1000000/FILETESTRATE);
                        }

                        else
                        {
                            sndflagfile.close();
                            ispresent = 1;
                        }
                    }

                    errstream() << "done\n";
                }

                else
                {
                    errstream() << "Syntax error: -Zp requires 1 argument\n";
                    retval  = 38;
                    stopnow = 1;
                }
            }

            else if ( currentarg == "-Zff" )
            {
                preelse = 1;

                // Create a file and write variable to it

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    std::ofstream flagfile;
                    flagfile.open(currentarg.c_str(),std::ofstream::out);

                    if ( flagfile.is_open() )
                    {
                        stopnow = grabnextarg(commstack,currentarg);

                        if ( !stopnow )
                        {
                            gentype tmpg;

                            flagfile << safeatowhatever(tmpg,currentarg,argvariables) << "\n";
                            flagfile.close();
                        }

                        else
                        {
                            flagfile.close();

                            errstream() << "Syntax error: -Zff requires 2 arguments\n";
                            retval  = 37;
                            stopnow = 1;
                        }
                    }

                    else
                    {
                        stopnow = grabnextarg(commstack,currentarg);

                        errstream() << "Unable to open file -Zff " << currentarg << "\n";
                        retval  = 207;
                        stopnow = 1;
                    }
                }

                else
                {
                    errstream() << "Syntax error: -Zff requires 2 arguments\n";
                    retval  = 37;
                    stopnow = 1;
                }
            }

            else if ( currentarg == "-Zpp" )
            {
                preelse = 1;

                // Wait for flag file to be created and read variable from it

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    std::string zppnum;

                    stopnow = grabnextarg(commstack,zppnum);

                    if ( !stopnow )
                    {
                        int nn = safeatoi(zppnum,argvariables);

                        errstream() << "Waiting for " << currentarg << " flagfile to be created...";

                        int ispresent = 0;
                        std::ifstream sndflagfile;

                        while ( !ispresent )
                        {
                            sndflagfile.open(currentarg.c_str(),std::ifstream::in);

                            if ( !(sndflagfile.is_open()) )
                            {
                                svm_usleep(1000000/FILETESTRATE);
                            }

                            else
                            {
                                //sndflagfile.close();
                                ispresent = 1;
                            }
                        }

                        sndflagfile >> argvariables("&",0)("&",nn);
                        sndflagfile.close();

                        errstream() << "done\n";
                    }

                    else
                    {
                        goto zpperr;
                    }
                }

                else
                {
                    zpperr:
                    errstream() << "Syntax error: -Zpp requires 2 arguments\n";
                    retval  = 38;
                    stopnow = 1;
                }
            }

            else if ( currentarg == "-Zdel" )
            {
                preelse = 1;

                // Remove file

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    remove(currentarg.c_str());
                }

                else
                {
                    errstream() << "Syntax error: -Zdel requires 1 argument\n";
                    retval  = 38;
                    stopnow = 1;
                }
            }







































































            else if ( ( currentarg == "-Lmute"    ) ||
                      ( currentarg == "-Lunmute"  ) ||
                      ( currentarg == "-LMute"    ) ||
                      ( currentarg == "-LunMute"  ) ||
                      ( currentarg == "-LMUTE"    ) ||
                      ( currentarg == "-LunMUTE"  )    )
            {
                preelse = 1;

                // Logging options

                if ( grabargs(1,loggingopt,commstack,currentarg) )
                {
                    retval  = 39;
                    stopnow = 1;
                }
            }

            else if ( ( currentarg == "-v"  ) ||
                      ( currentarg == "-L"  ) ||
                      ( currentarg == "-LL" )    )
            {
                preelse = 1;

                // Logging options

                if ( grabargs(2,loggingopt,commstack,currentarg) )
                {
                    retval  = 39;
                    stopnow = 1;
                }
            }

            else if ( ( currentarg == "-qpop" )    )
            {
                preelse = 1;

                // Setup options

                if ( grabargs(1,multirunopt,commstack,currentarg) )
                {
                    retval  = 40;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-qR"    ) ||
                      ( currentarg == "-qpush" ) ||
                      ( currentarg == "-qw"    )    )
            {
                preelse = 1;

                // Setup options

                if ( grabargs(2,multirunopt,commstack,currentarg) )
                {
                    retval  = 40;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-qc"    ) ||
                      ( currentarg == "-qsave" ) ||
                      ( currentarg == "-qs"    )    )
            {
                preelse = 1;

                // Setup options

                if ( grabargs(3,multirunopt,commstack,currentarg) )
                {
                    retval  = 41;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-z"  ) ||
                      ( currentarg == "-zc" ) ||
                      ( currentarg == "-zo" ) ||
                      ( currentarg == "-zd" ) ||
                      ( currentarg == "-zv" ) ||
                      ( currentarg == "-zl" )    )
            {
                preelse = 1;

                // Setup options

                if ( grabargs(2,svmpresetupopt,commstack,currentarg) )
                {
                    retval  = 42;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-bv"   ) ||
                      ( currentarg == "-bz"   ) ||
                      ( currentarg == "-bgv"  ) ||
                      ( currentarg == "-bgep" ) ||
                      ( currentarg == "-bgnc" ) ||
                      ( currentarg == "-bgz"  )    )
            {
                preelse = 1;

                // Setup options

                if ( grabargs(1,svmsetupopt,commstack,currentarg) )
                {
                    retval  = 43;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-ac"  ) ||
                      ( currentarg == "-B"   ) ||
                      ( currentarg == "-R"   ) ||
                      ( currentarg == "-mls" ) ||
                      ( currentarg == "-T"   ) ||
                      ( currentarg == "-TT"  ) ||
                      ( currentarg == "-N"   ) ||
                      ( currentarg == "-br"  ) ||
                      ( currentarg == "-bd"  ) ||
                      ( currentarg == "-bn"  ) ||
                      ( currentarg == "-bgn" ) ||
                      ( currentarg == "-XT"  )    )
            {
                preelse = 1;

                // Setup options

                if ( grabargs(2,svmsetupopt,commstack,currentarg) )
                {
                    retval  = 44;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fru" ) ||
                      ( currentarg == "-frn" ) ||
                      ( currentarg == "-fri" ) ||
                      ( currentarg == "-mc"  ) ||
                      ( currentarg == "-mcn" ) ||
                      ( currentarg == "-mbA" ) ||
                      ( currentarg == "-mbm" ) ||
                      ( currentarg == "-msn" ) ||
                      ( currentarg == "-msw" ) ||
                      ( currentarg == "-bat" ) ||
                      ( currentarg == "-bam" ) ||
                      ( currentarg == "-bac" ) ||
                      ( currentarg == "-bad" ) ||
                      ( currentarg == "-bav" ) ||
                      ( currentarg == "-baT" ) ||
                      ( currentarg == "-fat" )    )
            {
                preelse = 1;

                // Setup options

                if ( grabargs(2,svmsetupopt,commstack,currentarg) )
                {
                    retval  = 45;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fo"   ) ||
                      ( currentarg == "-foe"  ) ||
                      ( currentarg == "-fV"   ) ||
                      ( currentarg == "-mlR"  ) ||
                      ( currentarg == "-fW"   ) ||
                      ( currentarg == "-fWW"  ) ||
                      ( currentarg == "-fret" ) ||
                      ( currentarg == "-fVg"  ) ||
                      ( currentarg == "-fWg"  ) ||
                      ( currentarg == "-fVG"  ) ||
                      ( currentarg == "-fWG"  ) ||
                      ( currentarg == "-mbw"  ) ||
                      ( currentarg == "-mbI"  ) ||
                      ( currentarg == "-mba"  )    )
            {
                preelse = 1;

                // Setup options

                if ( grabargs(3,svmsetupopt,commstack,currentarg) )
                {
                    retval  = 45;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fWm" ) ||
                      ( currentarg == "-fWM" )    )
            {
                preelse = 1;

                // Setup options

                if ( grabargs(4,svmsetupopt,commstack,currentarg) )
                {
                    retval  = 45;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fu"  ) ||
                      ( currentarg == "-fuG" )    )
            {
                preelse = 1;

                // Setup options

                if ( grabargs(4,svmsetupopt,commstack,currentarg) )
                {
                    retval  = 46;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-ft"   ) ||
                      ( currentarg == "-ftG"  ) ||
                      ( currentarg == "-fuu"  ) ||
                      ( currentarg == "-fuuG" )    )
            {
                preelse = 1;

                // Setup options

                if ( grabargs(5,svmsetupopt,commstack,currentarg) )
                {
                    retval  = 46;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fM"  )    )
            {
                preelse = 1;

                // Setup options

                if ( grabargs(3,svmsetupopt,commstack,currentarg) )
                {
                    retval  = 47;
                    stopnow = 1;
                }

                stripcurlybrackets(svmsetupopt("&",svmsetupopt.size()-1)("&",2));

                updateargvars = 1;
            }

            else if ( ( currentarg == "-oo"  ) ||
                      ( currentarg == "-ofy" ) ||
                      ( currentarg == "-ofn" ) ||
                      ( currentarg == "-oO"  )    )
            {
                preelse = 1;

                // Optimisation options - here so that the cholesky doesn't get filled for smo/d2c/gradient, which is important for large datasets

                if ( grabargs(1,optimopt,commstack,currentarg) )
                {
                    retval  = 87;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-om"   ) ||
                      ( currentarg == "-oM"   ) ||
                      ( currentarg == "-oe"   ) ||
                      ( currentarg == "-oea"  ) ||
                      ( currentarg == "-oeb"  ) ||
                      ( currentarg == "-oz"   ) ||
                      ( currentarg == "-ot"   ) ||
                      ( currentarg == "-oy"   ) ||
                      ( currentarg == "-oY"   ) ||
                      ( currentarg == "-ofa"  ) ||
                      ( currentarg == "-ofe"  ) ||
                      ( currentarg == "-ofm"  ) ||
                      ( currentarg == "-ofr"  ) ||
                      ( currentarg == "-ofs"  ) ||
                      ( currentarg == "-oft"  ) ||
                      ( currentarg == "-ofM"  ) ||
                      ( currentarg == "-omr"  ) ||
                      ( currentarg == "-ome"  ) ||
                      ( currentarg == "-oms"  ) ||
                      ( currentarg == "-olr"  ) ||
                      ( currentarg == "-olrb" ) ||
                      ( currentarg == "-olrc" ) ||
                      ( currentarg == "-olrd" )    )
            {
                preelse = 1;

                // Optimisation options

                if ( grabargs(2,optimopt,commstack,currentarg) )
                {
                    retval  = 88;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-prz" ) ||
                      ( currentarg == "-pR"  ) ||
                      ( currentarg == "-pRR" ) ||
                      ( currentarg == "-pS"  ) ||
                      ( currentarg == "-fic" )    )
            {
                preelse = 1;

                // Preload options

                if ( grabargs(1,preloadopt,commstack,currentarg) )
                {
                    retval  = 50;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-pr"  ) ||
                      ( currentarg == "-pro" ) ||
                      ( currentarg == "-prm" ) ||
                      ( currentarg == "-pcs" ) ||
                      ( currentarg == "-pds" ) ||
                      ( currentarg == "-pws" ) ||
                      ( currentarg == "-pk"  ) ||
                      ( currentarg == "-ps"  )    )
            {
                preelse = 1;

                // Preload options

                if ( grabargs(2,preloadopt,commstack,currentarg) )
                {
                    retval  = 51;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-pcw" ) ||
                      ( currentarg == "-pdw" ) ||
                      ( currentarg == "-pww" ) ||
                      ( currentarg == "-psd" ) ||
                      ( currentarg == "-psz" )    )
            {
                preelse = 1;

                // Preload options

                if ( grabargs(3,preloadopt,commstack,currentarg) )
                {
                    retval  = 52;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-Acd"   ) ||
                      ( currentarg == "-Aby"   ) ||
                      ( currentarg == "-ABy"   ) ||
                      ( currentarg == "-Abu"   ) ||
                      ( currentarg == "-ABu"   )    )
            {
                preelse = 1;

                // Load options

                if ( grabargs(1,loadopt,commstack,currentarg) )
                {
                    retval  = 53;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-Ac"     ) ||
                      ( currentarg == "-As"     ) ||
                      ( currentarg == "-Ad"     ) ||
                      ( currentarg == "-AD"     ) ||
                      ( currentarg == "-Ar"     ) ||
                      ( currentarg == "-AR"     ) ||
                      ( currentarg == "-AA"     ) ||
                      ( currentarg == "-AAe"    ) ||
                      ( currentarg == "-AAi"    ) ||
                      ( currentarg == "-AAI"    ) ||
                      ( currentarg == "-AAr"    ) ||
                      ( currentarg == "-AAR"    ) ||
                      ( currentarg == "-AAu"    ) ||
                      ( currentarg == "-AAei"   ) ||
                      ( currentarg == "-AAeI"   ) ||
                      ( currentarg == "-AAer"   ) ||
                      ( currentarg == "-AAeR"   ) ||
                      ( currentarg == "-AAeu"   ) ||
                      ( currentarg == "-AAiu"   ) ||
                      ( currentarg == "-AAIu"   ) ||
                      ( currentarg == "-AAru"   ) ||
                      ( currentarg == "-AARu"   ) ||
                      ( currentarg == "-AAeiu"  ) ||
                      ( currentarg == "-AAeIu"  ) ||
                      ( currentarg == "-AAeru"  ) ||
                      ( currentarg == "-AAeRu"  ) ||
                      ( currentarg == "-ATA"    ) ||
                      ( currentarg == "-ATAe"   ) ||
                      ( currentarg == "-ATAi"   ) ||
                      ( currentarg == "-ATAI"   ) ||
                      ( currentarg == "-ATAr"   ) ||
                      ( currentarg == "-ATAR"   ) ||
                      ( currentarg == "-ATAu"   ) ||
                      ( currentarg == "-ATAei"  ) ||
                      ( currentarg == "-ATAeI"  ) ||
                      ( currentarg == "-ATAer"  ) ||
                      ( currentarg == "-ATAeR"  ) ||
                      ( currentarg == "-ATAeu"  ) ||
                      ( currentarg == "-ATAiu"  ) ||
                      ( currentarg == "-ATAIu"  ) ||
                      ( currentarg == "-ATAru"  ) ||
                      ( currentarg == "-ATARu"  ) ||
                      ( currentarg == "-ATAeiu" ) ||
                      ( currentarg == "-ATAeIu" ) ||
                      ( currentarg == "-ATAeru" ) ||
                      ( currentarg == "-ATAeRu" ) ||
                      ( currentarg == "-ATb"    ) ||
                      ( currentarg == "-ATa"    ) ||
                      ( currentarg == "-ATn"    ) ||
                      ( currentarg == "-ATx"    ) ||
                      ( currentarg == "-ATy"    ) ||
                      ( currentarg == "-Acz"    ) ||
                      ( currentarg == "-AeA"    ) ||
                      ( currentarg == "-AEA"    ) ||
                      ( currentarg == "-AeU"    ) ||
                      ( currentarg == "-AEU"    ) ||
                      ( currentarg == "-AGl"    ) ||
                      ( currentarg == "-AGu"    ) ||
                      ( currentarg == "-ID"     )    )
            {
                preelse = 1;

                // Load options

                if ( grabargs(2,loadopt,commstack,currentarg) )
                {
                    retval  = 54;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-AU"  ) ||
                      ( currentarg == "-AY"  ) ||
                      ( currentarg == "-AV"  ) ||
                      ( currentarg == "-AVv" ) ||
                      ( currentarg == "-AeR" ) ||
                      ( currentarg == "-AER" ) ||
                      ( currentarg == "-AW"  )    )
            {
                preelse = 1;

                // Load options

                if ( grabargs(3,loadopt,commstack,currentarg) )
                {
                    retval  = 55;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-AZ"  ) ||
                      ( currentarg == "-Aq"  ) ||
                      ( currentarg == "-AVV" )    )
            {
                preelse = 1;

                // Load options

                if ( grabargs(4,loadopt,commstack,currentarg) )
                {
                    retval  = 56;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-AAl"    ) ||
                      ( currentarg == "-AAel"   ) ||
                      ( currentarg == "-AAil"   ) ||
                      ( currentarg == "-AAIl"   ) ||
                      ( currentarg == "-AArl"   ) ||
                      ( currentarg == "-AARl"   ) ||
                      ( currentarg == "-AAeil"  ) ||
                      ( currentarg == "-AAeIl"  ) ||
                      ( currentarg == "-AAerl"  ) ||
                      ( currentarg == "-AAeRl"  ) ||
                      ( currentarg == "-ATAl"   ) ||
                      ( currentarg == "-ATAel"  ) ||
                      ( currentarg == "-ATAil"  ) ||
                      ( currentarg == "-ATAIl"  ) ||
                      ( currentarg == "-ATArl"  ) ||
                      ( currentarg == "-ATARl"  ) ||
                      ( currentarg == "-ATAeil" ) ||
                      ( currentarg == "-ATAeIl" ) ||
                      ( currentarg == "-ATAerl" ) ||
                      ( currentarg == "-ATAeRl" ) ||
                      ( currentarg == "-Aca"    )    )
            {
                preelse = 1;

                // Load options

                if ( grabargs(3,loadopt,commstack,currentarg) )
                {
                    retval  = 57;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-Ag"     ) ||
                      ( currentarg == "-AG"     ) ||
                      ( currentarg == "-AN"     ) ||
                      ( currentarg == "-ANe"    ) ||
                      ( currentarg == "-ANi"    ) ||
                      ( currentarg == "-ANI"    ) ||
                      ( currentarg == "-ANr"    ) ||
                      ( currentarg == "-ANR"    ) ||
                      ( currentarg == "-ANu"    ) ||
                      ( currentarg == "-ANeu"   ) ||
                      ( currentarg == "-ANiu"   ) ||
                      ( currentarg == "-ANIu"   ) ||
                      ( currentarg == "-ANru"   ) ||
                      ( currentarg == "-ANRu"   ) ||
                      ( currentarg == "-ANei"   ) ||
                      ( currentarg == "-ANeI"   ) ||
                      ( currentarg == "-ANer"   ) ||
                      ( currentarg == "-ANeR"   ) ||
                      ( currentarg == "-ANeiu"  ) ||
                      ( currentarg == "-ANeIu"  ) ||
                      ( currentarg == "-ANeru"  ) ||
                      ( currentarg == "-ANeRu"  ) ||
                      ( currentarg == "-ATN"    ) ||
                      ( currentarg == "-ATNe"   ) ||
                      ( currentarg == "-ATNi"   ) ||
                      ( currentarg == "-ATNI"   ) ||
                      ( currentarg == "-ATNr"   ) ||
                      ( currentarg == "-ATNR"   ) ||
                      ( currentarg == "-ATNu"   ) ||
                      ( currentarg == "-ATNeu"  ) ||
                      ( currentarg == "-ATNiu"  ) ||
                      ( currentarg == "-ATNIu"  ) ||
                      ( currentarg == "-ATNru"  ) ||
                      ( currentarg == "-ATNRu"  ) ||
                      ( currentarg == "-ATNei"  ) ||
                      ( currentarg == "-ATNeI"  ) ||
                      ( currentarg == "-ATNer"  ) ||
                      ( currentarg == "-ATNeR"  ) ||
                      ( currentarg == "-ATNeiu" ) ||
                      ( currentarg == "-ATNeIu" ) ||
                      ( currentarg == "-ATNeru" ) ||
                      ( currentarg == "-ATNeRu" )    )
            {
                preelse = 1;

                // Load options

                if ( grabargs(5,loadopt,commstack,currentarg) )
                {
                    retval  = 58;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-Agc"    ) ||
                      ( currentarg == "-AGc"    ) ||
                      ( currentarg == "-ANl"    ) ||
                      ( currentarg == "-ANel"   ) ||
                      ( currentarg == "-ANil"   ) ||
                      ( currentarg == "-ANIl"   ) ||
                      ( currentarg == "-ANrl"   ) ||
                      ( currentarg == "-ANRl"   ) ||
                      ( currentarg == "-ANeil"  ) ||
                      ( currentarg == "-ANeIl"  ) ||
                      ( currentarg == "-ANerl"  ) ||
                      ( currentarg == "-ANeRl"  ) ||
                      ( currentarg == "-ATNl"   ) ||
                      ( currentarg == "-ATNel"  ) ||
                      ( currentarg == "-ATNil"  ) ||
                      ( currentarg == "-ATNIl"  ) ||
                      ( currentarg == "-ATNrl"  ) ||
                      ( currentarg == "-ATNRl"  ) ||
                      ( currentarg == "-ATNeil" ) ||
                      ( currentarg == "-ATNeIl" ) ||
                      ( currentarg == "-ATNerl" ) ||
                      ( currentarg == "-ATNeRl" )    )
            {
                preelse = 1;

                // Load options

                if ( grabargs(6,loadopt,commstack,currentarg) )
                {
                    retval  = 59;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-Snx" ) ||
                      ( currentarg == "-Sna" ) ||
                      ( currentarg == "-Snb" ) ||
                      ( currentarg == "-Snc" ) ||
                      ( currentarg == "-SNa" ) ||
                      ( currentarg == "-SNb" ) ||
                      ( currentarg == "-SNc" ) ||
                      ( currentarg == "-SnA" ) ||
                      ( currentarg == "-SnB" ) ||
                      ( currentarg == "-SnC" ) ||
                      ( currentarg == "-SNA" ) ||
                      ( currentarg == "-SNB" ) ||
                      ( currentarg == "-SNC" ) ||
                      ( currentarg == "-St"  ) ||
                      ( currentarg == "-Spt" ) ||
                      ( currentarg == "-SPt" ) ||
                      ( currentarg == "-Sjt" ) ||
                      ( currentarg == "-Sjt+" ) ||
                      ( currentarg == "-Sjt-" ) ||
                      ( currentarg == "-Srt" ) ||
                      ( currentarg == "-Snt" )    )
            {
                preelse = 1;

                // Postload options

                if ( grabargs(1,postloadopt,commstack,currentarg) )
                {
                    retval  = 60;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-Sa"  ) ||
                      ( currentarg == "-Sb"  ) ||
                      ( currentarg == "-Saa" ) ||
                      ( currentarg == "-Sbb" ) ||
                      ( currentarg == "-SA"  ) ||
                      ( currentarg == "-SAA" ) ||
                      ( currentarg == "-Sdi" ) ||
                      ( currentarg == "-Stl" ) ||
                      ( currentarg == "-Stu" ) ||
                      ( currentarg == "-StN" ) ||
                      ( currentarg == "-Sts" ) ||
                      ( currentarg == "-Stx" ) ||
                      ( currentarg == "-Stt" ) ||
                      ( currentarg == "-Stc" ) ||
                      ( currentarg == "-Stq" ) ||
                      ( currentarg == "-Svc" ) ||
                      ( currentarg == "-Sx"  ) ||
                      ( currentarg == "-Sra" )    )
            {
                preelse = 1;

                // Postload options

                if ( grabargs(2,postloadopt,commstack,currentarg) )
                {
                    retval  = 61;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-Sai" ) ||
                      ( currentarg == "-SdI" )    )
            {
                preelse = 1;

                // Postload options

                if ( grabargs(3,postloadopt,commstack,currentarg) )
                {
                    retval  = 61;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-SAi" )    )
            {
                preelse = 1;

                // Postload options

                if ( grabargs(4,postloadopt,commstack,currentarg) )
                {
                    retval  = 61;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-Mn"  ) ||
                      ( currentarg == "-Mi"  ) ||
                      ( currentarg == "-Md"  )    )
            {
                preelse = 1;

                // Learning options

                if ( grabargs(1,learningopt,commstack,currentarg) )
                {
                    retval  = 62;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-c"    ) ||
                      ( currentarg == "-dc"   ) ||
                      ( currentarg == "-ec"   ) ||
                      ( currentarg == "-fc"   ) ||
                      ( currentarg == "-Gc"   ) ||
                      ( currentarg == "-dcs"  ) ||
                      ( currentarg == "-ecs"  ) ||
                      ( currentarg == "-fcs"  ) ||
                      ( currentarg == "-Gcs"  ) ||
                      ( currentarg == "-trf"  ) ||
                      ( currentarg == "-trk"  ) ||
                      ( currentarg == "-tmv"  ) ||
                      ( currentarg == "-rfs"  ) ||
                      ( currentarg == "-dia"  ) ||
                      ( currentarg == "-dog"  ) ||
                      ( currentarg == "-th"   ) ||
                      ( currentarg == "-thn"  ) ||
                      ( currentarg == "-c+"   ) ||
                      ( currentarg == "-c-"   ) ||
                      ( currentarg == "-c="   ) ||
                      ( currentarg == "-cs"   ) ||
                      ( currentarg == "-ds"   ) ||
                      ( currentarg == "-ccs"  ) ||
                      ( currentarg == "-c+s"  ) ||
                      ( currentarg == "-c-s"  ) ||
                      ( currentarg == "-c=s"  ) ||
                      ( currentarg == "-j"    ) ||
                      ( currentarg == "-jc"   ) ||
                      ( currentarg == "-dd"   ) ||
                      ( currentarg == "-w"    ) ||
                      ( currentarg == "-mu"   ) ||
                      ( currentarg == "-mugt" ) ||
                      ( currentarg == "-muml" ) ||
                      ( currentarg == "-w+"   ) ||
                      ( currentarg == "-w-"   ) ||
                      ( currentarg == "-w="   ) ||
                      ( currentarg == "-ws"   ) ||
                      ( currentarg == "-w+s"  ) ||
                      ( currentarg == "-w-s"  ) ||
                      ( currentarg == "-w=s"  ) ||
                      ( currentarg == "-jw"   ) ||
                      ( currentarg == "-nm"   ) ||
                      ( currentarg == "-nN"   ) ||
                      ( currentarg == "-mtl"  ) ||
                      ( currentarg == "-Bf"   ) ||
                      ( currentarg == "-Tl"   ) ||
                      ( currentarg == "-Tq"   ) ||
                      ( currentarg == "-Nl"   ) ||
                      ( currentarg == "-Nq"   ) ||
                      ( currentarg == "-Fi"   ) ||
                      ( currentarg == "-Flr"  ) ||
                      ( currentarg == "-Fzt"  ) ||
                      ( currentarg == "-mvi"  ) ||
                      ( currentarg == "-mvlr" ) ||
                      ( currentarg == "-mvzt" ) ||
                      ( currentarg == "-mvb"  ) ||
                      ( currentarg == "-Fc"   ) ||
                      ( currentarg == "-m"    ) ||
                      ( currentarg == "-blx"  ) ||
                      ( currentarg == "-bly"  ) ||
                      ( currentarg == "-blz"  ) ||
                      ( currentarg == "-bls"  ) ||
                      ( currentarg == "-bfx"  ) ||
                      ( currentarg == "-bfy"  ) ||
                      ( currentarg == "-bfxy" ) ||
                      ( currentarg == "-bfyx" ) ||
                      ( currentarg == "-bfr"  ) ||
                      ( currentarg == "-k"    ) ||
                      ( currentarg == "-ek"   ) ||
                      ( currentarg == "-rk"   ) ||
                      ( currentarg == "-K"    ) ||
                      ( currentarg == "-iz"   ) ||
                      ( currentarg == "-ie"   ) ||
                      ( currentarg == "-is"   ) ||
                      ( currentarg == "-ia"   ) ||
                      ( currentarg == "-in"   ) ||
                      ( currentarg == "-il"   ) ||
                      ( currentarg == "-d"    ) ||
                      ( currentarg == "-ccs"  )    )
            {
                preelse = 1;

                // Learning options

                if ( grabargs(2,learningopt,commstack,currentarg) )
                {
                    retval  = 62;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-cd"   ) ||
                      ( currentarg == "-cw"   ) ||
                      ( currentarg == "-dw"   ) ||
                      ( currentarg == "-mlc"  ) ||
                      ( currentarg == "-cds"  ) ||
                      ( currentarg == "-wd"   ) ||
                      ( currentarg == "-ww"   ) ||
                      ( currentarg == "-wds"  ) ||
                      ( currentarg == "-Nld"  ) ||
                      ( currentarg == "-Nqd"  )    )
            {
                preelse = 1;

                // Learning options

                if ( grabargs(3,learningopt,commstack,currentarg) )
                {
                    retval  = 63;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-kn"    ) ||
                      ( currentarg == "-kp"    ) ||
                      ( currentarg == "-knp"   ) ||
                      ( currentarg == "-ku"    ) ||
                      ( currentarg == "-kss"   ) ||
                      ( currentarg == "-kus"   ) ||
                      ( currentarg == "-knn"   ) ||
                      ( currentarg == "-kuu"   ) ||
                      ( currentarg == "-kc"    ) ||
                      ( currentarg == "-kuc"   ) ||
                      ( currentarg == "-km"    ) ||
                      ( currentarg == "-kum"   ) ||
                      ( currentarg == "-kS"    ) ||
                      ( currentarg == "-kA"    ) ||
                      ( currentarg == "-kuS"   ) ||
                      ( currentarg == "-kMS"   ) ||
                      ( currentarg == "-kMA"   ) ||
                      ( currentarg == "-kMuS"  ) ||
                      ( currentarg == "-kU"    ) ||
                      ( currentarg == "-koz"   ) ||
                      ( currentarg == "-mtb"   ) ||
                      ( currentarg == "-bmx"   ) ||
                      ( currentarg == "-kOz"   ) ||
                      ( currentarg == "-ekn"   ) ||
                      ( currentarg == "-ekp"   ) ||
                      ( currentarg == "-eknp"  ) ||
                      ( currentarg == "-eku"   ) ||
                      ( currentarg == "-ekss"  ) ||
                      ( currentarg == "-ekus"  ) ||
                      ( currentarg == "-eknn"  ) ||
                      ( currentarg == "-ekuu"  ) ||
                      ( currentarg == "-ekc"   ) ||
                      ( currentarg == "-ekuc"  ) ||
                      ( currentarg == "-ekm"   ) ||
                      ( currentarg == "-ekum"  ) ||
                      ( currentarg == "-ekS"   ) ||
                      ( currentarg == "-ekA"   ) ||
                      ( currentarg == "-ekuS"  ) ||
                      ( currentarg == "-ekMS"  ) ||
                      ( currentarg == "-ekMA"  ) ||
                      ( currentarg == "-ekMuS" ) ||
                      ( currentarg == "-ekU"   ) ||
                      ( currentarg == "-ekoz"  ) ||
                      ( currentarg == "-emtb"  ) ||
                      ( currentarg == "-ebmx"  ) ||
                      ( currentarg == "-ekOz"  ) ||
                      ( currentarg == "-rkn"   ) ||
                      ( currentarg == "-rkp"   ) ||
                      ( currentarg == "-rknp"  ) ||
                      ( currentarg == "-rku"   ) ||
                      ( currentarg == "-rkss"  ) ||
                      ( currentarg == "-rkus"  ) ||
                      ( currentarg == "-rknn"  ) ||
                      ( currentarg == "-rkuu"  ) ||
                      ( currentarg == "-rkc"   ) ||
                      ( currentarg == "-rkuc"  ) ||
                      ( currentarg == "-rkm"   ) ||
                      ( currentarg == "-rkum"  ) ||
                      ( currentarg == "-rkS"   ) ||
                      ( currentarg == "-rkA"   ) ||
                      ( currentarg == "-rkuS"  ) ||
                      ( currentarg == "-rkMS"  ) ||
                      ( currentarg == "-rkMA"  ) ||
                      ( currentarg == "-rkMuS" ) ||
                      ( currentarg == "-rkU"   ) ||
                      ( currentarg == "-rkoz"  ) ||
                      ( currentarg == "-rmtb"  ) ||
                      ( currentarg == "-rbmx"  ) ||
                      ( currentarg == "-rkOz"  )    )
            {
                preelse = 1;

                // Kernel options

                if ( grabargs(1,kernelopt,commstack,currentarg) )
                {
                    retval  = 64;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-ks"    ) ||
                      ( currentarg == "-ki"    ) ||
                      ( currentarg == "-ka"    ) ||
                      ( currentarg == "-kb"    ) ||
                      ( currentarg == "-ke"    ) ||
                      ( currentarg == "-kw"    ) ||
                      ( currentarg == "-kwlb"    ) ||
                      ( currentarg == "-kwub"    ) ||
                      ( currentarg == "-kt"    ) ||
                      ( currentarg == "-ktx"   ) ||
                      ( currentarg == "-ktk"   ) ||
                      ( currentarg == "-krn"   ) ||
                      ( currentarg == "-kgg"   ) ||
                      ( currentarg == "-kf"    ) ||
                      ( currentarg == "-kr"    ) ||
                      ( currentarg == "-kg"    ) ||
                      ( currentarg == "-kd"    ) ||
                      ( currentarg == "-kG"    ) ||
                      ( currentarg == "-krlb"    ) ||
                      ( currentarg == "-kglb"    ) ||
                      ( currentarg == "-kdlb"    ) ||
                      ( currentarg == "-kGlb"    ) ||
                      ( currentarg == "-krub"    ) ||
                      ( currentarg == "-kgub"    ) ||
                      ( currentarg == "-kdub"    ) ||
                      ( currentarg == "-kGub"    ) ||
                      ( currentarg == "-kI"    ) ||
                      ( currentarg == "-kan"   ) ||
                      ( currentarg == "-eks"   ) ||
                      ( currentarg == "-eki"   ) ||
                      ( currentarg == "-eka"   ) ||
                      ( currentarg == "-ekb"   ) ||
                      ( currentarg == "-eke"   ) ||
                      ( currentarg == "-ekw"   ) ||
                      ( currentarg == "-ekwlb"   ) ||
                      ( currentarg == "-ekwub"   ) ||
                      ( currentarg == "-ekt"   ) ||
                      ( currentarg == "-ektx"  ) ||
                      ( currentarg == "-ektk"  ) ||
                      ( currentarg == "-ekgg"  ) ||
                      ( currentarg == "-ekf"   ) ||
                      ( currentarg == "-ekrn"  ) ||
                      ( currentarg == "-ekr"   ) ||
                      ( currentarg == "-ekg"   ) ||
                      ( currentarg == "-ekd"   ) ||
                      ( currentarg == "-ekG"   ) ||
                      ( currentarg == "-ekrlb"   ) ||
                      ( currentarg == "-ekglb"   ) ||
                      ( currentarg == "-ekdlb"   ) ||
                      ( currentarg == "-ekGlb"   ) ||
                      ( currentarg == "-ekrub"   ) ||
                      ( currentarg == "-ekgub"   ) ||
                      ( currentarg == "-ekdub"   ) ||
                      ( currentarg == "-ekGub"   ) ||
                      ( currentarg == "-ekI"   ) ||
                      ( currentarg == "-ekan"  ) ||
                      ( currentarg == "-rks"   ) ||
                      ( currentarg == "-rki"   ) ||
                      ( currentarg == "-rkrn"  ) ||
                      ( currentarg == "-rka"   ) ||
                      ( currentarg == "-rkb"   ) ||
                      ( currentarg == "-rke"   ) ||
                      ( currentarg == "-rkw"   ) ||
                      ( currentarg == "-rkwlb"   ) ||
                      ( currentarg == "-rkwub"   ) ||
                      ( currentarg == "-rkt"   ) ||
                      ( currentarg == "-rktx"  ) ||
                      ( currentarg == "-rktk"  ) ||
                      ( currentarg == "-rkgg"  ) ||
                      ( currentarg == "-rkf"   ) ||
                      ( currentarg == "-rkr"   ) ||
                      ( currentarg == "-rkg"   ) ||
                      ( currentarg == "-rkd"   ) ||
                      ( currentarg == "-rkG"   ) ||
                      ( currentarg == "-rkrlb"   ) ||
                      ( currentarg == "-rkglb"   ) ||
                      ( currentarg == "-rkdlb"   ) ||
                      ( currentarg == "-rkGlb"   ) ||
                      ( currentarg == "-rkrub"   ) ||
                      ( currentarg == "-rkgub"   ) ||
                      ( currentarg == "-rkdub"   ) ||
                      ( currentarg == "-rkGub"   ) ||
                      ( currentarg == "-rkI"   ) ||
                      ( currentarg == "-rkan"  )    )
            {
                preelse = 1;

                // Kernel options

                if ( grabargs(2,kernelopt,commstack,currentarg) )
                {
                    retval  = 65;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-kv"  ) ||
                      ( currentarg == "-kV"  ) ||
                      ( currentarg == "-kvlb"  ) ||
                      ( currentarg == "-kVlb"  ) ||
                      ( currentarg == "-kvub"  ) ||
                      ( currentarg == "-kVub"  ) ||
                      ( currentarg == "-ko"  ) ||
                      ( currentarg == "-kO"  ) ||
                      ( currentarg == "-kx"  ) ||
                      ( currentarg == "-ekv" ) ||
                      ( currentarg == "-ekV" ) ||
                      ( currentarg == "-ekvlb" ) ||
                      ( currentarg == "-ekVlb" ) ||
                      ( currentarg == "-ekvub" ) ||
                      ( currentarg == "-ekVub" ) ||
                      ( currentarg == "-eko" ) ||
                      ( currentarg == "-ekO" ) ||
                      ( currentarg == "-ekx" ) ||
                      ( currentarg == "-rkv" ) ||
                      ( currentarg == "-rkV" ) ||
                      ( currentarg == "-rkvlb" ) ||
                      ( currentarg == "-rkVlb" ) ||
                      ( currentarg == "-rkvub" ) ||
                      ( currentarg == "-rkVub" ) ||
                      ( currentarg == "-rko" ) ||
                      ( currentarg == "-rkO" ) ||
                      ( currentarg == "-rkx" )    )
            {
                preelse = 1;

                // Kernel options

                if ( grabargs(3,kernelopt,commstack,currentarg) )
                {
                    retval  = 66;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-cua" ) ||
                      ( currentarg == "-cA"  ) ||
                      ( currentarg == "-cB"  ) ||
                      ( currentarg == "-cAN" ) ||
                      ( currentarg == "-cBN" ) ||
                      ( currentarg == "-bal" )    )
            {
                preelse = 1;

                // Tuning options

                if ( grabargs(1,tuningopt,commstack,currentarg) )
                {
                    retval  = 67;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-cX"      ) ||
                      ( currentarg == "-tkL"     ) ||
                      ( currentarg == "-tkloo"   ) ||
                      ( currentarg == "-tkrec"   ) ||
                      ( currentarg == "-tcL"     ) ||
                      ( currentarg == "-tcloo"   ) ||
                      ( currentarg == "-tcrec"   ) ||
                      ( currentarg == "-teL"     ) ||
                      ( currentarg == "-teloo"   ) ||
                      ( currentarg == "-terec"   ) ||
                      ( currentarg == "-tceL"    ) ||
                      ( currentarg == "-tceloo"  ) ||
                      ( currentarg == "-tcerec"  ) ||
                      ( currentarg == "-tkcL"    ) ||
                      ( currentarg == "-tkcloo"  ) ||
                      ( currentarg == "-tkcrec"  ) ||
                      ( currentarg == "-tkeL"    ) ||
                      ( currentarg == "-tkeloo"  ) ||
                      ( currentarg == "-tkerec"  ) ||
                      ( currentarg == "-tkceL"   ) ||
                      ( currentarg == "-tkceloo" ) ||
                      ( currentarg == "-tkcerec" )    )
            {
                preelse = 1;

                // Tuning options

                if ( grabargs(2,tuningopt,commstack,currentarg) )
                {
                    retval  = 68;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( currentarg == "-NlA" )
            {
                preelse = 1;

                // Tuning options

                if ( grabargs(3,tuningopt,commstack,currentarg) )
                {
                    retval  = 69;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            if ( preelse ) { ; }

            else if ( ( currentarg == "-g"  ) || ( currentarg == "-gd"  ) || ( currentarg == "-gN" ) || ( currentarg == "-gb"  ) )
            {
                // Gridsearch options
                //
                // To process we first read the number of arguments.  Then
                // there are 5a+2 arguments to come, where a is # args

                std::string storearg = currentarg;

                int a = 0;

                stopnow = grabnextarg(commstack,currentarg);

                if ( !stopnow )
                {
                    a = safeatoi(currentarg,argvariables);

                    if ( a >= 0 )
                    {
                        int acnt = (5*a)+2;

                        currentarg = storearg;

                        if ( grabargs(acnt+1,gridopt,commstack,currentarg) )
                        {
                            retval  = 70;
                            stopnow = 1;
                        }

                        //else
                        //{
                        //    stripquotes(gridopt("&",gridopt.size()-1)("&",1));
                        //    stripquotes(gridopt("&",gridopt.size()-1)("&",2));
                        //}
                    }

                    else
                    {
                        errstream() << "Syntax error: " << storearg << " argument count must be non-negative\n";
                        retval  = 71;
                        stopnow = 1;
                    }
                }

                else
                {
                    errstream() << "Syntax error: " << storearg << " requires 3 arguments minimum\n";
                    retval  = 72;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-gmo"     ) ||
                      ( currentarg == "-gms"     ) ||
                      ( currentarg == "-gmr"     ) ||
                      ( currentarg == "-gmR"     ) ||
                      ( currentarg == "-gnp"     ) ||
                      ( currentarg == "-gpb"     ) ||
                      ( currentarg == "-gph"     ) ||
                      ( currentarg == "-gnr"     ) ||
                      ( currentarg == "-gan"     ) ||
                      ( currentarg == "-gplot"   ) ||
                      ( currentarg == "-gphkn"   ) ||
                      ( currentarg == "-gphku"   ) ||
                      ( currentarg == "-gphkss"  ) ||
                      ( currentarg == "-gphkus"  ) ||
                      ( currentarg == "-gphknn"  ) ||
                      ( currentarg == "-gphkuu"  ) ||
                      ( currentarg == "-gphkc"   ) ||
                      ( currentarg == "-gphkuc"  ) ||
                      ( currentarg == "-gphkm"   ) ||
                      ( currentarg == "-gphkum"  ) ||
                      ( currentarg == "-gphkS"   ) ||
                      ( currentarg == "-gphkA"   ) ||
                      ( currentarg == "-gphkuS"  ) ||
                      ( currentarg == "-gphkMS"  ) ||
                      ( currentarg == "-gphkMA"  ) ||
                      ( currentarg == "-gphkMuS" ) ||
                      ( currentarg == "-gphkU"   ) ||
                      ( currentarg == "-gphkoz"  ) ||
                      ( currentarg == "-gphmtb"  ) ||
                      ( currentarg == "-gphbmx"  ) ||
                      ( currentarg == "-gphkOz"  ) ||
                      ( currentarg == "-gPkn"    ) ||
                      ( currentarg == "-gPku"    ) ||
                      ( currentarg == "-gPkss"   ) ||
                      ( currentarg == "-gPkus"   ) ||
                      ( currentarg == "-gPknn"   ) ||
                      ( currentarg == "-gPkuu"   ) ||
                      ( currentarg == "-gPkc"    ) ||
                      ( currentarg == "-gPkuc"   ) ||
                      ( currentarg == "-gPkm"    ) ||
                      ( currentarg == "-gPkum"   ) ||
                      ( currentarg == "-gPkS"    ) ||
                      ( currentarg == "-gPkA"    ) ||
                      ( currentarg == "-gPkuS"   ) ||
                      ( currentarg == "-gPkMS"   ) ||
                      ( currentarg == "-gPkMA"   ) ||
                      ( currentarg == "-gPkMuS"  ) ||
                      ( currentarg == "-gPkU"    ) ||
                      ( currentarg == "-gPkoz"   ) ||
                      ( currentarg == "-gPmtb"   ) ||
                      ( currentarg == "-gPbmx"   ) ||
                      ( currentarg == "-gPkOz"   ) ||
                      ( currentarg == "-gmkn"    ) ||
                      ( currentarg == "-gmku"    ) ||
                      ( currentarg == "-gmkss"   ) ||
                      ( currentarg == "-gmkus"   ) ||
                      ( currentarg == "-gmknn"   ) ||
                      ( currentarg == "-gmkuu"   ) ||
                      ( currentarg == "-gmkc"    ) ||
                      ( currentarg == "-gmkuc"   ) ||
                      ( currentarg == "-gmkm"    ) ||
                      ( currentarg == "-gmkum"   ) ||
                      ( currentarg == "-gmkS"    ) ||
                      ( currentarg == "-gmkA"    ) ||
                      ( currentarg == "-gmkuS"   ) ||
                      ( currentarg == "-gmkMS"   ) ||
                      ( currentarg == "-gmkMA"   ) ||
                      ( currentarg == "-gmkMuS"  ) ||
                      ( currentarg == "-gmkU"    ) ||
                      ( currentarg == "-gmkoz"   ) ||
                      ( currentarg == "-gmkOz"   ) ||
                      ( currentarg == "-gmekn"    ) ||
                      ( currentarg == "-gmeku"    ) ||
                      ( currentarg == "-gmekss"   ) ||
                      ( currentarg == "-gmekus"   ) ||
                      ( currentarg == "-gmeknn"   ) ||
                      ( currentarg == "-gmekuu"   ) ||
                      ( currentarg == "-gmekc"    ) ||
                      ( currentarg == "-gmekuc"   ) ||
                      ( currentarg == "-gmekm"    ) ||
                      ( currentarg == "-gmekum"   ) ||
                      ( currentarg == "-gmekS"    ) ||
                      ( currentarg == "-gmekA"    ) ||
                      ( currentarg == "-gmekuS"   ) ||
                      ( currentarg == "-gmekMS"   ) ||
                      ( currentarg == "-gmekMA"   ) ||
                      ( currentarg == "-gmekMuS"  ) ||
                      ( currentarg == "-gmekU"    ) ||
                      ( currentarg == "-gmekoz"   ) ||
                      ( currentarg == "-gmekOz"   ) ||
                      ( currentarg == "-gmrkn"    ) ||
                      ( currentarg == "-gmrku"    ) ||
                      ( currentarg == "-gmrkss"   ) ||
                      ( currentarg == "-gmrkus"   ) ||
                      ( currentarg == "-gmrknn"   ) ||
                      ( currentarg == "-gmrkuu"   ) ||
                      ( currentarg == "-gmrkc"    ) ||
                      ( currentarg == "-gmrkuc"   ) ||
                      ( currentarg == "-gmrkm"    ) ||
                      ( currentarg == "-gmrkum"   ) ||
                      ( currentarg == "-gmrkS"    ) ||
                      ( currentarg == "-gmrkA"    ) ||
                      ( currentarg == "-gmrkuS"   ) ||
                      ( currentarg == "-gmrkMS"   ) ||
                      ( currentarg == "-gmrkMA"   ) ||
                      ( currentarg == "-gmrkMuS"  ) ||
                      ( currentarg == "-gmrkU"    ) ||
                      ( currentarg == "-gmrkoz"   ) ||
                      ( currentarg == "-gmrkOz"   ) ||
                      ( currentarg == "-gmmtb"   ) ||
                      ( currentarg == "-gmbmx"   ) ||
                      ( currentarg == "-gmskn"   ) ||
                      ( currentarg == "-gmsku"   ) ||
                      ( currentarg == "-gmskss"  ) ||
                      ( currentarg == "-gmskus"  ) ||
                      ( currentarg == "-gmsknn"  ) ||
                      ( currentarg == "-gmskuu"  ) ||
                      ( currentarg == "-gmskc"   ) ||
                      ( currentarg == "-gmskuc"  ) ||
                      ( currentarg == "-gmskm"   ) ||
                      ( currentarg == "-gmskum"  ) ||
                      ( currentarg == "-gmskS"   ) ||
                      ( currentarg == "-gmskA"   ) ||
                      ( currentarg == "-gmskuS"  ) ||
                      ( currentarg == "-gmskMS"  ) ||
                      ( currentarg == "-gmskMA"  ) ||
                      ( currentarg == "-gmskMuS" ) ||
                      ( currentarg == "-gmskU"   ) ||
                      ( currentarg == "-gmskoz"  ) ||
                      ( currentarg == "-gmskOz"  ) ||
                      ( currentarg == "-gmsekn"   ) ||
                      ( currentarg == "-gmseku"   ) ||
                      ( currentarg == "-gmsekss"  ) ||
                      ( currentarg == "-gmsekus"  ) ||
                      ( currentarg == "-gmseknn"  ) ||
                      ( currentarg == "-gmsekuu"  ) ||
                      ( currentarg == "-gmsekc"   ) ||
                      ( currentarg == "-gmsekuc"  ) ||
                      ( currentarg == "-gmsekm"   ) ||
                      ( currentarg == "-gmsekum"  ) ||
                      ( currentarg == "-gmsekS"   ) ||
                      ( currentarg == "-gmsekA"   ) ||
                      ( currentarg == "-gmsekuS"  ) ||
                      ( currentarg == "-gmsekMS"  ) ||
                      ( currentarg == "-gmsekMA"  ) ||
                      ( currentarg == "-gmsekMuS" ) ||
                      ( currentarg == "-gmsekU"   ) ||
                      ( currentarg == "-gmsekoz"  ) ||
                      ( currentarg == "-gmsekOz"  ) ||
                      ( currentarg == "-gmsrkn"   ) ||
                      ( currentarg == "-gmsrku"   ) ||
                      ( currentarg == "-gmsrkss"  ) ||
                      ( currentarg == "-gmsrkus"  ) ||
                      ( currentarg == "-gmsrknn"  ) ||
                      ( currentarg == "-gmsrkuu"  ) ||
                      ( currentarg == "-gmsrkc"   ) ||
                      ( currentarg == "-gmsrkuc"  ) ||
                      ( currentarg == "-gmsrkm"   ) ||
                      ( currentarg == "-gmsrkum"  ) ||
                      ( currentarg == "-gmsrkS"   ) ||
                      ( currentarg == "-gmsrkA"   ) ||
                      ( currentarg == "-gmsrkuS"  ) ||
                      ( currentarg == "-gmsrkMS"  ) ||
                      ( currentarg == "-gmsrkMA"  ) ||
                      ( currentarg == "-gmsrkMuS" ) ||
                      ( currentarg == "-gmsrkU"   ) ||
                      ( currentarg == "-gmsrkoz"  ) ||
                      ( currentarg == "-gmsrkOz"  ) ||
                      ( currentarg == "-gmsmtb"  ) ||
                      ( currentarg == "-gmsbmx"  ) ||
                      ( currentarg == "-gmgtkn"   ) ||
                      ( currentarg == "-gmgtku"   ) ||
                      ( currentarg == "-gmgtkss"  ) ||
                      ( currentarg == "-gmgtkus"  ) ||
                      ( currentarg == "-gmgtknn"  ) ||
                      ( currentarg == "-gmgtkuu"  ) ||
                      ( currentarg == "-gmgtkc"   ) ||
                      ( currentarg == "-gmgtkuc"  ) ||
                      ( currentarg == "-gmgtkm"   ) ||
                      ( currentarg == "-gmgtkum"  ) ||
                      ( currentarg == "-gmgtkS"   ) ||
                      ( currentarg == "-gmgtkA"   ) ||
                      ( currentarg == "-gmgtkuS"  ) ||
                      ( currentarg == "-gmgtkMS"  ) ||
                      ( currentarg == "-gmgtkMA"  ) ||
                      ( currentarg == "-gmgtkMuS" ) ||
                      ( currentarg == "-gmgtkU"   ) ||
                      ( currentarg == "-gmgtkoz"  ) ||
                      ( currentarg == "-gmgtkOz"  ) ||
                      ( currentarg == "-gmgtekn"   ) ||
                      ( currentarg == "-gmgteku"   ) ||
                      ( currentarg == "-gmgtekss"  ) ||
                      ( currentarg == "-gmgtekus"  ) ||
                      ( currentarg == "-gmgteknn"  ) ||
                      ( currentarg == "-gmgtekuu"  ) ||
                      ( currentarg == "-gmgtekc"   ) ||
                      ( currentarg == "-gmgtekuc"  ) ||
                      ( currentarg == "-gmgtekm"   ) ||
                      ( currentarg == "-gmgtekum"  ) ||
                      ( currentarg == "-gmgtekS"   ) ||
                      ( currentarg == "-gmgtekA"   ) ||
                      ( currentarg == "-gmgtekuS"  ) ||
                      ( currentarg == "-gmgtekMS"  ) ||
                      ( currentarg == "-gmgtekMA"  ) ||
                      ( currentarg == "-gmgtekMuS" ) ||
                      ( currentarg == "-gmgtekU"   ) ||
                      ( currentarg == "-gmgtekoz"  ) ||
                      ( currentarg == "-gmgtekOz"  ) ||
                      ( currentarg == "-gmgtrkn"   ) ||
                      ( currentarg == "-gmgtrku"   ) ||
                      ( currentarg == "-gmgtrkss"  ) ||
                      ( currentarg == "-gmgtrkus"  ) ||
                      ( currentarg == "-gmgtrknn"  ) ||
                      ( currentarg == "-gmgtrkuu"  ) ||
                      ( currentarg == "-gmgtrkc"   ) ||
                      ( currentarg == "-gmgtrkuc"  ) ||
                      ( currentarg == "-gmgtrkm"   ) ||
                      ( currentarg == "-gmgtrkum"  ) ||
                      ( currentarg == "-gmgtrkS"   ) ||
                      ( currentarg == "-gmgtrkA"   ) ||
                      ( currentarg == "-gmgtrkuS"  ) ||
                      ( currentarg == "-gmgtrkMS"  ) ||
                      ( currentarg == "-gmgtrkMA"  ) ||
                      ( currentarg == "-gmgtrkMuS" ) ||
                      ( currentarg == "-gmgtrkU"   ) ||
                      ( currentarg == "-gmgtrkoz"  ) ||
                      ( currentarg == "-gmgtrkOz"  ) ||
                      ( currentarg == "-gmgtmtb"  ) ||
                      ( currentarg == "-gmgtbmx"  )    )
            {
                // Learning options

                if ( grabargs(1,gridopt,commstack,currentarg) )
                {
                    retval  = 73;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-gy"    ) ||
                      ( currentarg == "-gao"   ) ||
                      ( currentarg == "-gr"    ) ||
                      ( currentarg == "-gpr"   ) ||
                      ( currentarg == "-gnr"   ) ||
                      ( currentarg == "-gp"    ) ||
                      ( currentarg == "-gP"    ) ||
                      ( currentarg == "-gpd"   ) ||
                      ( currentarg == "-gf"    ) ||
                      ( currentarg == "-gc"    ) ||
                      ( currentarg == "-gC"    ) ||
                      ( currentarg == "-g+"    ) ||
                      ( currentarg == "-gpln"  ) ||
                      ( currentarg == "-gpld"  ) ||
                      ( currentarg == "-gplT"  ) ||
                      ( currentarg == "-gplt"  ) ||
                      ( currentarg == "-gplm"  ) ||
                      ( currentarg == "-gplM"  ) ||
                      ( currentarg == "-gns"   ) ||
                      ( currentarg == "-gkm"   ) ||
                      ( currentarg == "-gkt"   ) ||
                      ( currentarg == "-gfm"   ) ||
                      ( currentarg == "-gfu"   ) ||
                      ( currentarg == "-gfM"   ) ||
                      ( currentarg == "-gfU"   ) ||
                      ( currentarg == "-gxs"   ) ||
                      ( currentarg == "-ggm"   ) ||
                      ( currentarg == "-ggi"   ) ||
                      ( currentarg == "-gdc"   ) ||
                      ( currentarg == "-gdf"   ) ||
                      ( currentarg == "-gde"   ) ||
                      ( currentarg == "-gda"   ) ||
                      ( currentarg == "-gbH"   ) ||
                      ( currentarg == "-gbs"   ) ||
                      ( currentarg == "-gbm"   ) ||
                      ( currentarg == "-gbj"   ) ||
                      ( currentarg == "-gba"   ) ||
                      ( currentarg == "-gbb"   ) ||
                      ( currentarg == "-gbt"   ) ||
                      ( currentarg == "-gbe"   ) ||
                      ( currentarg == "-gbts"  ) ||
                      ( currentarg == "-gbTm"  ) ||
                      ( currentarg == "-gbTs"  ) ||
                      ( currentarg == "-gbTx"  ) ||
                      ( currentarg == "-gbTv"  ) ||
                      ( currentarg == "-gbeu"  ) ||
                      ( currentarg == "-gbTn"  ) ||
                      ( currentarg == "-gtp"   ) ||
                      ( currentarg == "-gtP"   ) ||
                      ( currentarg == "-gtx"   ) ||
                      ( currentarg == "-gref"  ) ||
                      ( currentarg == "-gbpp"  ) ||
                      ( currentarg == "-gbfid" ) ||
                      ( currentarg == "-gbfn"  ) ||
                      ( currentarg == "-gbfo"  ) ||
                      ( currentarg == "-gbfp"  ) ||
                      ( currentarg == "-gbfv"  ) ||
                      ( currentarg == "-gbfb"  ) ||
                      ( currentarg == "-gbfk"  ) ||
                      ( currentarg == "-gbz"   ) ||
                      ( currentarg == "-gbZ"   ) ||
                      ( currentarg == "-gpB"   ) ||
                      ( currentarg == "-gmx"   ) ||
                      ( currentarg == "-gmxa"  ) ||
                      ( currentarg == "-gmxb"  ) ||
                      ( currentarg == "-gmy"   ) ||
                      ( currentarg == "-gmya"  ) ||
                      ( currentarg == "-gmyb"  ) ||
                      ( currentarg == "-gbD"   ) ||
                      ( currentarg == "-gbzz"  ) ||
                      ( currentarg == "-gbk"   ) ||
                      ( currentarg == "-gbx"   ) ||
                      ( currentarg == "-gbo"   ) ||
                      ( currentarg == "-gbB"   ) ||
                      ( currentarg == "-gbr"   ) ||
                      ( currentarg == "-gbRR"  ) ||
                      ( currentarg == "-gbBB"  ) ||
                      ( currentarg == "-gbu"   ) ||
                      ( currentarg == "-gbv"   ) ||
                      ( currentarg == "-gmw"   ) ||
                      ( currentarg == "-gmsc"  ) ||
                      ( currentarg == "-gmsn"  ) ||
                      ( currentarg == "-gmsy"  ) ||
                      ( currentarg == "-gmsw"  ) ||
                      ( currentarg == "-gmsa"  ) ||
                      ( currentarg == "-gmsd"  ) ||
                      ( currentarg == "-gmsg"  ) ||
                      ( currentarg == "-gmsgg" ) ||
                      ( currentarg == "-gmsmd" ) ||
                      ( currentarg == "-gmgtc"  ) ||
                      ( currentarg == "-gmgtn"  ) ||
                      ( currentarg == "-gmgty"  ) ||
                      ( currentarg == "-gmgtw"  ) ||
                      ( currentarg == "-gmgta"  ) ||
                      ( currentarg == "-gmgtd"  ) ||
                      ( currentarg == "-gmgtg"  ) ||
                      ( currentarg == "-gmgtgg" ) ||
                      ( currentarg == "-gmgtmd" ) ||
                      ( currentarg == "-gmn"   ) ||
                      ( currentarg == "-gmLf"  ) ||
                      ( currentarg == "-gmLF"  ) ||
                      ( currentarg == "-gmLn"  ) ||
                      ( currentarg == "-gmhplb") ||
                      ( currentarg == "-gmt"   ) ||
                      ( currentarg == "-gmrff" ) ||
                      ( currentarg == "-gmq"   ) ||
                      ( currentarg == "-gbim"  ) ||
                      ( currentarg == "-gbq"   ) ||
                      ( currentarg == "-gma"   ) ||
                      ( currentarg == "-gmT"   ) ||
                      ( currentarg == "-gmd"   ) ||
                      ( currentarg == "-gmbgn" ) ||
                      ( currentarg == "-gmsbgn" ) ||
                      ( currentarg == "-gmgtbgn" ) ||
                      ( currentarg == "-gmg"   ) ||
                      ( currentarg == "-gmgg"  ) ||
                      ( currentarg == "-gmma"  ) ||
                      ( currentarg == "-gmmb"  ) ||
                      ( currentarg == "-gmmc"  ) ||
                      ( currentarg == "-gmmd"  ) ||
                      ( currentarg == "-gbG"   ) ||
                      ( currentarg == "-gbmm"  ) ||
                      ( currentarg == "-gbpd"  ) ||
                      ( currentarg == "-gbp"   ) ||
                      ( currentarg == "-gbpl"  ) ||
                      ( currentarg == "-gbpu"  ) ||
                      ( currentarg == "-gdy"   ) ||
                      ( currentarg == "-gNa"   ) ||
                      ( currentarg == "-gNb"   ) ||
                      ( currentarg == "-gNc"   ) ||
                      ( currentarg == "-gNd"   ) ||
                      ( currentarg == "-gNg"   ) ||
                      ( currentarg == "-gNe"   ) ||
                      ( currentarg == "-gNf"   ) ||
                      ( currentarg == "-gBy"   ) ||
                      ( currentarg == "-gBfm"  ) ||
                      ( currentarg == "-gBfM"  ) ||
                      ( currentarg == "-gBfU"  ) ||
                      ( currentarg == "-gBdc"  ) ||
                      ( currentarg == "-gBdf"  ) ||
                      ( currentarg == "-gBde"  ) ||
                      ( currentarg == "-gBda"  ) ||
                      ( currentarg == "-gBdy"  ) ||
                      ( currentarg == "-gBbj"  ) ||
                      ( currentarg == "-gBbt"  ) ||
                      ( currentarg == "-gBbH"  ) ||
                      ( currentarg == "-gbsp"  ) ||
                      ( currentarg == "-gbsP"  ) ||
                      ( currentarg == "-gbsA"  ) ||
                      ( currentarg == "-gbsB"  ) ||
                      ( currentarg == "-gbsF"  ) ||
                      ( currentarg == "-gbsr"  ) ||
                      ( currentarg == "-gbsz"  ) ||
                      ( currentarg == "-gbss"  ) ||
                      ( currentarg == "-gbst"  ) ||
                      ( currentarg == "-gbuu"  ) ||
                      ( currentarg == "-gbuk"  ) ||
                      ( currentarg == "-gbuS"  ) ||
                      ( currentarg == "-gphks"   ) ||
                      ( currentarg == "-gphki"   ) ||
                      ( currentarg == "-gphka"   ) ||
                      ( currentarg == "-gphkb"   ) ||
                      ( currentarg == "-gphke"   ) ||
                      ( currentarg == "-gphkw"   ) ||
                      ( currentarg == "-gphkwlb"   ) ||
                      ( currentarg == "-gphkwub"   ) ||
                      ( currentarg == "-gphkt"   ) ||
                      ( currentarg == "-gphktx"  ) ||
                      ( currentarg == "-gphktk"  ) ||
                      ( currentarg == "-gphkgg"  ) ||
                      ( currentarg == "-gphkf"   ) ||
                      ( currentarg == "-gphkr"   ) ||
                      ( currentarg == "-gphkg"   ) ||
                      ( currentarg == "-gphkd"   ) ||
                      ( currentarg == "-gphkG"   ) ||
                      ( currentarg == "-gphkrlb"   ) ||
                      ( currentarg == "-gphkglb"   ) ||
                      ( currentarg == "-gphkdlb"   ) ||
                      ( currentarg == "-gphkGlb"   ) ||
                      ( currentarg == "-gphkrub"   ) ||
                      ( currentarg == "-gphkgub"   ) ||
                      ( currentarg == "-gphkdub"   ) ||
                      ( currentarg == "-gphkGub"   ) ||
                      ( currentarg == "-gphkI"   ) ||
                      ( currentarg == "-gphkan"  ) ||
                      ( currentarg == "-gPks"    ) ||
                      ( currentarg == "-gPki"    ) ||
                      ( currentarg == "-gPka"    ) ||
                      ( currentarg == "-gPkb"    ) ||
                      ( currentarg == "-gPke"    ) ||
                      ( currentarg == "-gPkw"    ) ||
                      ( currentarg == "-gPkwlb"    ) ||
                      ( currentarg == "-gPkwub"    ) ||
                      ( currentarg == "-gPkt"    ) ||
                      ( currentarg == "-gPktx"   ) ||
                      ( currentarg == "-gPktk"   ) ||
                      ( currentarg == "-gPkgg"   ) ||
                      ( currentarg == "-gPkf"    ) ||
                      ( currentarg == "-gPkr"    ) ||
                      ( currentarg == "-gPkg"    ) ||
                      ( currentarg == "-gPkd"    ) ||
                      ( currentarg == "-gPkG"    ) ||
                      ( currentarg == "-gPkrlb"    ) ||
                      ( currentarg == "-gPkglb"    ) ||
                      ( currentarg == "-gPkdlb"    ) ||
                      ( currentarg == "-gPkGlb"    ) ||
                      ( currentarg == "-gPkrub"    ) ||
                      ( currentarg == "-gPkgub"    ) ||
                      ( currentarg == "-gPkdub"    ) ||
                      ( currentarg == "-gPkGub"    ) ||
                      ( currentarg == "-gPkI"    ) ||
                      ( currentarg == "-gPkan"   ) ||
                      ( currentarg == "-gmks"    ) ||
                      ( currentarg == "-gmki"    ) ||
                      ( currentarg == "-gmka"    ) ||
                      ( currentarg == "-gmkb"    ) ||
                      ( currentarg == "-gmke"    ) ||
                      ( currentarg == "-gmkw"    ) ||
                      ( currentarg == "-gmkwlb"    ) ||
                      ( currentarg == "-gmkwub"    ) ||
                      ( currentarg == "-gmkt"    ) ||
                      ( currentarg == "-gmktx"   ) ||
                      ( currentarg == "-gmktk"   ) ||
                      ( currentarg == "-gmkgg"   ) ||
                      ( currentarg == "-gmkf"    ) ||
                      ( currentarg == "-gmkr"    ) ||
                      ( currentarg == "-gmkg"    ) ||
                      ( currentarg == "-gmkd"    ) ||
                      ( currentarg == "-gmkG"    ) ||
                      ( currentarg == "-gmkrlb"    ) ||
                      ( currentarg == "-gmkglb"    ) ||
                      ( currentarg == "-gmkdlb"    ) ||
                      ( currentarg == "-gmkGlb"    ) ||
                      ( currentarg == "-gmkrub"    ) ||
                      ( currentarg == "-gmkgub"    ) ||
                      ( currentarg == "-gmkdub"    ) ||
                      ( currentarg == "-gmkGub"    ) ||
                      ( currentarg == "-gmkI"    ) ||
                      ( currentarg == "-gmkan"   ) ||
                      ( currentarg == "-gmeks"    ) ||
                      ( currentarg == "-gmeki"    ) ||
                      ( currentarg == "-gmeka"    ) ||
                      ( currentarg == "-gmekb"    ) ||
                      ( currentarg == "-gmeke"    ) ||
                      ( currentarg == "-gmekw"    ) ||
                      ( currentarg == "-gmekwlb"    ) ||
                      ( currentarg == "-gmekwub"    ) ||
                      ( currentarg == "-gmekt"    ) ||
                      ( currentarg == "-gmektx"   ) ||
                      ( currentarg == "-gmektk"   ) ||
                      ( currentarg == "-gmekgg"   ) ||
                      ( currentarg == "-gmekf"    ) ||
                      ( currentarg == "-gmekr"    ) ||
                      ( currentarg == "-gmekg"    ) ||
                      ( currentarg == "-gmekd"    ) ||
                      ( currentarg == "-gmekG"    ) ||
                      ( currentarg == "-gmekrlb"    ) ||
                      ( currentarg == "-gmekglb"    ) ||
                      ( currentarg == "-gmekdlb"    ) ||
                      ( currentarg == "-gmekGlb"    ) ||
                      ( currentarg == "-gmekrub"    ) ||
                      ( currentarg == "-gmekgub"    ) ||
                      ( currentarg == "-gmekdub"    ) ||
                      ( currentarg == "-gmekGub"    ) ||
                      ( currentarg == "-gmekI"    ) ||
                      ( currentarg == "-gmekan"   ) ||
                      ( currentarg == "-gmrks"    ) ||
                      ( currentarg == "-gmrki"    ) ||
                      ( currentarg == "-gmrka"    ) ||
                      ( currentarg == "-gmrkb"    ) ||
                      ( currentarg == "-gmrke"    ) ||
                      ( currentarg == "-gmrkw"    ) ||
                      ( currentarg == "-gmrkwlb"    ) ||
                      ( currentarg == "-gmrkwub"    ) ||
                      ( currentarg == "-gmrkt"    ) ||
                      ( currentarg == "-gmrktx"   ) ||
                      ( currentarg == "-gmrktk"   ) ||
                      ( currentarg == "-gmrkgg"   ) ||
                      ( currentarg == "-gmrkf"    ) ||
                      ( currentarg == "-gmrkr"    ) ||
                      ( currentarg == "-gmrkg"    ) ||
                      ( currentarg == "-gmrkd"    ) ||
                      ( currentarg == "-gmrkG"    ) ||
                      ( currentarg == "-gmrkrlb"    ) ||
                      ( currentarg == "-gmrkglb"    ) ||
                      ( currentarg == "-gmrkdlb"    ) ||
                      ( currentarg == "-gmrkGlb"    ) ||
                      ( currentarg == "-gmrkrub"    ) ||
                      ( currentarg == "-gmrkgub"    ) ||
                      ( currentarg == "-gmrkdub"    ) ||
                      ( currentarg == "-gmrkGub"    ) ||
                      ( currentarg == "-gmrkI"    ) ||
                      ( currentarg == "-gmrkan"   ) ||
                      ( currentarg == "-gmsks"   ) ||
                      ( currentarg == "-gmski"   ) ||
                      ( currentarg == "-gmska"   ) ||
                      ( currentarg == "-gmskb"   ) ||
                      ( currentarg == "-gmske"   ) ||
                      ( currentarg == "-gmskw"   ) ||
                      ( currentarg == "-gmskwlb"   ) ||
                      ( currentarg == "-gmskwub"   ) ||
                      ( currentarg == "-gmskt"   ) ||
                      ( currentarg == "-gmsktx"  ) ||
                      ( currentarg == "-gmsktk"  ) ||
                      ( currentarg == "-gmskgg"  ) ||
                      ( currentarg == "-gmskf"   ) ||
                      ( currentarg == "-gmskr"   ) ||
                      ( currentarg == "-gmskg"   ) ||
                      ( currentarg == "-gmskd"   ) ||
                      ( currentarg == "-gmskG"   ) ||
                      ( currentarg == "-gmskrlb"   ) ||
                      ( currentarg == "-gmskglb"   ) ||
                      ( currentarg == "-gmskdlb"   ) ||
                      ( currentarg == "-gmskGlb"   ) ||
                      ( currentarg == "-gmskrub"   ) ||
                      ( currentarg == "-gmskgub"   ) ||
                      ( currentarg == "-gmskdub"   ) ||
                      ( currentarg == "-gmskGub"   ) ||
                      ( currentarg == "-gmskI"   ) ||
                      ( currentarg == "-gmskan"  ) ||
                      ( currentarg == "-gmseks"   ) ||
                      ( currentarg == "-gmseki"   ) ||
                      ( currentarg == "-gmseka"   ) ||
                      ( currentarg == "-gmsekb"   ) ||
                      ( currentarg == "-gmseke"   ) ||
                      ( currentarg == "-gmsekw"   ) ||
                      ( currentarg == "-gmsekwlb"   ) ||
                      ( currentarg == "-gmsekwub"   ) ||
                      ( currentarg == "-gmsekt"   ) ||
                      ( currentarg == "-gmsektx"  ) ||
                      ( currentarg == "-gmsektk"  ) ||
                      ( currentarg == "-gmsekgg"  ) ||
                      ( currentarg == "-gmsekf"   ) ||
                      ( currentarg == "-gmsekr"   ) ||
                      ( currentarg == "-gmsekg"   ) ||
                      ( currentarg == "-gmsekd"   ) ||
                      ( currentarg == "-gmsekG"   ) ||
                      ( currentarg == "-gmsekrlb"   ) ||
                      ( currentarg == "-gmsekglb"   ) ||
                      ( currentarg == "-gmsekdlb"   ) ||
                      ( currentarg == "-gmsekGlb"   ) ||
                      ( currentarg == "-gmsekrub"   ) ||
                      ( currentarg == "-gmsekgub"   ) ||
                      ( currentarg == "-gmsekdub"   ) ||
                      ( currentarg == "-gmsekGub"   ) ||
                      ( currentarg == "-gmsekI"   ) ||
                      ( currentarg == "-gmsekan"  ) ||
                      ( currentarg == "-gmsrks"   ) ||
                      ( currentarg == "-gmsrki"   ) ||
                      ( currentarg == "-gmsrka"   ) ||
                      ( currentarg == "-gmsrkb"   ) ||
                      ( currentarg == "-gmsrke"   ) ||
                      ( currentarg == "-gmsrkw"   ) ||
                      ( currentarg == "-gmsrkwlb"   ) ||
                      ( currentarg == "-gmsrkwub"   ) ||
                      ( currentarg == "-gmsrkt"   ) ||
                      ( currentarg == "-gmsrktx"  ) ||
                      ( currentarg == "-gmsrktk"  ) ||
                      ( currentarg == "-gmsrkgg"  ) ||
                      ( currentarg == "-gmsrkf"   ) ||
                      ( currentarg == "-gmsrkr"   ) ||
                      ( currentarg == "-gmsrkg"   ) ||
                      ( currentarg == "-gmsrkd"   ) ||
                      ( currentarg == "-gmsrkG"   ) ||
                      ( currentarg == "-gmsrkrlb"   ) ||
                      ( currentarg == "-gmsrkglb"   ) ||
                      ( currentarg == "-gmsrkdlb"   ) ||
                      ( currentarg == "-gmsrkGlb"   ) ||
                      ( currentarg == "-gmsrkrub"   ) ||
                      ( currentarg == "-gmsrkgub"   ) ||
                      ( currentarg == "-gmsrkdub"   ) ||
                      ( currentarg == "-gmsrkGub"   ) ||
                      ( currentarg == "-gmsrkI"   ) ||
                      ( currentarg == "-gmsrkan"  ) ||
                      ( currentarg == "-gmgtks"   ) ||
                      ( currentarg == "-gmgtki"   ) ||
                      ( currentarg == "-gmgtka"   ) ||
                      ( currentarg == "-gmgtkb"   ) ||
                      ( currentarg == "-gmgtke"   ) ||
                      ( currentarg == "-gmgtkw"   ) ||
                      ( currentarg == "-gmgtkwlb"   ) ||
                      ( currentarg == "-gmgtkwub"   ) ||
                      ( currentarg == "-gmgtkt"   ) ||
                      ( currentarg == "-gmgtktx"  ) ||
                      ( currentarg == "-gmgtktk"  ) ||
                      ( currentarg == "-gmgtkgg"  ) ||
                      ( currentarg == "-gmgtkf"   ) ||
                      ( currentarg == "-gmgtkr"   ) ||
                      ( currentarg == "-gmgtkg"   ) ||
                      ( currentarg == "-gmgtkd"   ) ||
                      ( currentarg == "-gmgtkG"   ) ||
                      ( currentarg == "-gmgtkrlb"   ) ||
                      ( currentarg == "-gmgtkglb"   ) ||
                      ( currentarg == "-gmgtkdlb"   ) ||
                      ( currentarg == "-gmgtkGlb"   ) ||
                      ( currentarg == "-gmgtkrub"   ) ||
                      ( currentarg == "-gmgtkgub"   ) ||
                      ( currentarg == "-gmgtkdub"   ) ||
                      ( currentarg == "-gmgtkGub"   ) ||
                      ( currentarg == "-gmgtkI"   ) ||
                      ( currentarg == "-gmgtkan"  ) ||
                      ( currentarg == "-gmgteks"   ) ||
                      ( currentarg == "-gmgteki"   ) ||
                      ( currentarg == "-gmgteka"   ) ||
                      ( currentarg == "-gmgtekb"   ) ||
                      ( currentarg == "-gmgteke"   ) ||
                      ( currentarg == "-gmgtekw"   ) ||
                      ( currentarg == "-gmgtekwlb"   ) ||
                      ( currentarg == "-gmgtekwub"   ) ||
                      ( currentarg == "-gmgtekt"   ) ||
                      ( currentarg == "-gmgtektx"  ) ||
                      ( currentarg == "-gmgtektk"  ) ||
                      ( currentarg == "-gmgtekgg"  ) ||
                      ( currentarg == "-gmgtekf"   ) ||
                      ( currentarg == "-gmgtekr"   ) ||
                      ( currentarg == "-gmgtekg"   ) ||
                      ( currentarg == "-gmgtekd"   ) ||
                      ( currentarg == "-gmgtekG"   ) ||
                      ( currentarg == "-gmgtekrlb"   ) ||
                      ( currentarg == "-gmgtekglb"   ) ||
                      ( currentarg == "-gmgtekdlb"   ) ||
                      ( currentarg == "-gmgtekGlb"   ) ||
                      ( currentarg == "-gmgtekrub"   ) ||
                      ( currentarg == "-gmgtekgub"   ) ||
                      ( currentarg == "-gmgtekdub"   ) ||
                      ( currentarg == "-gmgtekGub"   ) ||
                      ( currentarg == "-gmgtekI"   ) ||
                      ( currentarg == "-gmgtekan"  ) ||
                      ( currentarg == "-gmgtrks"   ) ||
                      ( currentarg == "-gmgtrki"   ) ||
                      ( currentarg == "-gmgtrka"   ) ||
                      ( currentarg == "-gmgtrkb"   ) ||
                      ( currentarg == "-gmgtrke"   ) ||
                      ( currentarg == "-gmgtrkw"   ) ||
                      ( currentarg == "-gmgtrkwlb"   ) ||
                      ( currentarg == "-gmgtrkwub"   ) ||
                      ( currentarg == "-gmgtrkt"   ) ||
                      ( currentarg == "-gmgtrktx"  ) ||
                      ( currentarg == "-gmgtrktk"  ) ||
                      ( currentarg == "-gmgtrkgg"  ) ||
                      ( currentarg == "-gmgtrkf"   ) ||
                      ( currentarg == "-gmgtrkr"   ) ||
                      ( currentarg == "-gmgtrkg"   ) ||
                      ( currentarg == "-gmgtrkd"   ) ||
                      ( currentarg == "-gmgtrkG"   ) ||
                      ( currentarg == "-gmgtrkrlb"   ) ||
                      ( currentarg == "-gmgtrkglb"   ) ||
                      ( currentarg == "-gmgtrkdlb"   ) ||
                      ( currentarg == "-gmgtrkGlb"   ) ||
                      ( currentarg == "-gmgtrkrub"   ) ||
                      ( currentarg == "-gmgtrkgub"   ) ||
                      ( currentarg == "-gmgtrkdub"   ) ||
                      ( currentarg == "-gmgtrkGub"   ) ||
                      ( currentarg == "-gmgtrkI"   ) ||
                      ( currentarg == "-gmgtrkan"  )    )
            {
                // Learning options

                if ( grabargs(2,gridopt,commstack,currentarg) )
                {
                    retval  = 74;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-gphkv" ) ||
                      ( currentarg == "-gphkV" ) ||
                      ( currentarg == "-gphkvlb" ) ||
                      ( currentarg == "-gphkVlb" ) ||
                      ( currentarg == "-gphkvub" ) ||
                      ( currentarg == "-gphkVub" ) ||
                      ( currentarg == "-gphko" ) ||
                      ( currentarg == "-gphkO" ) ||
                      ( currentarg == "-gphkx" ) ||
                      ( currentarg == "-gPkv"  ) ||
                      ( currentarg == "-gPkV"  ) ||
                      ( currentarg == "-gPkvlb"  ) ||
                      ( currentarg == "-gPkVlb"  ) ||
                      ( currentarg == "-gPkvub"  ) ||
                      ( currentarg == "-gPkVub"  ) ||
                      ( currentarg == "-gPko"  ) ||
                      ( currentarg == "-gPkO"  ) ||
                      ( currentarg == "-gPkx"  ) ||
                      ( currentarg == "-gmkv"  ) ||
                      ( currentarg == "-gmkV"  ) ||
                      ( currentarg == "-gmkvlb"  ) ||
                      ( currentarg == "-gmkVlb"  ) ||
                      ( currentarg == "-gmkvub"  ) ||
                      ( currentarg == "-gmkVub"  ) ||
                      ( currentarg == "-gmko"  ) ||
                      ( currentarg == "-gmkO"  ) ||
                      ( currentarg == "-gmkx"  ) ||
                      ( currentarg == "-gmekv"  ) ||
                      ( currentarg == "-gmekV"  ) ||
                      ( currentarg == "-gmekvlb"  ) ||
                      ( currentarg == "-gmekVlb"  ) ||
                      ( currentarg == "-gmekvub"  ) ||
                      ( currentarg == "-gmekVub"  ) ||
                      ( currentarg == "-gmeko"  ) ||
                      ( currentarg == "-gmekO"  ) ||
                      ( currentarg == "-gmekx"  ) ||
                      ( currentarg == "-gmrkv"  ) ||
                      ( currentarg == "-gmrkV"  ) ||
                      ( currentarg == "-gmrkvlb"  ) ||
                      ( currentarg == "-gmrkVlb"  ) ||
                      ( currentarg == "-gmrkvub"  ) ||
                      ( currentarg == "-gmrkVub"  ) ||
                      ( currentarg == "-gmrko"  ) ||
                      ( currentarg == "-gmrkO"  ) ||
                      ( currentarg == "-gmrkx"  ) ||
                      ( currentarg == "-gmskv" ) ||
                      ( currentarg == "-gmskV" ) ||
                      ( currentarg == "-gmskvlb" ) ||
                      ( currentarg == "-gmskVlb" ) ||
                      ( currentarg == "-gmskvub" ) ||
                      ( currentarg == "-gmskVub" ) ||
                      ( currentarg == "-gmsko" ) ||
                      ( currentarg == "-gmskO" ) ||
                      ( currentarg == "-gmskx" ) ||
                      ( currentarg == "-gmsekv" ) ||
                      ( currentarg == "-gmsekV" ) ||
                      ( currentarg == "-gmsekvlb" ) ||
                      ( currentarg == "-gmsekVlb" ) ||
                      ( currentarg == "-gmsekvub" ) ||
                      ( currentarg == "-gmsekVub" ) ||
                      ( currentarg == "-gmseko" ) ||
                      ( currentarg == "-gmsekO" ) ||
                      ( currentarg == "-gmsekx" ) ||
                      ( currentarg == "-gmsrkv" ) ||
                      ( currentarg == "-gmsrkV" ) ||
                      ( currentarg == "-gmsrkvlb" ) ||
                      ( currentarg == "-gmsrkVlb" ) ||
                      ( currentarg == "-gmsrkvub" ) ||
                      ( currentarg == "-gmsrkVub" ) ||
                      ( currentarg == "-gmsrko" ) ||
                      ( currentarg == "-gmsrkO" ) ||
                      ( currentarg == "-gmsrkx" ) ||
                      ( currentarg == "-gmgtkv" ) ||
                      ( currentarg == "-gmgtkV" ) ||
                      ( currentarg == "-gmgtkvlb" ) ||
                      ( currentarg == "-gmgtkVlb" ) ||
                      ( currentarg == "-gmgtkvub" ) ||
                      ( currentarg == "-gmgtkVub" ) ||
                      ( currentarg == "-gmgtko" ) ||
                      ( currentarg == "-gmgtkO" ) ||
                      ( currentarg == "-gmgtkx" ) ||
                      ( currentarg == "-gmgtekv" ) ||
                      ( currentarg == "-gmgtekV" ) ||
                      ( currentarg == "-gmgtekvlb" ) ||
                      ( currentarg == "-gmgtekVlb" ) ||
                      ( currentarg == "-gmgtekvub" ) ||
                      ( currentarg == "-gmgtekVub" ) ||
                      ( currentarg == "-gmgteko" ) ||
                      ( currentarg == "-gmgtekO" ) ||
                      ( currentarg == "-gmgtekx" ) ||
                      ( currentarg == "-gmgtrkv" ) ||
                      ( currentarg == "-gmgtrkV" ) ||
                      ( currentarg == "-gmgtrkvlb" ) ||
                      ( currentarg == "-gmgtrkVlb" ) ||
                      ( currentarg == "-gmgtrkvub" ) ||
                      ( currentarg == "-gmgtrkVub" ) ||
                      ( currentarg == "-gmgtrko" ) ||
                      ( currentarg == "-gmgtrkO" ) ||
                      ( currentarg == "-gmgtrkx" )    )
            {
                // Learning options

                if ( grabargs(3,gridopt,commstack,currentarg) )
                {
                    retval  = 74;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-xi"  ) ||
                      ( currentarg == "-xo"  ) ||
                      ( currentarg == "-xh"  ) ||
                      ( currentarg == "-xC"  ) ||
                      ( currentarg == "-xR"  ) ||
                      ( currentarg == "-xt"  ) ||
                      ( currentarg == "-xr"  ) ||
                      ( currentarg == "-xrv" ) ||
                      ( currentarg == "-xs"  ) ||
                      ( currentarg == "-xa"  ) ||
                      ( currentarg == "-xl"  )    )
            {
                // Kernel transfer options

                if ( grabargs(2,xferopt,commstack,currentarg) )
                {
                    retval  = 75;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( currentarg == "-x" )
            {
                // Kernel transfer options

                if ( grabargs(4,xferopt,commstack,currentarg) )
                {
                    retval  = 75;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fsx"   ) ||
                      ( currentarg == "-fsxz"  ) ||
                      ( currentarg == "-fsxB"  ) ||
                      ( currentarg == "-fsxzB" ) ||
                      ( currentarg == "-fsr"   ) ||
                      ( currentarg == "-fsrB"  ) ||
                      ( currentarg == "-fSx"   ) ||
                      ( currentarg == "-fSxz"  ) ||
                      ( currentarg == "-fSxB"  ) ||
                      ( currentarg == "-fSxzB" ) ||
                      ( currentarg == "-fSr"   ) ||
                      ( currentarg == "-fSrB"  ) ||
                      ( currentarg == "-fsd"   ) ||
                      ( currentarg == "-fsD"   )    )
            {
                // Feature selection options

                if ( grabargs(1,featureopt,commstack,currentarg) )
                {
                    retval  = 75;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fsc"    ) ||
                      ( currentarg == "-fscz"   ) ||
                      ( currentarg == "-fscB"   ) ||
                      ( currentarg == "-fsczB"  ) ||
                      ( currentarg == "-fSc"    ) ||
                      ( currentarg == "-fScz"   ) ||
                      ( currentarg == "-fScB"   ) ||
                      ( currentarg == "-fSczB"  ) ||
                      ( currentarg == "-fsf"    ) ||
                      ( currentarg == "-fsfe"   ) ||
                      ( currentarg == "-fsfi"   ) ||
                      ( currentarg == "-fsfI"   ) ||
                      ( currentarg == "-fsfu"   ) ||
                      ( currentarg == "-fsfeu"  ) ||
                      ( currentarg == "-fsfiu"  ) ||
                      ( currentarg == "-fsfIu"  ) ||
                      ( currentarg == "-fsfB"   ) ||
                      ( currentarg == "-fsfeB"  ) ||
                      ( currentarg == "-fsfiB"  ) ||
                      ( currentarg == "-fsfIB"  ) ||
                      ( currentarg == "-fsfuB"  ) ||
                      ( currentarg == "-fsfeuB" ) ||
                      ( currentarg == "-fsfiuB" ) ||
                      ( currentarg == "-fsfIuB" ) ||
                      ( currentarg == "-fSf"    ) ||
                      ( currentarg == "-fSfe"   ) ||
                      ( currentarg == "-fSfi"   ) ||
                      ( currentarg == "-fSfI"   ) ||
                      ( currentarg == "-fSfu"   ) ||
                      ( currentarg == "-fSfeu"  ) ||
                      ( currentarg == "-fSfiu"  ) ||
                      ( currentarg == "-fSfIu"  ) ||
                      ( currentarg == "-fSfB"   ) ||
                      ( currentarg == "-fSfeB"  ) ||
                      ( currentarg == "-fSfiB"  ) ||
                      ( currentarg == "-fSfIB"  ) ||
                      ( currentarg == "-fSfuB"  ) ||
                      ( currentarg == "-fSfeuB" ) ||
                      ( currentarg == "-fSfiuB" ) ||
                      ( currentarg == "-fSfIuB" ) ||
                      ( currentarg == "-fss"    )    )
            {
                // Feature selection options

                if ( grabargs(2,featureopt,commstack,currentarg) )
                {
                    retval  = 76;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fsC"    ) ||
                      ( currentarg == "-fsCz"   ) ||
                      ( currentarg == "-fsCB"   ) ||
                      ( currentarg == "-fsCzB"  ) ||
                      ( currentarg == "-fsfl"   ) ||
                      ( currentarg == "-fsfel"  ) ||
                      ( currentarg == "-fsfil"  ) ||
                      ( currentarg == "-fsfIl"  ) ||
                      ( currentarg == "-fsflB"  ) ||
                      ( currentarg == "-fsfelB" ) ||
                      ( currentarg == "-fsfilB" ) ||
                      ( currentarg == "-fsfIlB" ) ||
                      ( currentarg == "-fSC"    ) ||
                      ( currentarg == "-fSCz"   ) ||
                      ( currentarg == "-fSCB"   ) ||
                      ( currentarg == "-fSCzB"  ) ||
                      ( currentarg == "-fSfl"   ) ||
                      ( currentarg == "-fSfel"  ) ||
                      ( currentarg == "-fSfil"  ) ||
                      ( currentarg == "-fSfIl"  ) ||
                      ( currentarg == "-fSflB"  ) ||
                      ( currentarg == "-fSfelB" ) ||
                      ( currentarg == "-fSfilB" ) ||
                      ( currentarg == "-fSfIlB" )    )
            {
                // Feature selection options

                if ( grabargs(3,featureopt,commstack,currentarg) )
                {
                    retval  = 77;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fsF"    ) ||
                      ( currentarg == "-fsFe"   ) ||
                      ( currentarg == "-fsFi"   ) ||
                      ( currentarg == "-fsFI"   ) ||
                      ( currentarg == "-fsFr"   ) ||
                      ( currentarg == "-fsFR"   ) ||
                      ( currentarg == "-fsFu"   ) ||
                      ( currentarg == "-fsFeu"  ) ||
                      ( currentarg == "-fsFiu"  ) ||
                      ( currentarg == "-fsFIu"  ) ||
                      ( currentarg == "-fsFru"  ) ||
                      ( currentarg == "-fsFRu"  ) ||
                      ( currentarg == "-fsFB"   ) ||
                      ( currentarg == "-fsFeB"  ) ||
                      ( currentarg == "-fsFiB"  ) ||
                      ( currentarg == "-fsFIB"  ) ||
                      ( currentarg == "-fsFrB"  ) ||
                      ( currentarg == "-fsFRB"  ) ||
                      ( currentarg == "-fsFuB"  ) ||
                      ( currentarg == "-fsFeuB" ) ||
                      ( currentarg == "-fsFiuB" ) ||
                      ( currentarg == "-fsFIuB" ) ||
                      ( currentarg == "-fsFruB" ) ||
                      ( currentarg == "-fsFRuB" ) ||
                      ( currentarg == "-fSF"    ) ||
                      ( currentarg == "-fSFe"   ) ||
                      ( currentarg == "-fSFi"   ) ||
                      ( currentarg == "-fSFI"   ) ||
                      ( currentarg == "-fSFr"   ) ||
                      ( currentarg == "-fSFR"   ) ||
                      ( currentarg == "-fSFu"   ) ||
                      ( currentarg == "-fSFeu"  ) ||
                      ( currentarg == "-fSFiu"  ) ||
                      ( currentarg == "-fSFIu"  ) ||
                      ( currentarg == "-fSFru"  ) ||
                      ( currentarg == "-fSFRu"  ) ||
                      ( currentarg == "-fSFB"   ) ||
                      ( currentarg == "-fSFeB"  ) ||
                      ( currentarg == "-fSFiB"  ) ||
                      ( currentarg == "-fSFIB"  ) ||
                      ( currentarg == "-fSFrB"  ) ||
                      ( currentarg == "-fSFRB"  ) ||
                      ( currentarg == "-fSFuB"  ) ||
                      ( currentarg == "-fSFeuB" ) ||
                      ( currentarg == "-fSFiuB" ) ||
                      ( currentarg == "-fSFIuB" ) ||
                      ( currentarg == "-fSFruB" ) ||
                      ( currentarg == "-fSFRuB" )    )
            {
                // Feature selection options

                if ( grabargs(4,featureopt,commstack,currentarg) )
                {
                    retval  = 78;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fsFl"   ) ||
                      ( currentarg == "-fsFel"  ) ||
                      ( currentarg == "-fsFil"  ) ||
                      ( currentarg == "-fsFIl"  ) ||
                      ( currentarg == "-fsFrl"  ) ||
                      ( currentarg == "-fsFRl"  ) ||
                      ( currentarg == "-fsFlB"  ) ||
                      ( currentarg == "-fsFelB" ) ||
                      ( currentarg == "-fsFilB" ) ||
                      ( currentarg == "-fsFIlB" ) ||
                      ( currentarg == "-fsFrlB" ) ||
                      ( currentarg == "-fsFRlB" ) ||
                      ( currentarg == "-fSFl"   ) ||
                      ( currentarg == "-fSFel"  ) ||
                      ( currentarg == "-fSFil"  ) ||
                      ( currentarg == "-fSFIl"  ) ||
                      ( currentarg == "-fSFrl"  ) ||
                      ( currentarg == "-fSFRl"  ) ||
                      ( currentarg == "-fSFlB"  ) ||
                      ( currentarg == "-fSFelB" ) ||
                      ( currentarg == "-fSFilB" ) ||
                      ( currentarg == "-fSFIlB" ) ||
                      ( currentarg == "-fSFrlB" ) ||
                      ( currentarg == "-fSFRlB" )    )
            {
                // Feature selection options

                if ( grabargs(5,featureopt,commstack,currentarg) )
                {
                    retval  = 79;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fzt"    ) ||
                      ( currentarg == "-fzs"    ) ||
                      ( currentarg == "-fztf"   ) ||
                      ( currentarg == "-fztm"   ) ||
                      ( currentarg == "-fztNlA" ) ||
                      ( currentarg == "-fzsf"   ) ||
                      ( currentarg == "-fzsm"   ) ||
                      ( currentarg == "-fzsNlA" )    )
            {
                // Fuzzy options

                if ( grabargs(2,fuzzyopt,commstack,currentarg) )
                {
                    retval  = 80;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fztkn"   ) ||
                      ( currentarg == "-fztku"   ) ||
                      ( currentarg == "-fztkss"  ) ||
                      ( currentarg == "-fztkus"  ) ||
                      ( currentarg == "-fztknn"  ) ||
                      ( currentarg == "-fztkuu"  ) ||
                      ( currentarg == "-fztkc"   ) ||
                      ( currentarg == "-fztkuc"  ) ||
                      ( currentarg == "-fztkm"   ) ||
                      ( currentarg == "-fztkum"  ) ||
                      ( currentarg == "-fztkS"   ) ||
                      ( currentarg == "-fztkA"   ) ||
                      ( currentarg == "-fztkuS"  ) ||
                      ( currentarg == "-fztkMS"  ) ||
                      ( currentarg == "-fztkMA"  ) ||
                      ( currentarg == "-fztkMuS" ) ||
                      ( currentarg == "-fztkU"   ) ||
                      ( currentarg == "-fztkoz"  ) ||
                      ( currentarg == "-fztkmtb" ) ||
                      ( currentarg == "-fztkbmx" ) ||
                      ( currentarg == "-fztkOz"  ) ||
                      ( currentarg == "-fzskn"   ) ||
                      ( currentarg == "-fzsku"   ) ||
                      ( currentarg == "-fzskns"  ) ||
                      ( currentarg == "-fzskus"  ) ||
                      ( currentarg == "-fzsknn"  ) ||
                      ( currentarg == "-fzskuu"  ) ||
                      ( currentarg == "-fzskc"   ) ||
                      ( currentarg == "-fzskuc"  ) ||
                      ( currentarg == "-fzskm"   ) ||
                      ( currentarg == "-fzskum"  ) ||
                      ( currentarg == "-fzskS"   ) ||
                      ( currentarg == "-fzskA"   ) ||
                      ( currentarg == "-fzskuS"  ) ||
                      ( currentarg == "-fzskMS"  ) ||
                      ( currentarg == "-fzskMA"  ) ||
                      ( currentarg == "-fzskMuS" ) ||
                      ( currentarg == "-fzskU"   ) ||
                      ( currentarg == "-fzskoz"  ) ||
                      ( currentarg == "-fzskmtb" ) ||
                      ( currentarg == "-fzskbmx" ) ||
                      ( currentarg == "-fzskOz"  )    )
            {
                // Fuzzy options

                if ( grabargs(1,fuzzyopt,commstack,currentarg) )
                {
                    retval  = 81;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fztks"   ) ||
                      ( currentarg == "-fztki"   ) ||
                      ( currentarg == "-fztka"   ) ||
                      ( currentarg == "-fztkb"   ) ||
                      ( currentarg == "-fztke"   ) ||
                      ( currentarg == "-fztkw"   ) ||
                      ( currentarg == "-fztkwlb"   ) ||
                      ( currentarg == "-fztkwub"   ) ||
                      ( currentarg == "-fztkt"   ) ||
                      ( currentarg == "-fztktx"  ) ||
                      ( currentarg == "-fztktk"  ) ||
                      ( currentarg == "-fztkgg"  ) ||
                      ( currentarg == "-fztkf"   ) ||
                      ( currentarg == "-fztkr"   ) ||
                      ( currentarg == "-fztkg"   ) ||
                      ( currentarg == "-fztkd"   ) ||
                      ( currentarg == "-fztkG"   ) ||
                      ( currentarg == "-fztkrlb"   ) ||
                      ( currentarg == "-fztkglb"   ) ||
                      ( currentarg == "-fztkdlb"   ) ||
                      ( currentarg == "-fztkGlb"   ) ||
                      ( currentarg == "-fztkrub"   ) ||
                      ( currentarg == "-fztkgub"   ) ||
                      ( currentarg == "-fztkdub"   ) ||
                      ( currentarg == "-fztkGub"   ) ||
                      ( currentarg == "-fztkI"   ) ||
                      ( currentarg == "-fztkan"  ) ||
                      ( currentarg == "-fzsks"   ) ||
                      ( currentarg == "-fzski"   ) ||
                      ( currentarg == "-fzska"   ) ||
                      ( currentarg == "-fzskb"   ) ||
                      ( currentarg == "-fzske"   ) ||
                      ( currentarg == "-fzskw"   ) ||
                      ( currentarg == "-fzskwlb"   ) ||
                      ( currentarg == "-fzskwub"   ) ||
                      ( currentarg == "-fzskt"   ) ||
                      ( currentarg == "-fzsktx"  ) ||
                      ( currentarg == "-fzsktk"  ) ||
                      ( currentarg == "-fzskgg"  ) ||
                      ( currentarg == "-fzskf"   ) ||
                      ( currentarg == "-fzskr"   ) ||
                      ( currentarg == "-fzskg"   ) ||
                      ( currentarg == "-fzskd"   ) ||
                      ( currentarg == "-fzskG"   ) ||
                      ( currentarg == "-fzskrlb"   ) ||
                      ( currentarg == "-fzskglb"   ) ||
                      ( currentarg == "-fzskdlb"   ) ||
                      ( currentarg == "-fzskGlb"   ) ||
                      ( currentarg == "-fzskrub"   ) ||
                      ( currentarg == "-fzskgub"   ) ||
                      ( currentarg == "-fzskdub"   ) ||
                      ( currentarg == "-fzskGub"   ) ||
                      ( currentarg == "-fzskI"   ) ||
                      ( currentarg == "-fzskan"  )    )
            {
                // Fuzzy options

                if ( grabargs(2,fuzzyopt,commstack,currentarg) )
                {
                    retval  = 82;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-fztkv" ) ||
                      ( currentarg == "-fztkV" ) ||
                      ( currentarg == "-fztkvlb" ) ||
                      ( currentarg == "-fztkVlb" ) ||
                      ( currentarg == "-fztkvub" ) ||
                      ( currentarg == "-fztkVub" ) ||
                      ( currentarg == "-fztko" ) ||
                      ( currentarg == "-fztkO" ) ||
                      ( currentarg == "-fztkx" ) ||
                      ( currentarg == "-fzskv" ) ||
                      ( currentarg == "-fzskV" ) ||
                      ( currentarg == "-fzskvlb" ) ||
                      ( currentarg == "-fzskVlb" ) ||
                      ( currentarg == "-fzskvub" ) ||
                      ( currentarg == "-fzskVub" ) ||
                      ( currentarg == "-fzsko" ) ||
                      ( currentarg == "-fzskO" ) ||
                      ( currentarg == "-fzskx" )    )
            {
                // Fuzzy options

                if ( grabargs(3,fuzzyopt,commstack,currentarg) )
                {
                    retval  = 84;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-boot" ) )
            {
                // Bootstrap options

                if ( grabargs(1,bootopt,commstack,currentarg) )
                {
                    retval  = 85;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-tx"    ) ||
                      ( currentarg == "-tl"    ) ||
                      ( currentarg == "-tmg"   ) ||
                      ( currentarg == "-ta"    ) ||
                      ( currentarg == "-tQ"    ) ||
                      ( currentarg == "-tnQ"   ) ||
                      ( currentarg == "-tQx"   ) ||
                      ( currentarg == "-tnQx"  ) ||
                      ( currentarg == "-tvar"  ) ||
                      ( currentarg == "-txz"   ) ||
                      ( currentarg == "-txf"   ) ||
                      ( currentarg == "-txB"   ) ||
                      ( currentarg == "-txzB"  ) ||
                      ( currentarg == "-txfB"  ) ||
                      ( currentarg == "-tr"    ) ||
                      ( currentarg == "-trB"   )    )
            {
                // Testing options

                if ( grabargs(1,performopt,commstack,currentarg) )
                {
                    retval  = 89;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-tm"     ) ||
                      ( currentarg == "-tn"     ) ||
                      ( currentarg == "-tT"     ) ||
                      ( currentarg == "-tM"     ) ||
                      ( currentarg == "-tMv"    ) ||
                      ( currentarg == "-tMx"    ) ||
                      ( currentarg == "-tc"     ) ||
                      ( currentarg == "-tcZ"    ) ||
                      ( currentarg == "-tcz"    ) ||
                      ( currentarg == "-tcF"    ) ||
                      ( currentarg == "-tcf"    ) ||
                      ( currentarg == "-tcB"    ) ||
                      ( currentarg == "-tczB"   ) ||
                      ( currentarg == "-tcfB"   ) ||
                      ( currentarg == "-tf"     ) ||
                      ( currentarg == "-tfe"    ) ||
                      ( currentarg == "-tfi"    ) ||
                      ( currentarg == "-tfI"    ) ||
                      ( currentarg == "-tfr"    ) ||
                      ( currentarg == "-tfR"    ) ||
                      ( currentarg == "-tfu"    ) ||
                      ( currentarg == "-tfeu"   ) ||
                      ( currentarg == "-tfei"   ) ||
                      ( currentarg == "-tfeI"   ) ||
                      ( currentarg == "-tfer"   ) ||
                      ( currentarg == "-tfeR"   ) ||
                      ( currentarg == "-tfiu"   ) ||
                      ( currentarg == "-tfIu"   ) ||
                      ( currentarg == "-tfru"   ) ||
                      ( currentarg == "-tfRu"   ) ||
                      ( currentarg == "-tfeiu"  ) ||
                      ( currentarg == "-tfeIu"  ) ||
                      ( currentarg == "-tferu"  ) ||
                      ( currentarg == "-tfeRu"  ) ||
                      ( currentarg == "-tfB"    ) ||
                      ( currentarg == "-tfeB"   ) ||
                      ( currentarg == "-tfiB"   ) ||
                      ( currentarg == "-tfIB"   ) ||
                      ( currentarg == "-tfrB"   ) ||
                      ( currentarg == "-tfRB"   ) ||
                      ( currentarg == "-tfuB"   ) ||
                      ( currentarg == "-tfeiB"  ) ||
                      ( currentarg == "-tfeIB"  ) ||
                      ( currentarg == "-tferB"  ) ||
                      ( currentarg == "-tfeRB"  ) ||
                      ( currentarg == "-tfeuB"  ) ||
                      ( currentarg == "-tfiuB"  ) ||
                      ( currentarg == "-tfIuB"  ) ||
                      ( currentarg == "-tfruB"  ) ||
                      ( currentarg == "-tfRuB"  ) ||
                      ( currentarg == "-tfeiuB" ) ||
                      ( currentarg == "-tfeIuB" ) ||
                      ( currentarg == "-tferuB" ) ||
                      ( currentarg == "-tfeRuB" )    )
            {
                // Testing options

                if ( grabargs(2,performopt,commstack,currentarg) )
                {
                    retval  = 90;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-tC"     ) ||
                      ( currentarg == "-tCZ"    ) ||
                      ( currentarg == "-tCz"    ) ||
                      ( currentarg == "-tCF"    ) ||
                      ( currentarg == "-tCf"    ) ||
                      ( currentarg == "-tCB"    ) ||
                      ( currentarg == "-tCzB"   ) ||
                      ( currentarg == "-tCfB"   ) ||
                      ( currentarg == "-tMMv"   ) ||
                      ( currentarg == "-tMMx"   ) ||
                      ( currentarg == "-tMpy"   ) ||
                      ( currentarg == "-tMpyv"  ) ||
                      ( currentarg == "-tMpyf"  ) ||
                      ( currentarg == "-tMexe"  ) ||
                      ( currentarg == "-tMexev" ) ||
                      ( currentarg == "-tMexef" ) ||
                      ( currentarg == "-tfl"    ) ||
                      ( currentarg == "-tfel"   ) ||
                      ( currentarg == "-tfil"   ) ||
                      ( currentarg == "-tfIl"   ) ||
                      ( currentarg == "-tfrl"   ) ||
                      ( currentarg == "-tfRl"   ) ||
                      ( currentarg == "-tfeil"  ) ||
                      ( currentarg == "-tfeIl"  ) ||
                      ( currentarg == "-tferl"  ) ||
                      ( currentarg == "-tfeRl"  ) ||
                      ( currentarg == "-tflB"   ) ||
                      ( currentarg == "-tfelB"  ) ||
                      ( currentarg == "-tfilB"  ) ||
                      ( currentarg == "-tfIlB"  ) ||
                      ( currentarg == "-tfrlB"  ) ||
                      ( currentarg == "-tfRlB"  ) ||
                      ( currentarg == "-tfeilB" ) ||
                      ( currentarg == "-tfeIlB" ) ||
                      ( currentarg == "-tferlB" ) ||
                      ( currentarg == "-tfeRlB" ) ||
                      ( currentarg == "-tMvx"   ) ||
                      ( currentarg == "-tV"     ) ||
                      ( currentarg == "-tU"     ) ||
                      ( currentarg == "-tW"     )    )
            {
                // Testing options

                if ( grabargs(3,performopt,commstack,currentarg) )
                {
                    retval  = 91;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-tMd"    ) ||
                      ( currentarg == "-tMD"    ) ||
                      ( currentarg == "-tMMvx"  ) ||
                      ( currentarg == "-tF"     ) ||
                      ( currentarg == "-tFe"    ) ||
                      ( currentarg == "-tFi"    ) ||
                      ( currentarg == "-tFI"    ) ||
                      ( currentarg == "-tFr"    ) ||
                      ( currentarg == "-tFR"    ) ||
                      ( currentarg == "-tFu"    ) ||
                      ( currentarg == "-tFei"   ) ||
                      ( currentarg == "-tFeI"   ) ||
                      ( currentarg == "-tFer"   ) ||
                      ( currentarg == "-tFeR"   ) ||
                      ( currentarg == "-tFeu"   ) ||
                      ( currentarg == "-tFiu"   ) ||
                      ( currentarg == "-tFIu"   ) ||
                      ( currentarg == "-tFru"   ) ||
                      ( currentarg == "-tFRu"   ) ||
                      ( currentarg == "-tFeiu"  ) ||
                      ( currentarg == "-tFeIu"  ) ||
                      ( currentarg == "-tFeru"  ) ||
                      ( currentarg == "-tFeRu"  ) ||
                      ( currentarg == "-tFB"    ) ||
                      ( currentarg == "-tFeB"   ) ||
                      ( currentarg == "-tFiB"   ) ||
                      ( currentarg == "-tFIB"   ) ||
                      ( currentarg == "-tFrB"   ) ||
                      ( currentarg == "-tFRB"   ) ||
                      ( currentarg == "-tFuB"   ) ||
                      ( currentarg == "-tFeiB"  ) ||
                      ( currentarg == "-tFeIB"  ) ||
                      ( currentarg == "-tFerB"  ) ||
                      ( currentarg == "-tFeRB"  ) ||
                      ( currentarg == "-tFeuB"  ) ||
                      ( currentarg == "-tFiuB"  ) ||
                      ( currentarg == "-tFIuB"  ) ||
                      ( currentarg == "-tFruB"  ) ||
                      ( currentarg == "-tFRuB"  ) ||
                      ( currentarg == "-tFeiuB" ) ||
                      ( currentarg == "-tFeIuB" ) ||
                      ( currentarg == "-tFeruB" ) ||
                      ( currentarg == "-tFeRuB" )    )
            {
                // Testing options

                if ( grabargs(4,performopt,commstack,currentarg) )
                {
                    retval  = 92;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-tFl"    ) ||
                      ( currentarg == "-tFel"   ) ||
                      ( currentarg == "-tFil"   ) ||
                      ( currentarg == "-tFIl"   ) ||
                      ( currentarg == "-tFrl"   ) ||
                      ( currentarg == "-tFRl"   ) ||
                      ( currentarg == "-tFeil"  ) ||
                      ( currentarg == "-tFeIl"  ) ||
                      ( currentarg == "-tFerl"  ) ||
                      ( currentarg == "-tFeRl"  ) ||
                      ( currentarg == "-tFlB"   ) ||
                      ( currentarg == "-tFelB"  ) ||
                      ( currentarg == "-tFilB"  ) ||
                      ( currentarg == "-tFIlB"  ) ||
                      ( currentarg == "-tFrlB"  ) ||
                      ( currentarg == "-tFRlB"  ) ||
                      ( currentarg == "-tFeilB" ) ||
                      ( currentarg == "-tFeIlB" ) ||
                      ( currentarg == "-tFerlB" ) ||
                      ( currentarg == "-tFeRlB" ) ||
                      ( currentarg == "-tb"     ) ||
                      ( currentarg == "-tg"     ) ||
                      ( currentarg == "-tG"     )    )
            {
                // Testing options

                if ( grabargs(5,performopt,commstack,currentarg) )
                {
                    retval  = 93;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-tgc" ) ||
                      ( currentarg == "-tGc" )    )
            {
                // Testing options

                if ( grabargs(6,performopt,commstack,currentarg) )
                {
                    retval  = 93;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-hX"  ) ||
                      ( currentarg == "-K0"  ) ||
                      ( currentarg == "-hhX" ) ||
                      ( currentarg == "-hXv" )    )

            {
                // Reporting options

                if ( grabargs(1,reportopt,commstack,currentarg) )
                {
                    retval  = 94;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-a"    ) ||
                      ( currentarg == "-b"    ) ||
                      ( currentarg == "-s"    ) ||
                      ( currentarg == "-hpln" ) ||
                      ( currentarg == "-hpld" ) ||
                      ( currentarg == "-hplt" ) ||
                      ( currentarg == "-hpls" ) ||
                      ( currentarg == "-hplD" ) ||
                      ( currentarg == "-hplv" ) ||
                      ( currentarg == "-hplb" ) ||
                      ( currentarg == "-hplm" ) ||
                      ( currentarg == "-hplM" ) ||
                      ( currentarg == "-hplx" ) ||
                      ( currentarg == "-hU"   ) ||
                      ( currentarg == "-hhU"  ) ||
                      ( currentarg == "-ak"   ) ||
                      ( currentarg == "-K1"   ) ||
                      ( currentarg == "-phi2" ) ||
                      ( currentarg == "-hY"   ) ||
                      ( currentarg == "-hhY"  ) ||
                      ( currentarg == "-hV"   ) ||
                      ( currentarg == "-hhV"  ) ||
                      ( currentarg == "-hW"   ) ||
                      ( currentarg == "-hhW"  ) ||
                      ( currentarg == "-hWe"  ) ||
                      ( currentarg == "-hWE"  ) ||
                      ( currentarg == "-hUv"  ) ||
                      ( currentarg == "-hYv"  ) ||
                      ( currentarg == "-hVv"  ) ||
                      ( currentarg == "-hVV"  ) ||
                      ( currentarg == "-hWv"  ) ||
                      ( currentarg == "-echo" ) ||
                      ( currentarg == "-ECHO" )    )
            {
                // Reporting options

                if ( grabargs(2,reportopt,commstack,currentarg) )
                {
                    retval  = 95;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-hM"   ) ||
                      ( currentarg == "-hYvn" ) ||
                      ( currentarg == "-hUe"  ) ||
                      ( currentarg == "-hUE"  ) ||
                      ( currentarg == "-hYe"  ) ||
                      ( currentarg == "-hYE"  ) ||
                      ( currentarg == "-hN"   ) ||
                      ( currentarg == "-hZ"   ) ||
                      ( currentarg == "-hhZ"  ) ||
                      ( currentarg == "-hUc"  ) ||
                      ( currentarg == "-K2"   ) ||
                      ( currentarg == "-hYc"  ) ||
                      ( currentarg == "-hWc"  ) ||
                      ( currentarg == "-hZv"  )    )
            {
                // Reporting options

                if ( grabargs(3,reportopt,commstack,currentarg) )
                {
                    retval  = 96;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-hZc"  ) ||
                      ( currentarg == "-plot" ) ||
                      ( currentarg == "-K3"   )    )
            {
                // Reporting options

                if ( grabargs(4,reportopt,commstack,currentarg) )
                {
                    retval  = 97;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-K4"   ) ||
                      ( currentarg == "-hp"   ) ||
                      ( currentarg == "-hpi"  )    )
            {
                // Reporting options

                if ( grabargs(5,reportopt,commstack,currentarg) )
                {
                    retval  = 97;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-surf" ) )
            {
                // Reporting options

                if ( grabargs(7,reportopt,commstack,currentarg) )
                {
                    retval  = 97;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-plotfn" ) )
            {
                // Reporting options

                if ( grabargs(9,reportopt,commstack,currentarg) )
                {
                    retval  = 97;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-surffn" ) )
            {
                // Reporting options

                if ( grabargs(12,reportopt,commstack,currentarg) )
                {
                    retval  = 97;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else if ( ( currentarg == "-Km" )    )
            {
                // Reporting options
                //
                // To process we first read the number of arguments.  Then
                // there are a arguments to come, where a is # args

                std::string storearg = currentarg;

                int a = 0;
                int b = reportopt.size();

                stopnow = grabnextarg(commstack,currentarg);

                std::string mlarg = currentarg;

                if ( !stopnow )
                {
                    a = safeatoi(currentarg,argvariables);

                    if ( ( a >= 0 ) && ( grabargs(a+1,reportopt,commstack,( currentarg = storearg )) ) )
                    {
                        retval  = 70;
                        stopnow = 1;
                    }

                    // I was using "else if" here, and a nested structure above, but...
                    // "compiler limit : blocks nested too deeply"
                    // ...spake microsoft c-c++ compiler driver (in mex)

                    if ( a < 0 )
                    {
                        errstream() << "Syntax error: " << storearg << " argument count must be non-negative\n";
                        retval  = 71;
                        stopnow = 1;
                    }
                }

                else
                {
                    errstream() << "Syntax error: " << storearg << " requires 1 argument minimum\n";
                    retval  = 72;
                    stopnow = 1;
                }

                reportopt("&",b).add(1);
                reportopt("&",b)("&",1) = mlarg;

                updateargvars = 1;
            }

            else if ( ( currentarg == "-hP"   ) ||
                      ( currentarg == "-hPi"  )    )
            {
                // Reporting options

                if ( grabargs(6,reportopt,commstack,currentarg) )
                {
                    retval  = 97;
                    stopnow = 1;
                }

                updateargvars = 1;
            }

            else
            {
                // Unknown command

                errstream() << "Syntax error: command " << currentarg << " unknown.\n";
                retval  = 98;
                stopnow = 1;
            }
        }







































// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================
// ===========================================================================





        else if ( !retval && ( skipon   ||
                  loggingopt.size()     ||
                  multirunopt.size()    ||
                  svmsetupopt.size()    ||
                  svmpresetupopt.size() ||
                  preloadopt.size()     ||
                  loadopt.size()        ||
                  postloadopt.size()    ||
                  learningopt.size()    ||
                  kernelopt.size()      ||
                  tuningopt.size()      ||
                  gridopt.size()        ||
                  xferopt.size()        ||
                  featureopt.size()     ||
                  fuzzyopt.size()       ||
                  bootopt.size()        ||
                  optimopt.size()       ||
                  performopt.size()     ||
                  reportopt.size()         ) )
        {
        processnow:

          skipon       = 0;
          argbatchsize = 0;

          Vector<std::string> currcommand;

          try
          {
            multiruntime    = 0;
            svmsetuptime    = 0;
            svmpresetuptime = 0;
            preloadtime     = 0;
            loadtime        = 0;
            postloadtime    = 0;
            learningtime    = 0;
            kerneltime      = 0;
            tuningtime      = 0;
            gridtime        = 0;
            xfertime        = 0;
            featuretime     = 0;
            fuzzytime       = 0;
            optimtime       = 0;
            performtime     = 0;
            reporttime      = 0;

            // Run through relevant SVM tests

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Logging steps

            if ( loggingopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Processing logging setup operations... ";

                while ( loggingopt.size() )
                {
                    currcommand = loggingopt(0);
                    loggingopt.remove(0);

                    if ( currcommand(0) == "-v" )
                    {
                        // Change verbosity level

                        verblevel = safeatoi(currcommand(1),argvariables);

                        argvariables("&",1)("&",13) = verblevel;

                        if ( ( verblevel != 0 ) && ( verblevel != 1 ) )
                        {
                            STRTHROW("Syntax error: -v options are 0,1");
                        }
                    }

                    else if ( currcommand(0) == "-L" )
                    {
                        // Change log file name

                        logfile = currcommand(1);
                        argvariables("&",1)("&",12).makeString(logfile);
                    }

                    else if ( currcommand(0) == "-LL" )
                    {
                        // Change log file name

                        gentype temparg;

                        safeatowhatever(temparg,currcommand(1),argvariables);
                        logfile = (const std::string &) temparg;
                        argvariables("&",1)("&",12).makeString(logfile);
                    }

                    else if ( currcommand(0) == "-Lmute"     ) {   suppresserrstreamfile();  }
                    else if ( currcommand(0) == "-Lunmute"   ) { unsuppresserrstreamfile();  }
                    else if ( currcommand(0) == "-LMute"     ) {   suppressoutstreamfile();  }
                    else if ( currcommand(0) == "-LunMute"   ) { unsuppressoutstreamfile();  }
                    else if ( currcommand(0) == "-LMUTE"     ) {   suppressallstreamfile(); }
                    else if ( currcommand(0) == "-LunMUTE"   ) { unsuppressallstreamfile(); }
                }

                time_used endtime = TIMECALL;
                loggingtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << loggingtime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Multirun steps

            if ( multirunopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Processing multi-run setup operations... ";

                while ( multirunopt.size() )
                {
                    currcommand = multirunopt(0);
                    multirunopt.remove(0);

                    if ( currcommand(0) == "-qR" )
                    {
                        // Delete SVM completely

                        int nInd = safeatoi(currcommand(1),argvariables);

                        if ( nInd <= 0 )
                        {
                            STRTHROW("Non-positive SVM index "+currcommand(1)+" in -qR");
                        }

                        // First claim the ML in question for the current thread...

                        grabsvm(svmThreadOwner,svmbase,threadInd,nInd,svmContext);

                        // ...delete it...

                        MEMDEL(svmbase("&",nInd));

                        // ...and remove all traces

                        svmbase.zero(nInd);
                        svmThreadOwner.zero(nInd);

                        // NB: if nInd = svmInd then the SVM will be automatically
                        // re-created in restarted state on first dereference
                        // using svmbase(svmInd).

                        if ( nInd == svmInd )
                        {
                            argvariables("&",42)("&",42) = svmInd;
                        }
                    }

                    else if ( currcommand(0) == "-qc" )
                    {
                        // copy SVM - overwrite n with m

                        int nInd = safeatoi(currcommand(1),argvariables);
                        int mInd = safeatoi(currcommand(2),argvariables);

                        if ( nInd <= 0 )
                        {
                            STRTHROW("Non-positive first SVM index "+currcommand(1)+" in -qs");
                        }

                        if ( mInd <= 0 )
                        {
                            STRTHROW("Non-positive second SVM index "+currcommand(2)+" in -qs");
                        }

                        if ( nInd != mInd )
                        {
                            // Claim both SVMs for current thread...

                            grabsvm(svmThreadOwner,svmbase,threadInd,nInd,svmContext);
                            grabsvm(svmThreadOwner,svmbase,threadInd,mInd,svmContext);

                            // ...overwrite...

                            getMLref(svmThreadOwner,svmbase,threadInd,nInd,svmContext) = getMLrefconst(svmThreadOwner,svmbase,threadInd,mInd,svmContext);

                            // ...and disowm both as/if required.

                            svmThreadOwner("&",nInd) = ( svmInd == nInd ) ? threadInd : -1;
                            svmThreadOwner("&",mInd) = ( svmInd == mInd ) ? threadInd : -1;

                            if ( nInd == svmInd )
                            {
                                argvariables("&",42)("&",42) = svmInd;
                            }
                        }
                    }

                    else if ( currcommand(0) == "-qsave" )
                    {
                        // copy SVM - overwrite n with m

                        int mInd = safeatoi(currcommand(1),argvariables);

                        std::ofstream savefile(currcommand(2).c_str());

                        savefile << getMLrefconst(svmThreadOwner,svmbase,threadInd,mInd,svmContext);

                        savefile.close();
                    }

                    else if ( currcommand(0) == "-qs" )
                    {
                        // swap SVMs

                        int nInd = safeatoi(currcommand(1),argvariables);
                        int mInd = safeatoi(currcommand(2),argvariables);

                        if ( nInd <= 0 )
                        {
                            STRTHROW("Non-positive first SVM index "+currcommand(1)+" in -qs");
                        }

                        if ( mInd <= 0 )
                        {
                            STRTHROW("Non-positive second SVM index "+currcommand(2)+" in -qs");
                        }

                        if ( nInd != mInd )
                        {
                            // Claim both SVMs for current thread...

                            grabsvm(svmThreadOwner,svmbase,threadInd,nInd,svmContext);
                            grabsvm(svmThreadOwner,svmbase,threadInd,mInd,svmContext);

                            // ...swap...

                            svmbase.squareswap(nInd,mInd);

                            // ...and disowm both as/if required.

                            svmThreadOwner("&",nInd) = ( svmInd == nInd ) ? threadInd : -1;
                            svmThreadOwner("&",mInd) = ( svmInd == mInd ) ? threadInd : -1;

                            if ( ( nInd == svmInd ) || ( mInd == svmInd ) )
                            {
                                argvariables("&",42)("&",42) = svmInd;
                            }
                        }
                    }

                    else if ( currcommand(0) == "-qw" )
                    {
                        // set working SVM

                        int nInd = safeatoi(currcommand(1),argvariables);

                        if ( nInd <= 0 )
                        {
                            STRTHROW("Non-positive SVM index "+currcommand(1)+" in -qw");
                        }

                        if ( nInd != svmInd )
                        {
                            // Disown current ML...

                            svmThreadOwner("&",svmInd) = -1;

                            // ...claim new ML...

                            grabsvm(svmThreadOwner,svmbase,threadInd,nInd,svmContext);

                            // ...and update indexes

                            svmInd = nInd;

                            argvariables("&",42)("&",42) = svmInd;
                        }
                    }

                    else if ( currcommand(0) == "-qpush" )
                    {
                        // set working SVM after pushing old one onto the stack

                        MLindstack.push(svmInd);

                        int nInd = safeatoi(currcommand(1),argvariables);

                        if ( nInd <= 0 )
                        {
                            STRTHROW("Non-positive SVM index "+currcommand(1)+" in -qpush");
                        }

                        if ( nInd != svmInd )
                        {
                            // Disown current ML...

                            svmThreadOwner("&",svmInd) = -1;

                            // ...claim new ML...

                            grabsvm(svmThreadOwner,svmbase,threadInd,nInd,svmContext);

                            // ...and update indexes

                            svmInd = nInd;

                            argvariables("&",42)("&",42) = svmInd;
                        }
                    }

                    else if ( currcommand(0) == "-qpop" )
                    {
                        // set working SVM to one popped off stack

                        int nInd = -1;

                        MLindstack.pop(nInd);

                        if ( nInd <= 0 )
                        {
                            STRTHROW("Non-positive SVM index "+currcommand(1)+" in -qpush");
                        }

                        if ( nInd != svmInd )
                        {
                            // Disown current ML...

                            svmThreadOwner("&",svmInd) = -1;

                            // ...claim new ML...

                            grabsvm(svmThreadOwner,svmbase,threadInd,nInd,svmContext);

                            // ...and update indexes

                            svmInd = nInd;

                            argvariables("&",42)("&",42) = svmInd;
                        }
                    }
                }

                time_used endtime = TIMECALL;
                multiruntime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << multiruntime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Pre-Setup steps

            if ( svmpresetupopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Processing setup operations... ";

                while ( svmpresetupopt.size() )
                {
                    currcommand = svmpresetupopt(0);
                    svmpresetupopt.remove(0);

                    if ( currcommand(0) == "-zl" )
                    {
                        // Load SVM from file

                        argvariables("&",1)("&",14).makeString(currcommand(1));

                        std::ifstream loadfile(currcommand(1).c_str());

                        if ( !loadfile.is_open() )
                        {
                            STRTHROW("Unable to open SVM file "+currcommand(1));
                        }

                        loadfile >> getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        loadfile.close();
                    }

                    else if ( currcommand(0) == "-z" )
                    {
                        // Set SVM type, and reset SVM

                             if ( currcommand(1) == "r"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(0);   }
                        else if ( currcommand(1) == "c"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(1);   }
                        else if ( currcommand(1) == "s"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(2);   }
                        else if ( currcommand(1) == "m"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(3);   }
                        else if ( currcommand(1) == "v"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(4);   }
                        else if ( currcommand(1) == "a"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(5);   }
                        else if ( currcommand(1) == "p"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(7);   }
                        else if ( currcommand(1) == "t"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(8);   }
                        else if ( currcommand(1) == "l"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(12);  }
                        else if ( currcommand(1) == "o"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(13);  }
                        else if ( currcommand(1) == "g"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(15);  }
                        else if ( currcommand(1) == "i"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(16);  }
                        else if ( currcommand(1) == "h"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(17);  }
                        else if ( currcommand(1) == "j"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(18);  }
                        else if ( currcommand(1) == "b"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(19);  }
                        else if ( currcommand(1) == "u"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(20);  }
                        else if ( currcommand(1) == "d"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(21);  }
                        else if ( currcommand(1) == "R"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(22);  }
                        else if ( currcommand(1) == "B"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(23);  }

                        else if ( currcommand(1) == "knp" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(300); }
                        else if ( currcommand(1) == "knc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(301); }
                        else if ( currcommand(1) == "kng" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(302); }
                        else if ( currcommand(1) == "knr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(303); }
                        else if ( currcommand(1) == "knv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(304); }
                        else if ( currcommand(1) == "kna" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(305); }
                        else if ( currcommand(1) == "knm" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(307); }

                        else if ( currcommand(1) == "gpr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(400); }
                        else if ( currcommand(1) == "gpv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(401); }
                        else if ( currcommand(1) == "gpa" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(402); }
                        else if ( currcommand(1) == "gpg" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(408); }
                        else if ( currcommand(1) == "gpc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(409); }
                        else if ( currcommand(1) == "gpR" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(410); }
                        else if ( currcommand(1) == "gpC" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(411); }

                        else if ( currcommand(1) == "mlr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(800); }
                        else if ( currcommand(1) == "mlc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(801); }
                        else if ( currcommand(1) == "mlv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(802); }

                        else if ( currcommand(1) == "lsr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(500); }
                        else if ( currcommand(1) == "lsv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(501); }
                        else if ( currcommand(1) == "lsa" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(502); }
                        else if ( currcommand(1) == "lso" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(505); }
                        else if ( currcommand(1) == "lsg" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(508); }
                        else if ( currcommand(1) == "lsi" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(509); }
                        else if ( currcommand(1) == "lsh" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(510); }
                        else if ( currcommand(1) == "lsc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(511); }
                        else if ( currcommand(1) == "lsR" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(512); }

                        else if ( currcommand(1) == "nop" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(200); }
                        else if ( currcommand(1) == "con" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(201); }
                        else if ( currcommand(1) == "fna" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(203); }
                        else if ( currcommand(1) == "io"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(204); }
                        else if ( currcommand(1) == "avr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(202); }
                        else if ( currcommand(1) == "avv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(205); }
                        else if ( currcommand(1) == "ava" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(206); }
                        else if ( currcommand(1) == "fnb" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(207); }
                        else if ( currcommand(1) == "fcb" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(208); }
                        else if ( currcommand(1) == "mxa" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(209); }
                        else if ( currcommand(1) == "mxb" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(210); }
                        else if ( currcommand(1) == "mer" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(211); }
                        else if ( currcommand(1) == "mba" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(212); }
                        else if ( currcommand(1) == "sys" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(213); }
                        else if ( currcommand(1) == "ker" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(214); }
                        else if ( currcommand(1) == "ber" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(215); }
                        else if ( currcommand(1) == "bat" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(216); }

                        else if ( currcommand(1) == "ei"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(600); }
                        else if ( currcommand(1) == "svm" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(601); }
                        else if ( currcommand(1) == "rls" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(602); }
                        else if ( currcommand(1) == "rns" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(603); }

                        else if ( currcommand(1) == "ser" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(-2);  }
                        else if ( currcommand(1) == "par" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeClean(-3);  }

                        else { STRTHROW("Syntax error: -z options are many, but this is not one of them"); }
                    }

                    else if ( currcommand(0) == "-zd" )
                    {
                        // Set SVM type, try to transition data

                             if ( currcommand(1) == "r"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(0);   }
                        else if ( currcommand(1) == "c"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(1);   }
                        else if ( currcommand(1) == "s"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(2);   }
                        else if ( currcommand(1) == "m"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(3);   }
                        else if ( currcommand(1) == "v"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(4);   }
                        else if ( currcommand(1) == "a"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(5);   }
                        else if ( currcommand(1) == "p"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(7);   }
                        else if ( currcommand(1) == "t"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(8);   }
                        else if ( currcommand(1) == "l"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(12);  }
                        else if ( currcommand(1) == "o"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(13);  }
                        else if ( currcommand(1) == "g"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(15);  }
                        else if ( currcommand(1) == "i"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(16);  }
                        else if ( currcommand(1) == "h"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(17);  }
                        else if ( currcommand(1) == "j"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(18);  }
                        else if ( currcommand(1) == "b"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(19);  }
                        else if ( currcommand(1) == "u"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(20);  }
                        else if ( currcommand(1) == "d"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(21);  }
                        else if ( currcommand(1) == "R"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(22);  }
                        else if ( currcommand(1) == "B"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(23);  }

                        else if ( currcommand(1) == "knp" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(300); }
                        else if ( currcommand(1) == "knc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(301); }
                        else if ( currcommand(1) == "kng" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(302); }
                        else if ( currcommand(1) == "knr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(303); }
                        else if ( currcommand(1) == "knv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(304); }
                        else if ( currcommand(1) == "kna" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(305); }
                        else if ( currcommand(1) == "knm" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(307); }

                        else if ( currcommand(1) == "gpr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(400); }
                        else if ( currcommand(1) == "gpv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(401); }
                        else if ( currcommand(1) == "gpa" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(402); }
                        else if ( currcommand(1) == "gpg" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(408); }
                        else if ( currcommand(1) == "gpc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(409); }
                        else if ( currcommand(1) == "gpR" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(410); }
                        else if ( currcommand(1) == "gpC" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(411); }

                        else if ( currcommand(1) == "mlr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(800); }
                        else if ( currcommand(1) == "mlc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(801); }
                        else if ( currcommand(1) == "mlv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(802); }

                        else if ( currcommand(1) == "lsr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(500); }
                        else if ( currcommand(1) == "lsv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(501); }
                        else if ( currcommand(1) == "lsa" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(502); }
                        else if ( currcommand(1) == "lso" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(505); }
                        else if ( currcommand(1) == "lsg" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(508); }
                        else if ( currcommand(1) == "lsi" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(509); }
                        else if ( currcommand(1) == "lsh" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(510); }
                        else if ( currcommand(1) == "lsc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(511); }
                        else if ( currcommand(1) == "lsR" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(512); }

                        else if ( currcommand(1) == "nop" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(200); }
                        else if ( currcommand(1) == "con" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(201); }
                        else if ( currcommand(1) == "fna" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(203); }
                        else if ( currcommand(1) == "io"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(204); }
                        else if ( currcommand(1) == "avr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(202); }
                        else if ( currcommand(1) == "avv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(205); }
                        else if ( currcommand(1) == "ava" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(206); }
                        else if ( currcommand(1) == "fnb" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(207); }
                        else if ( currcommand(1) == "fcb" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(208); }
                        else if ( currcommand(1) == "mxa" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(209); }
                        else if ( currcommand(1) == "mxb" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(210); }
                        else if ( currcommand(1) == "mer" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(211); }
                        else if ( currcommand(1) == "mba" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(212); }
                        else if ( currcommand(1) == "sys" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(213); }
                        else if ( currcommand(1) == "ker" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(214); }
                        else if ( currcommand(1) == "ber" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(215); }
                        else if ( currcommand(1) == "bat" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(216); }

                        else if ( currcommand(1) == "ei"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(600); }
                        else if ( currcommand(1) == "svm" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(601); }
                        else if ( currcommand(1) == "rls" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(602); }
                        else if ( currcommand(1) == "rns" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(603); }

                        else if ( currcommand(1) == "ser" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(-2);  }
                        else if ( currcommand(1) == "par" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMLTypeMorph(-3);  }

                        else { STRTHROW("Syntax error: -zd options are a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z"); }
                    }

                    else if ( currcommand(0) == "-zv" )
                    {
                        // Set SVM type, try to transition data

                        if      ( currcommand(1) == "once" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setatonce(); }
                        else if ( currcommand(1) == "red"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setredbin(); }
                        else { STRTHROW("Syntax error: -zv options are atonce (once) and redbin (red)"); }
                    }

                    else if ( currcommand(0) == "-zc" )
                    {
                        // Set multiclsas SVM type

                        if      ( currcommand(1) == "1vsA"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).set1vsA();    }
                        else if ( currcommand(1) == "1vs1"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).set1vs1();    }
                        else if ( currcommand(1) == "DAG"    ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setDAGSVM();  }
                        else if ( currcommand(1) == "MOC"    ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setMOC();     }
                        else if ( currcommand(1) == "maxwin" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmaxwins(); }
                        else if ( currcommand(1) == "recdiv" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setrecdiv();  }
                        else { STRTHROW("Syntax error: -zc options are 1vsA,1vs1,DAG,MOC,maxwin,recdiv"); }
                    }

                    else if ( currcommand(0) == "-zo" )
                    {
                        // Set 1-class SVM type

                        if      ( currcommand(1) == "sch" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsingmethod(0); }
                        else if ( currcommand(1) == "tax" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsingmethod(1); }
                        else { STRTHROW("Syntax error: -zo options are sch,tax"); }
                    }
                }

                time_used endtime = TIMECALL;
                svmpresetuptime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << svmpresetuptime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Setup steps

            if ( svmsetupopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Processing setup operations... ";

                double mooalpha = DEFAULT_MOOALPHA;

                while ( svmsetupopt.size() )
                {
                    currcommand = svmsetupopt(0);
                    svmsetupopt.remove(0);

                         if ( currcommand(0) == "-fV"   ) { argvariables("&",0)("&",safeatoi(currcommand(1),argvariables)) = currcommand(2); }
                    else if ( currcommand(0) == "-fW"   ) { safeatowhatever(argvariables("&",0)("&",safeatoi(currcommand(1),argvariables)),currcommand(2),argvariables); argvariables("&",0)("&",safeatoi(currcommand(1),argvariables)).finalise(); }
                    else if ( currcommand(0) == "-fWW"  ) { safeatowhatever(argvariables("&",0)("&",safeatoi(currcommand(1),argvariables)),currcommand(2),argvariables); }
                    else if ( currcommand(0) == "-fret" ) { returntag("&",safeatoi(currcommand(1),argvariables))("&",safeatoi(currcommand(2),argvariables)) = 1; }
                    else if ( currcommand(0) == "-fru"  ) { randufill(argvariables("&",0)("&",safeatoi(currcommand(1),argvariables))); }
                    else if ( currcommand(0) == "-frn"  ) { randnfill(argvariables("&",0)("&",safeatoi(currcommand(1),argvariables))); }
                    else if ( currcommand(0) == "-fri"  ) { (argvariables("&",0)("&",safeatoi(currcommand(1),argvariables))).force_int() = rand(); } //svm_rand(); }
                    else if ( currcommand(0) == "-mc"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmercachesize(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-mcn"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmercachenorm(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-mba"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmlqlist(safeatoi(currcommand(1),argvariables),getMLref(svmThreadOwner,svmbase,threadInd,safeatoi(currcommand(2),argvariables),svmContext)); }
                    else if ( currcommand(0) == "-mbw"  ) { gentype tmpg; getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmlqweight(safeatoi(currcommand(1),argvariables),safeatowhatever(tmpg,currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-mbm"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmlqmode(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-mbA"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).removemlqlist(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-mbI"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).addmlqlist(safeatoi(currcommand(1),argvariables),getMLref(svmThreadOwner,svmbase,threadInd,safeatoi(currcommand(2),argvariables),svmContext)); }
                    else if ( currcommand(0) == "-msn"  ) { gentype tmpg; getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBernDegree(safeatowhatever(tmpg,currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-msw"  ) { gentype tmpg; getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBernIndex(safeatowhatever(tmpg,currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-bat"  ) { Vector<gentype> tmp; getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setbattparam(safeatowhatever(tmp,currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-bam"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setbatttmax(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-bac"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setbattImax(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-bad"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setbatttdelta(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-bav"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setbattVstart(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-baT"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setbattthetaStart(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-TT"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setkconstWeights(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-br"   ) { binaryRelabel = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-bd"   ) { singleDrop = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-XT"   ) { std::stringstream xstr(currcommand(1)); SparseVector<gentype> xt; streamItIn(xstr,xt,0); xtemplate = xt; }

                    else if ( currcommand(0) == "-N"    )
                    {
                        int pssize = safeatoi(currcommand(1),argvariables);

                        pssize = ( pssize >= 0 ) ? pssize : INT_MAX-1;

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).prealloc(pssize);

                        if ( pssize > ((int) (1.5*LARGE_TRAIN_BOUNDARY)) )
                        {
                            disableAltContent();
                        }
                    }

                    else if ( ( currcommand(0) == "-fo" ) || ( currcommand(0) == "-foe" ) )
                    {
                        int filenum;
                        int targpos = ( currcommand(0) == "-foe" ) ? 1 : 0;

                        filenum = safeatoi(currcommand(1),argvariables);

                        if ( filenum < 0 )
                        {
                            STRTHROW("Syntax error: file number in -fo must be non-negative.");
                        }

                        // Open the file

                        std::ifstream datfile(currcommand(2).c_str());

                        if ( !datfile.is_open() )
                        {
                            STRTHROW("Unable to open file -fo "+currcommand(2));
                        }

                        ofiletype templace(filenum,currcommand(2),targpos,datfile);
                        filevariables("&",filenum) = templace;

                        datfile.close();

                        argvariables("&",0)("&",filenum) = templace.getlinecnt();
                    }

                    else if ( currcommand(0) == "-fWm" )
                    {
                        // Set integer variable to result of mex function

                        int nn = safeatoi(currcommand(1),argvariables);
                        gentype srcvar;

                        safeatowhatever(srcvar,currcommand(3),argvariables);
                        gentype &resvar = argvariables("&",0)("&",nn);

                        resvar.makeString(currcommand(2));

                        (*getsetExtVar)(resvar,srcvar,-3);
                    }

                    else if ( currcommand(0) == "-fWM" )
                    {
                        // Set integer variable

                        int nn = safeatoi(currcommand(1),argvariables);
                        int ii = safeatoi(currcommand(2),argvariables);
                        gentype srcvar;

                        safeatowhatever(srcvar,currcommand(3),argvariables);
                        gentype &resvar = argvariables("&",0)("&",nn);

                        (*getsetExtVar)(resvar,srcvar,ii);
                    }

                    else if ( currcommand(0) == "-fu" )
                    {
                        // Single-objective test function evaluation

                        int nn = safeatoi(currcommand(1),argvariables);
                        int fnnum = safeatoi(currcommand(2),argvariables);

                        Vector<double> xxx;

                        safeatowhatever(xxx,currcommand(3),argvariables);

                        double rrr;

                        evalTestFn(fnnum,rrr,xxx);

                        argvariables("&",0)("&",nn) = rrr;
                    }

                    else if ( currcommand(0) == "-fuu" )
                    {
                        // Single-objective test function evaluation

                        int nn = safeatoi(currcommand(1),argvariables);
                        int fnnum = safeatoi(currcommand(2),argvariables);

                        Vector<double> xxx;
                        Matrix<double> aaa;

                        safeatowhatever(xxx,currcommand(3),argvariables);
                        safeatowhatever(aaa,currcommand(4),argvariables);

                        double rrr;

                        evalTestFn(fnnum,rrr,xxx,&aaa);

                        argvariables("&",0)("&",nn) = rrr;
                    }

                    else if ( currcommand(0) == "-ft" )
                    {
                        // Multi-objective test function evaluation

                        int nn = safeatoi(currcommand(1),argvariables);
                        int fnnum = safeatoi(currcommand(2),argvariables);
                        int MM = safeatoi(currcommand(3),argvariables);

                        Vector<double> xxx;

                        safeatowhatever(xxx,currcommand(4),argvariables);

                        Vector<double> rrr(MM);

                        evalTestFn(fnnum,xxx.size(),MM,rrr,xxx,mooalpha);

                        argvariables("&",0)("&",nn) = rrr;
                    }

                    else if ( currcommand(0) == "-fat" )
                    {
                        // Set something

                        mooalpha = safeatof(currcommand(1),argvariables);
                    }

                    else if ( currcommand(0) == "-fVg" )
                    {
                        // Set integer variable

                        int nn = safeatoi(currcommand(1),argvariables);

#ifdef ENABLE_THREADS
                        globargvariableslock.lock();
#endif
                        argvariables("&",0)("&",nn) = currcommand(2);
#ifdef ENABLE_THREADS
                        globargvariableslock.unlock();
#endif
                    }

                    else if ( currcommand(0) == "-fWg" )
                    {
                        // Set integer variable

                        int nn = safeatoi(currcommand(1),argvariables);

#ifdef ENABLE_THREADS
                        globargvariableslock.lock();
#endif
                        safeatowhatever(argvariables("&",0)("&",nn),currcommand(2),const_cast<SparseVector<SparseVector<gentype> > &>(globargvariables));
                        argvariables("&",0)("&",nn).finalise();
#ifdef ENABLE_THREADS
                        globargvariableslock.unlock();
#endif
                    }

                    else if ( currcommand(0) == "-fVG" )
                    {
                        // Set integer variable

                        int nn = safeatoi(currcommand(1),argvariables);

#ifdef ENABLE_THREADS
                        globargvariableslock.lock();
#endif
                        const_cast<SparseVector<SparseVector<gentype> > &>(globargvariables)("&",0)("&",nn) = currcommand(2);
#ifdef ENABLE_THREADS
                        globargvariableslock.unlock();
#endif
                    }

                    else if ( currcommand(0) == "-fWG" )
                    {
                        // Set integer variable

                        int nn = safeatoi(currcommand(1),argvariables);

#ifdef ENABLE_THREADS
                        globargvariableslock.lock();
#endif
                        safeatowhatever(const_cast<SparseVector<SparseVector<gentype> > &>(globargvariables)("&",0)("&",nn),currcommand(2),argvariables);
                        const_cast<SparseVector<SparseVector<gentype> > &>(globargvariables)("&",0)("&",nn).finalise();
#ifdef ENABLE_THREADS
                        globargvariableslock.unlock();
#endif
                    }

                    else if ( currcommand(0) == "-fuG" )
                    {
                        // Single-objective test function evaluation

                        int nn = safeatoi(currcommand(1),argvariables);
                        int fnnum = safeatoi(currcommand(2),argvariables);

                        Vector<double> xxx;

                        safeatowhatever(xxx,currcommand(3),argvariables);

                        double rrr;

                        evalTestFn(fnnum,rrr,xxx);

#ifdef ENABLE_THREADS
                        globargvariableslock.lock();
#endif
                        const_cast<SparseVector<SparseVector<gentype> > &>(globargvariables)("&",0)("&",nn) = rrr;
#ifdef ENABLE_THREADS
                        globargvariableslock.unlock();
#endif
                    }

                    else if ( currcommand(0) == "-fuuG" )
                    {
                        // Single-objective test function evaluation

                        int nn = safeatoi(currcommand(1),argvariables);
                        int fnnum = safeatoi(currcommand(2),argvariables);

                        Vector<double> xxx;
                        Matrix<double> aaa;

                        safeatowhatever(xxx,currcommand(3),argvariables);
                        safeatowhatever(aaa,currcommand(3),argvariables);

                        double rrr;

                        evalTestFn(fnnum,rrr,xxx,&aaa);

#ifdef ENABLE_THREADS
                        globargvariableslock.lock();
#endif
                        const_cast<SparseVector<SparseVector<gentype> > &>(globargvariables)("&",0)("&",nn) = rrr;
#ifdef ENABLE_THREADS
                        globargvariableslock.unlock();
#endif
                    }

                    else if ( currcommand(0) == "-ftG" )
                    {
                        // Multi-objective test function evaluation

                        int nn = safeatoi(currcommand(1),argvariables);
                        int fnnum = safeatoi(currcommand(2),argvariables);
                        int MM = safeatoi(currcommand(3),argvariables);

                        Vector<double> xxx;

                        safeatowhatever(xxx,currcommand(4),argvariables);

                        Vector<double> rrr(MM);

                        evalTestFn(fnnum,xxx.size(),MM,rrr,xxx,mooalpha);

#ifdef ENABLE_THREADS
                        globargvariableslock.lock();
#endif
                        const_cast<SparseVector<SparseVector<gentype> > &>(globargvariables)("&",0)("&",nn) = rrr;
#ifdef ENABLE_THREADS
                        globargvariableslock.unlock();
#endif
                    }

                    else if ( currcommand(0) == "-fM" )
                    {
                        // Set macro variable

                        int nn = safeatoi(currcommand(1),argvariables);

                        std::string newmacro = currcommand(2);

                        // Curly brackets already stripped at this point
                        argvariables("&",130)("&",nn).makeString(newmacro);
                    }

                    else if ( currcommand(0) == "-ac" )
                    {
                        // Set classifier SVM type

                        if      ( currcommand(1) == "svc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setClassifyViaSVM(); }
                        else if ( currcommand(1) == "svr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setClassifyViaSVR(); }
                        else { STRTHROW("Error: "+currentarg+" is not a valid -ac mode."); }
                    }

                    else if ( currcommand(0) == "-bv"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setVardelta();   }
                    else if ( currcommand(0) == "-bz"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setZerodelta();  }
                    else if ( currcommand(0) == "-bn"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setvarApprox(safeatoi(currcommand(1),argvariables));}
                    else if ( currcommand(0) == "-bgn"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setvarApproxim(safeatoi(currcommand(1),argvariables));}
                    else if ( currcommand(0) == "-bgv"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setVarmuBias();  }
                    else if ( currcommand(0) == "-bgz"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setZeromuBias(); }
                    else if ( currcommand(0) == "-bgep" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setEPConst();    }
                    else if ( currcommand(0) == "-bgnc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setNaiveConst(); }

                    else if ( currcommand(0) == "-B" )
                    {
                        // Set bias type

                        if      ( currcommand(1) == "f" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setFixedBias(biasdefault); }
                        else if ( currcommand(1) == "v" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setVarBias();              }
                        else if ( currcommand(1) == "p" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setPosBias();              }
                        else if ( currcommand(1) == "n" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setNegBias();              }
                        else { STRTHROW("Error: "+currentarg+" is not a valid -B mode."); }
                    }

                    else if ( currcommand(0) == "-R" )
                    {
                        // Set empirical risk type

                        if      ( currcommand(1) == "l" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setLinearCost();    getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setusefuzzt(0); }
                        else if ( currcommand(1) == "q" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setQuadraticCost(); getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setusefuzzt(0); }
                        else if ( currcommand(1) == "o" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).set1NormCost();     getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setusefuzzt(0); }
                        else if ( currcommand(1) == "g" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setLinearCost();    getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setusefuzzt(1); }
                        else if ( currcommand(1) == "G" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setQuadraticCost(); getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setusefuzzt(1); }
                        else { STRTHROW("Error: "+currentarg+" is not a valid -R mode."); }
                    }

                    else if ( currcommand(0) == "-mls" )
                    {
                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).settsize(safeatoi(currcommand(1),argvariables));
                    }

                    else if ( currcommand(0) == "-mlR" )
                    {
                        // Set empirical risk type

                        if      ( currcommand(2) == "l" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setregtype(safeatoi(currcommand(1),argvariables),1); }
                        else if ( currcommand(2) == "q" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setregtype(safeatoi(currcommand(1),argvariables),2); }
                        else { STRTHROW("Error: "+currentarg+" is not a valid -mlR mode."); }
                    }

                    else if ( currcommand(0) == "-T" )
                    {
                        // Set tube type

                        if      ( currcommand(1) == "f" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setFixedTube();  }
                        else if ( currcommand(1) == "s" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setShrinkTube(); }
                        else { STRTHROW("Error: "+currentarg+" is not a valid -T mode."); }
                    }
                }

                time_used endtime = TIMECALL;
                svmsetuptime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << svmsetuptime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Optimisation options - here so that the cholesky doesn't get filled for smo/d2c/gradient, which is important for large datasets

            {
                errstream() << "Setting optimisation parameters... ";

                time_used begintime = TIMECALL;

                if ( optimopt.size() )
                {
                    while ( optimopt.size() )
                    {
                        currcommand = optimopt(0);
                        optimopt.remove(0);

                             if ( currcommand(0) == "-oo"   ) { doopt = 0;                                                                                                            }
                        else if ( currcommand(0) == "-oO"   ) { doopt = 1;                                                                                                            }
                        else if ( currcommand(0) == "-oz"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setzerotol(safeatof(currcommand(1),argvariables));       }
                        else if ( currcommand(0) == "-ot"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmaxitcnt(safeatoi(currcommand(1),argvariables));      }
                        else if ( currcommand(0) == "-oy"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmaxtraintime(safeatof(currcommand(1),argvariables));  }
                        else if ( currcommand(0) == "-oY"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).settraintimeend(safeatof(currcommand(1),argvariables));  }
                        else if ( currcommand(0) == "-olr"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setlr(safeatof(currcommand(1),argvariables));            }
                        else if ( currcommand(0) == "-olrb" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setlrb(safeatof(currcommand(1),argvariables));           }
                        else if ( currcommand(0) == "-olrc" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setlrc(safeatof(currcommand(1),argvariables));           }
                        else if ( currcommand(0) == "-olrd" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setlrd(safeatof(currcommand(1),argvariables));           }
                        else if ( currcommand(0) == "-oM"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmemsize(safeatoi(currcommand(1),argvariables));       }
                        else if ( currcommand(0) == "-ofa"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setoutermethod(safeatoi(currcommand(1),argvariables));   }
                        else if ( currcommand(0) == "-ofe"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setoutertol(safeatof(currcommand(1),argvariables));      }
                        else if ( currcommand(0) == "-ofm"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setoutermom(safeatof(currcommand(1),argvariables));      }
                        else if ( currcommand(0) == "-ofr"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setouterlr(safeatof(currcommand(1),argvariables));       }
                        else if ( currcommand(0) == "-ofs"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setouterovsc(safeatof(currcommand(1),argvariables));     }
                        else if ( currcommand(0) == "-oft"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setoutermaxitcnt(safeatoi(currcommand(1),argvariables)); }
                        else if ( currcommand(0) == "-ofM"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setoutermaxcache(safeatoi(currcommand(1),argvariables)); }
                        else if ( currcommand(0) == "-ofy"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).fudgeOn();                                               }
                        else if ( currcommand(0) == "-ofn"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).fudgeOff();                                              }
                        else if ( currcommand(0) == "-omr"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmlmlr(safeatof(currcommand(1),argvariables));         }
                        else if ( currcommand(0) == "-ome"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setdiffstop(safeatof(currcommand(1),argvariables));      }
                        else if ( currcommand(0) == "-oms"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setlsparse(safeatof(currcommand(1),argvariables));       }

                        else if ( currcommand(0) == "-oe" )
                        {
                            double etol = 0;

                            if ( currcommand(1) == "A" )
                            {
                                if ( isSVM(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext)) )
                                {
                                    etol = ( 0.01*(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).eps()) > 100*(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).zerotol()) ) ? 0.01*(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).eps()) : 100*(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).zerotol());
                                }


                                else
                                {
                                    etol = 100*getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).zerotol();
                                }
                            }

                            else
                            {
                                etol = safeatof(currcommand(1),argvariables);
                            }

                            getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setOpttol(etol);
                            getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setOpttolb(etol);
                        }

                        else if ( currcommand(0) == "-oea" )
                        {
                            double etol = 0;

                            if ( currcommand(1) == "A" )
                            {
                                if ( isSVM(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext)) )
                                {
                                    etol = ( 0.01*(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).eps()) > 100*(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).zerotol()) ) ? 0.01*(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).eps()) : 100*(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).zerotol());
                                }


                                else
                                {
                                    etol = 100*getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).zerotol();
                                }
                            }

                            else
                            {
                                etol = safeatof(currcommand(1),argvariables);
                            }

                            getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setOpttol(etol);
                        }

                        else if ( currcommand(0) == "-oeb" )
                        {
                            double etol = 0;

                            if ( currcommand(1) == "A" )
                            {
                                if ( isSVM(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext)) )
                                {
                                    etol = ( 0.01*(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).eps()) > 100*(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).zerotol()) ) ? 0.01*(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).eps()) : 100*(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).zerotol());
                                }


                                else
                                {
                                    etol = 100*getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).zerotol();
                                }
                            }

                            else
                            {
                                etol = safeatof(currcommand(1),argvariables);
                            }

                            getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setOpttolb(etol);
                        }

                        else if ( currcommand(0) == "-om" )
                        {
                            if      ( currcommand(1) == "a" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setOptActive(); }
                            else if ( currcommand(1) == "s" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setOptSMO();    }
                            else if ( currcommand(1) == "d" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setOptD2C();    }
                            else if ( currcommand(1) == "g" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setOptGrad();   }

                            else
                            {
                                STRTHROW("Error: "+currcommand(1)+" is not a valid -om mode."); 
                            }
                        }

                    }
                }

                time_used endtime = TIMECALL;
                svmsetuptime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << svmsetuptime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Preload steps

            if ( preloadopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Processing preload operations... ";

                while ( preloadopt.size() )
                {
                    currcommand = preloadopt(0);
                    preloadopt.remove(0);

                    if      ( currcommand(0) == "-pr"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).removeTrainingVector(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-pcw" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCweight(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-pcs" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).scaleCweight(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-pww" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsweight(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-pws" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).scaleepsweight(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-pS"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).scale(1/abs2(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).alpha())); }
                    else if ( currcommand(0) == "-ps"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).scale(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-psz" ) { gentype tmpg; getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).sety(safeatoi(currcommand(1),argvariables),safeatowhatever(tmpg,currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-pR"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).reset(); }
                    else if ( currcommand(0) == "-pRR" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).restart(); }
                    else if ( currcommand(0) == "-pro" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).removeTrainingVector(0,safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-fic" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).fillCache(); }
                    else if ( currcommand(0) == "-pdw" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsigmaweight(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-pds" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).scalesigmaweight(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-prz" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).removeNonSupports(); }
                    else if ( currcommand(0) == "-prm" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).trimTrainingSet(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-psd" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setd(safeatoi(currcommand(1),argvariables),safeatoi(currcommand(2),argvariables)); }

                    else if ( currcommand(0) == "-pk" )
                    {
                        std::ifstream datfile(currcommand(1).c_str());

                        if ( !datfile.is_open() )
                        {
                            STRTHROW("Unable to open kernel file "+currcommand(1));
                        }

                        Matrix<gentype> kernmat;

                        datfile >> kernmat;

                        datfile.close();

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).K2bypass(kernmat);
                    }
                }

                time_used endtime = TIMECALL;
                preloadtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << preloadtime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Load steps

            if ( loadopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Processing load operations... ";

                double adist  = 1.0;
                double nadist = 1.0;
                int Nantrig = 0;
                int daclass = 0;
                int addVectsToML = 3;
                gentype AGlb(0.0);
                gentype AGub(1.0);

                while ( loadopt.size() )
                {
                    currcommand = loadopt(0);
                    loadopt.remove(0);

                         if ( currcommand(0) == "-ATa"  ) { adist        = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-ATb"  ) { nadist       = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-ATn"  ) { Nantrig      = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-ATx"  ) { daclass      = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-ATy"  ) { addVectsToML = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-Ad"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).settspaceDim(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-AD"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setorder(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-Ac"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).addclass(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-As"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setanomalyclass(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-Acz"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).addclass(safeatoi(currcommand(1),argvariables),1); }
                    else if ( currcommand(0) == "-Aca"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).anomalyOn(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-Acd"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).anomalyOff(); }
                    else if ( currcommand(0) == "-Aby"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBasisYUU(); }
                    else if ( currcommand(0) == "-Abu"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBasisUUU(); }
                    else if ( currcommand(0) == "-AeU"  ) { gentype tmpg; getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).addToBasisUU(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).NbasisUU(),safeatowhatever(tmpg,currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-AeR"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBasisUU(safeatoi(currcommand(1),argvariables),safeatoi(currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-Ar"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).removeFromBasisUU(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-ABy"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBasisYVV(); }
                    else if ( currcommand(0) == "-ABu"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBasisUVV(); }
                    else if ( currcommand(0) == "-AEU"  ) { gentype tmpg; getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).addToBasisVV(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).NbasisVV(),safeatowhatever(tmpg,currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-AER"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBasisVV(safeatoi(currcommand(1),argvariables),safeatoi(currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-AR"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).removeFromBasisVV(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-AGl"  ) { safeatowhatever(AGlb,currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-AGu"  ) { safeatowhatever(AGub,currcommand(1),argvariables); }

                    else if ( ( currcommand(0) == "-Ag" ) || ( currcommand(0) == "-AG" ) || ( currcommand(0) == "-Agc" ) || ( currcommand(0) == "-AGc" ) )
                    {
                        int N = safeatoi(currcommand(1),argvariables);
                        int d = safeatoi(currcommand(2),argvariables);
                        gentype f(currcommand(3)); // No processing, deliberately
                        double v = safeatof(currcommand(4),argvariables);
                        double nadd = 0;
                        int gorG = ( currcommand(0) == "-Ag" ) ? 0 : 1;

                        std::string cfn("1");

                        if ( ( currcommand(0) == "-Agc" ) || ( currcommand(0) == "-AGc" ) )
                        {
                            cfn = currcommand(5);
                        }

                        gentype cf(cfn);

                        int jj,kk;
                        Vector<SparseVector<gentype> > xdata(N);
                        Vector<gentype> ydata(N);
                        SparseVector<SparseVector<gentype> > z;
                        Vector<double> Qweight(N);

                        Qweight = 1.0;

                        // Generate x data

                        for ( jj = 0 ; jj < N ; ++jj )
                        {
                            for ( kk = 0 ; kk < d ; ++kk )
                            {
                                if ( !gorG )
                                {
                                    randnfill(xdata("&",jj)("&",kk)); // Gaussian, zero mean, unit variance.
                                }

                                else
                                {
                                    double lb = 0;
                                    double ub = 1;

                                    if ( AGlb.isValVector() )
                                    {
                                        lb = (double) ((const Vector<gentype> &) AGlb)(kk);
                                    }

                                    else
                                    {
                                        lb = (double) AGlb;
                                    }

                                    if ( AGub.isValVector() )
                                    {
                                        ub = (double) ((const Vector<gentype> &) AGub)(kk);
                                    }

                                    else
                                    {
                                        ub = (double) AGub;
                                    }

                                    randufill(xdata("&",jj)("&",kk)); // Uniform 0,1

                                    xdata("&",jj)("&",kk) *= ub-lb; // Uniform 0,ub-lb
                                    xdata("&",jj)("&",kk) += lb;    // Uniform lb,ub
                                }
                            }

                            z("&",0) = xdata(jj);

                            randnfill(nadd);

                            ydata("&",jj) = f(z) + (nadd*v);

                            if ( ( (int) cf(z) ) != 1 )
                            {
                                --jj;
                            }
                        }

                        addtemptox(xdata,xtemplate);

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).addTrainingVector(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N(),ydata,xdata,Qweight,Qweight);
                    }

                    else if ( currcommand(0) == "-Aq" )
                    {
                        int nbad = safeatoi(currcommand(1),argvariables);

                        double nmean = safeatof(currcommand(2),argvariables);
                        double nvar  = safeatof(currcommand(3),argvariables);

                        double nvadd;

                        int jj,kk;
                        int xdim = (getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).indKey())(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).indKey().size()-1)+1;

                        for ( jj = 0 ; jj < getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N() ; ++jj )
                        {
                            SparseVector<gentype> xj = (getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).x(jj));

                            for ( kk = 0 ; kk < nbad ; ++kk )
                            {
                                randnfill(nvadd);

                                xj("[]",xdim+kk) = nmean+(nvadd*nvar);
                            }

                            getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setx(jj,xj);
                        }
                    }

                    else if ( ( (currcommand(0)).substr(0,3) == "-AA" ) || ( (currcommand(0)).substr(0,3) == "-AN" ) )
                    {
                        std::string subcom = (currcommand(0)).substr(3,((currcommand(0)).length())-3); // Contains suffixes only
                        int isANtype = ( (currcommand(0)).substr(0,3) == "-AN" );                      // Set if i j {k} suffixes present
                        int fileargpos = isANtype ? 4 : 1;                                             // position of filename/number
                        int setibase = 1;                                                              // set if ibase (k) present

                        int reverse = 0;              // set 1 if -AAe used.
                        int ignoreStart = 0;          // number to ignore at start
                        int imax = -1;                // max number to add, or -1 if no limit
                        int ibase = -1;               // where to start adding points, or -1 if end.
                        int uselinesvector = 0;       // if 1 then use linesread vector
                        int israw = 0;                // set if output is to be saved in raw format (not used here)
                        int startpoint = 0;           // set if reoptimisation should start clean-slate (not used here)
                        int coercetosingle = 0;       // if 1 then class label / target is read but disgarded and
                        int coercefromsingle = 0;     // if 1 then class label / target is given and file is assumed unlabelled
                        gentype fromsingletarget;     // see above
                        std::string trainfile;        // name of training file
                        Vector<int> linesread;        // vector containing lines to be read (if uselinesvector is set)

                        argvariables("&",1)("&",11).makeString(trainfile);

                        xlateDataSourceSuffixes(isANtype,fileargpos,setibase,currcommand,subcom,argvariables,filevariables,reverse,ignoreStart,imax,ibase,uselinesvector,israw,startpoint,coercetosingle,coercefromsingle,fromsingletarget,trainfile,linesread);

                        if ( logfile.length() == 0 )
                        {
                            logfile = logfile+trainfile; // This will be logfile+filenum when uselinesvector is set
                            argvariables("&",1)("&",12).makeString(logfile);
                        }

                        if ( uselinesvector )
                        {
                            std::string indexfilename = logfile+"_index_"+((currcommand(0)).substr(1,((currcommand(0)).length())-1));

                            writeLog(linesread,indexfilename,getsetExtVar);
                        }

                        std::string savefiledummy("");

                        int pointsadded = addtrainingdata(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,trainfile,reverse,ignoreStart,imax,ibase,coercetosingle,coercefromsingle,fromsingletarget,binaryRelabel,singleDrop,uselinesvector,linesread,savefiledummy);

                        errstream() << "Added " << pointsadded << " training vectors/";
                    }

                    else if ( ( (currcommand(0)).substr(0,4) == "-ATA" ) || ( (currcommand(0)).substr(0,4) == "-ATN" ) )
                    {
                        std::string subcom = (currcommand(0)).substr(4,((currcommand(0)).length())-4); // Contains suffixes only
                        int isANtype = ( (currcommand(0)).substr(0,4) == "-ATN" );                     // Set if i j {k} suffixes present
                        int fileargpos = isANtype ? 4 : 1;                                             // position of filename/number
                        int setibase = 1;                                                              // set if ibase (k) present

                        int reverse = 0;              // set 1 if -AAe used.
                        int ignoreStart = 0;          // number to ignore at start
                        int imax = -1;                // max number to add, or -1 if no limit
                        int ibase = -1;               // where to start adding points, or -1 if end.
                        int uselinesvector = 0;       // if 1 then use linesread vector
                        int israw = 0;                // set if output is to be saved in raw format (not used here)
                        int startpoint = 0;           // set if reoptimisation should start clean-slate (not used here)
                        int coercetosingle = 0;       // if 1 then class label / target is read but disgarded and
                        int coercefromsingle = 0;     // if 1 then class label / target is given and file is assumed unlabelled
                        gentype fromsingletarget;     // see above
                        std::string trainfile;        // name of training file
                        Vector<int> linesread;        // vector containing lines to be read (if uselinesvector is set)

                        argvariables("&",1)("&",11).makeString(trainfile);

                        xlateDataSourceSuffixes(isANtype,fileargpos,setibase,currcommand,subcom,argvariables,filevariables,reverse,ignoreStart,imax,ibase,uselinesvector,israw,startpoint,coercetosingle,coercefromsingle,fromsingletarget,trainfile,linesread);

                        if ( logfile.length() == 0 )
                        {
                            logfile = logfile+trainfile; // This will be logfile+filenum when uselinesvector is set
                            argvariables("&",1)("&",12).makeString(logfile);
                        }

                        if ( uselinesvector )
                        {
                            std::string indexfilename = logfile+"_index_"+((currcommand(0)).substr(1,((currcommand(0)).length())-1));

                            writeLog(linesread,indexfilename,getsetExtVar);
                        }

                        int anomaliesRelabelled = 0;

                        Vector<SparseVector<gentype> > xxx;
                        Vector<gentype> yyy;

                        Vector<gentype> zassign;
                        Vector<int> trigVect;
                        Vector<int> anomVect;
                        Vector<int> addVect;

                        loadFileForHillClimb(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,trainfile,reverse,ignoreStart,imax,coercetosingle,coercefromsingle,fromsingletarget,binaryRelabel,singleDrop,uselinesvector,linesread,xxx,yyy);
                        anomaliesRelabelled = anAnomalyCreate(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getSVM(),xxx,zassign,trigVect,anomVect,addVect,Nantrig,adist,nadist,daclass,addVectsToML,0);

                        errstream() << "Added " << addVect.size() << " out of " << xxx.size() << " training vectors (" << anomaliesRelabelled << " relabels, " << anomVect.size() << " ignored)/";
                    }

                    else if ( currcommand(0) == "-AU" )
                    {
                        std::stringstream dstr(currcommand(1));
                        std::stringstream xstr(currcommand(2));

                        Vector<gentype> dz(1);
                        Vector<SparseVector<gentype> > x(1);
                        gentype temp;

                        streamItIn(xstr,x("&",0),0);
                        safeatowhatever(dz("&",0),currcommand(1),argvariables);

                        addtrainingdata(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,x,dz,-1,0,0,temp);
                    }

                    else if ( currcommand(0) == "-AY" )
                    {
                        Vector<gentype> dz(1);
                        Vector<SparseVector<gentype> > x(1);
                        gentype temp,tmpg;

                        safeatowhatever(dz("&",0),currcommand(1),argvariables);
                        x("&",0) = safeatowhatever(tmpg,currcommand(2),argvariables).cast_vector(1);

                        addtrainingdata(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,x,dz,-1,0,0,temp);
                    }

                    else if ( currcommand(0) == "-AZ" )
                    {
                        Vector<gentype> dz(1);
                        Vector<SparseVector<gentype> > x(1);
                        gentype temp,tmpg;
                        int nInd = safeatoi(currcommand(3),argvariables);

                        if ( nInd < 0 )
                        {
                            STRTHROW("Negative SVM index "+currcommand(2)+" in -AZ");
                        }

                        safeatowhatever(dz("&",0),currcommand(1),argvariables);
                        x("&",0) = safeatowhatever(tmpg,currcommand(2),argvariables).cast_vector(1);

                        addtrainingdata(getMLref(svmThreadOwner,svmbase,threadInd,nInd,svmContext),xtemplate,x,dz,-1,0,0,temp);
                    }

                    else if ( currcommand(0) == "-AV" )
                    {
                        std::stringstream dstr(currcommand(1));
                        std::stringstream xstr(currcommand(2));

                        Vector<gentype> dz;
                        Vector<SparseVector<gentype> > x;
                        gentype temp;

                        dstr >> dz;
                        streamItIn(xstr,x,0);

                        addtrainingdata(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,x,dz,-1,0,0,temp);
                    }

                    else if ( currcommand(0) == "-AVv" )
                    {
                        gentype dd;
                        gentype xx;
                        gentype temp;

                        safeatowhatever(dd,currcommand(1),argvariables);
                        safeatowhatever(xx,currcommand(2),argvariables);

                        Vector<SparseVector<gentype> > x(xx.size());

                        int ij;

                        const Vector<gentype> &ghgh = (const Vector<gentype> &) xx;

                        for ( ij = 0 ; ij < xx.size() ; ++ij )
                        {
                             x("&",ij) = ghgh(ij);
                        }

                        addtrainingdata(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,x,(const Vector<gentype> &) dd,-1,0,0,temp);
                    }

                    else if ( currcommand(0) == "-AVV" )
                    {
                        gentype dd;
                        gentype xx;
                        gentype ss;
                        gentype temp;

                        safeatowhatever(dd,currcommand(1),argvariables);
                        safeatowhatever(xx,currcommand(2),argvariables);
                        safeatowhatever(ss,currcommand(3),argvariables);

                        Vector<SparseVector<gentype> > x(xx.size());

                        int ij;

                        const Vector<gentype> &ghgh = (const Vector<gentype> &) xx;

                        for ( ij = 0 ; ij < xx.size() ; ++ij )
                        {
                             x("&",ij) = (const Vector<gentype> &) ghgh(ij);
                        }

                        addtrainingdata(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,x,(const Vector<gentype> &) dd,(const Vector<gentype> &) ss,-1,0,0,temp);
                    }

                    else if ( currcommand(0) == "-AW" )
                    {
                        Vector<gentype> dz;
                        Vector<SparseVector<gentype> > x;
                        gentype temp;

                        int pointsadded = loadDataFromMatlab(currcommand(2),currcommand(1),x,dz,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).targType(),getsetExtVar);

                        addtrainingdata(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,x,dz,-1,0,0,temp);

                        errstream() << "Added " << pointsadded << " vectors from Matlab/";
                    }

                    else if ( currcommand(0) == "-AeA" ) 
                    {
                        int pointsadded = addbasisdataUU(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),currcommand(1));

                        errstream() << "Added " << pointsadded << " U basis vectors/";
                    }

                    else if ( currcommand(0) == "-AEA" ) 
                    {
                        int pointsadded = addbasisdataVV(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),currcommand(1));

                        errstream() << "Added " << pointsadded << " V basis vectors/";
                    }
                }

                time_used endtime = TIMECALL;
                loadtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << loadtime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Post-load steps

            if ( postloadopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Processing post-load operations... ";

                Vector<gentype> xmin(1);
                Vector<gentype> xmax(1);
                int xsamType = 0;
                int Nsamp    = DEFAULT_SAMPLES_SAMPLE;
                int samSplit = 0; //1;
                int samType  = 0;
                double samScale = 1.0;
                double samSlack = 0;

                xmin("&",0) = 0.0;
                xmax("&",0) = 1.0;

                while ( postloadopt.size() )
                {
                    currcommand = postloadopt(0);
                    postloadopt.remove(0);

                         if ( currcommand(0) == "-Snx"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelNone(); }
                    else if ( currcommand(0) == "-Sna"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelZeroMeanUnitVariance(); }
                    else if ( currcommand(0) == "-Snb"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelZeroMedianUnitVariance(); }
                    else if ( currcommand(0) == "-Snc"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelUnitRange(); }
                    else if ( currcommand(0) == "-SNa"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelZeroMeanUnitVariance(0,1); }
                    else if ( currcommand(0) == "-SNb"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelZeroMedianUnitVariance(0,1); }
                    else if ( currcommand(0) == "-SNc"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelUnitRange(0,1); }
                    else if ( currcommand(0) == "-SnA"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelZeroMeanUnitVariance(1,0); }
                    else if ( currcommand(0) == "-SnB"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelZeroMedianUnitVariance(1,0); }
                    else if ( currcommand(0) == "-SnC"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelUnitRange(1,0); }
                    else if ( currcommand(0) == "-SNA"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelZeroMeanUnitVariance(1,1); }
                    else if ( currcommand(0) == "-SNB"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelZeroMedianUnitVariance(1,1); }
                    else if ( currcommand(0) == "-SNC"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).normKernelUnitRange(1,1); }
                    else if ( currcommand(0) == "-Sra"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).randomise(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-Stl"  ) { safeatowhatever(xmin,currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-Stu"  ) { safeatowhatever(xmax,currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-StN"  ) { Nsamp     = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-Sts"  ) { samSplit  = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-Stx"  ) { xsamType  = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-Stt"  ) { samType   = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-Stc"  ) { samScale  = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-Stq"  ) { samSlack  = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-Snt"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setSampleMode(0,xmin,xmax,Nsamp,samSplit,samType,xsamType,samScale,samSlack); }
                    else if ( currcommand(0) == "-St"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setSampleMode(1,xmin,xmax,Nsamp,samSplit,samType,xsamType,samScale,samSlack); }
                    else if ( currcommand(0) == "-Spt"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setSampleMode(2,xmin,xmax,Nsamp,samSplit,samType,xsamType,samScale,samSlack); }
                    else if ( currcommand(0) == "-Sjt"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setSampleMode(3,xmin,xmax,Nsamp,samSplit,samType,xsamType,samScale,samSlack); }
                    else if ( currcommand(0) == "-Sjt+" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setSampleMode(4,xmin,xmax,Nsamp,samSplit,samType,xsamType,samScale,samSlack); }
                    else if ( currcommand(0) == "-Sjt-" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setSampleMode(5,xmin,xmax,Nsamp,samSplit,samType,xsamType,samScale,samSlack); }
                    else if ( currcommand(0) == "-Svc"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsigma_cut(safeatof(currcommand(1),argvariables)); }

                    else if ( currcommand(0) == "-Sdi"  )
                    {
                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).disable(-safeatoi(currcommand(1),argvariables)-1);
                    }

                    else if ( currcommand(0) == "-SdI"  )
                    {
                        int lb = safeatoi(currcommand(1),argvariables);
                        int ub = safeatoi(currcommand(2),argvariables);

                        retVector<int> tmpva;
                        Vector<int> iii(cntintvec(ub-lb,tmpva));

                        iii += lb;
                        iii *= -1;
                        iii -= -1;

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).disable(iii);
                    }

                    if ( currcommand(0) == "-Sa" )
                    {
                        std::ifstream datfile(currcommand(1).c_str());

                        argvariables("&",1)("&",15).makeString(currcommand(1));

                        if ( !datfile.is_open() )
                        {
                            STRTHROW("Unable to open file -Sa "+currcommand(1));
                        }

                        Vector<gentype> alpha(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).alpha());

                        datfile >> alpha;
                        datfile.close();

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setAlpha(alpha);
                    }

                    else if ( currcommand(0) == "-Sai" )
                    {
                        Vector<gentype> alpha(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).alpha());

                        safeatowhatever(alpha("&",safeatoi(currcommand(1),argvariables)),currcommand(2),argvariables);

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setAlpha(alpha);
                    }

                    else if ( currcommand(0) == "-Saa" )
                    {
                        std::stringstream xstr(currcommand(1));

                        Vector<gentype> alpha(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).alpha());

                        streamItIn(xstr,alpha,0);

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setAlpha(alpha);
                    }

                    else if ( currcommand(0) == "-Sb" )
                    {
                        argvariables("&",1)("&",16).makeString(currcommand(1));

                        std::ifstream datfile(currcommand(1).c_str());

                        if ( !datfile.is_open() )
                        {
                            STRTHROW("Unable to open file -Sb "+currcommand(1));
                        }

                        gentype bias;
                        datfile >> bias;
                        datfile.close();
                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBias(bias);
                    }

                    else if ( currcommand(0) == "-Sbb" )
                    {
                        gentype tmpg;

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setBias(safeatowhatever(tmpg,currcommand(1),argvariables));
                    }

                    else if ( currcommand(0) == "-SA" )
                    {
                        std::ifstream datfile(currcommand(1).c_str());

                        argvariables("&",1)("&",15).makeString(currcommand(1));

                        if ( !datfile.is_open() )
                        {
                            STRTHROW("Unable to open file -SA "+currcommand(1));
                        }

                        Matrix<double> lambda(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).lambdaKB());

                        datfile >> lambda;
                        datfile.close();

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setlambdaKB(lambda);
                    }

                    else if ( currcommand(0) == "-SAi" )
                    {
                        Matrix<double> lambda(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).lambdaKB());

                        safeatowhatever(lambda("&",safeatoi(currcommand(1),argvariables),safeatoi(currcommand(2),argvariables)),currcommand(3),argvariables);

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setlambdaKB(lambda);
                    }

                    else if ( currcommand(0) == "-SAA" )
                    {
                        std::stringstream xstr(currcommand(1));

                        Matrix<double> lambda(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).lambdaKB());

                        streamItIn(xstr,lambda,0);

                        getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setlambdaKB(lambda);
                    }

                    else if ( currcommand(0) == "-Sx"  )
                    {
                        ML_Base &model = getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        gentype f;

                        safeatowhatever(f,currcommand(1),argvariables);

                        Vector<gentype> y(model.y());

                        int i;

                        for ( i = 0 ; i < y.size() ; ++i )
                        {
                            y("&",i) = f.evalyonly(model.y()(i));
                        }

                        model.sety(y);
                    }
                }

                time_used endtime = TIMECALL;
                postloadtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << postloadtime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Learning steps

            if ( learningopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Setting Learning parameters... ";
                std::string tfilename;

                while ( learningopt.size() )
                {
                    currcommand = learningopt(0);
                    learningopt.remove(0);

                         if ( currcommand(0) == "-c"    ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setC(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-dc"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setD(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-ec"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setE(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-fc"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setF(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-Gc"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setG(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-dcs"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsigmaD(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-ecs"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsigmaE(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-fcs"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsigmaF(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-Gcs"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsigmaG(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-trf"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).settunev(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-trk"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setpegk(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-tmv"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setminv(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-rfs"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setReOnly(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-dia"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setinAdam(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-dog"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setoutGrad(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-nN"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setNRff(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-mtl"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setNRffRep(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-th"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).settheta(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-thn"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsimnorm(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-c+"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(+1,safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-c-"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(-1,safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-c="   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(2,safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-cd"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-cs"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setC(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).C()*safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-c+s"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(+1,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).Cclass(+1)*safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-c-s"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(-1,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).Cclass(-1)*safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-c=s"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(2,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).Cclass(2)*safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-cds"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(safeatoi(currcommand(1),argvariables),getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).Cclass(safeatoi(currcommand(1),argvariables))*safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-j"    ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(+1,safeatof(currcommand(1),argvariables)); getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(-1,1); }
                    else if ( currcommand(0) == "-jc"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(+1,safeatof(currcommand(1),argvariables)); getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCclass(-1,1); }
                    else if ( currcommand(0) == "-dd"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setRejectThreshold(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-w"    ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).seteps(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-w+"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(+1,safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-w-"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(-1,safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-w="   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(2,safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-wd"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-ws"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).seteps(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).eps()*safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-w+s"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(+1,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).epsclass(+1)*safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-w-s"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(-1,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).epsclass(-1)*safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-w=s"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(2,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).epsclass(2)*safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-wds"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(safeatoi(currcommand(1),argvariables),getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).Cclass(safeatoi(currcommand(1),argvariables))*safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-jw"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(+1,safeatof(currcommand(1),argvariables)); getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsclass(-1,1); }
                    else if ( currcommand(0) == "-cw"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setCweight(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-ww"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setepsweight(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-mvb"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setbetarank(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-ds"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsigma(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).sigma()*safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-dw"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsigmaweight(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-mlc"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setregC(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-Mn"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setNoMonotonicConstraints(); }
                    else if ( currcommand(0) == "-Mi"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setForcedMonotonicIncreasing(); }
                    else if ( currcommand(0) == "-Md"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setForcedMonotonicDecreasing(); }
                    else if ( currcommand(0) == "-nm"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setm(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-Tl"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setnu(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-Tq"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setnuQuad(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-Nl"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setLinBiasForce(-2,safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-Nq"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setQuadBiasForce(-2,safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-Nld"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setLinBiasForce(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-Nqd"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setQuadBiasForce(safeatoi(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }
                    else if ( currcommand(0) == "-mvi"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmaxitermvrank(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-mvlr" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setlrmvrank(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-mvzt" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setztmvrank(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-Fi"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmaxiterfuzzt(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-Flr"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setlrfuzzt(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-Fzt"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setztfuzzt(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-Fc"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setcostfnfuzzt(currcommand(1)); }
                    else if ( currcommand(0) == "-blx"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setoutfn(currcommand(1)); }
                    else if ( currcommand(0) == "-bly"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmexcall(currcommand(1)); }
                    else if ( currcommand(0) == "-blz"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmexcallid(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-bls"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsyscall(currcommand(1)); }
                    else if ( currcommand(0) == "-bfx"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setxfilename(currcommand(1)); }
                    else if ( currcommand(0) == "-bfy"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setyfilename(currcommand(1)); }
                    else if ( currcommand(0) == "-bfxy" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setxyfilename(currcommand(1)); }
                    else if ( currcommand(0) == "-bfyx" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setyxfilename(currcommand(1)); }
                    else if ( currcommand(0) == "-bfr"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setrfilename(currcommand(1)); }
                    else if ( currcommand(0) == "-k"    ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setk(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-K"    ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setktp(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-d"    ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsigma(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-ccs"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsigma(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-iz"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setzref(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-ie"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setehimethod(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-is"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setscaltype (safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-ia"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setscalalpha(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-in"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setNsamp    (safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-il"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setsampSlack(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-mu"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setmpri(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-mugt" ) { gentype mudef(currcommand(1)); getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setprival(mudef); }
                    else if ( currcommand(0) == "-muml" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setpriml(&(getMLref(svmThreadOwner,svmbase,threadInd,safeatoi(currcommand(1),argvariables),svmContext))); }

                    else if ( currcommand(0) == "-Bf"   )
                    {
                        safeatowhatever(biasdefault,currcommand(1),argvariables);

                        if ( getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).isFixedBias() )
                        {
                            getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setFixedBias(biasdefault);
                        }
                    }

                    else if ( currcommand(0) == "-m"    )
                    {
                        if ( currcommand(1) == "r" )
                        {
                            getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setKreal();
                        }

                        else if ( currcommand(1) == "m" )
                        {
                            getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).setKunreal();
                        }

                        else
                        {
                            STRTHROW("Error: -m arguments are {r,m}");
                        }
                    }
                }

                time_used endtime = TIMECALL;
                learningtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << learningtime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Kernel steps

            if ( kernelopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Setting Kernel parameters... ";

                int kernnum = 0;
                int firstcall = 1;

                int ekernnum = 0;
                int efirstcall = 1;

                int rkernnum = 0;
                int rfirstcall = 1;

                while ( kernelopt.size() )
                {
                    currcommand = kernelopt(0);
                    kernelopt.remove(0);

                    if ( currcommand(0)[1] == 'e' )
                    {
                        // Process output kernel

                        std::string currcommandis = "-" + ((currcommand(0)).substr(2));

                        ML_Base &kernML = getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext);
                        MercerKernel &theKern = kernML.getUUOutputKernel_unsafe();

                        processKernel(kernML,theKern,currcommandis,currcommand,1,argvariables,ekernnum,efirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        efirstcall = 0;
                    }

                    else if ( currcommand(0)[1] == 'r' )
                    {
                        // Process RFF similarity space kernel

                        std::string currcommandis = "-" + ((currcommand(0)).substr(2));

                        ML_Base &kernML = getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext);
                        MercerKernel &theKern = kernML.getRFFKernel_unsafe();

                        processKernel(kernML,theKern,currcommandis,currcommand,3,argvariables,rkernnum,rfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        rfirstcall = 0;
                    }

                    else
                    {
                        // Process main kernel

                        std::string currcommandis = currcommand(0);

                        ML_Base &kernML = getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext);
                        MercerKernel &theKern = kernML.getKernel_unsafe();

                        processKernel(kernML,theKern,currcommandis,currcommand,0,argvariables,kernnum,firstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        firstcall = 0;
                    }
                }

                time_used endtime = TIMECALL;
                kerneltime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << kerneltime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Tuning steps

            if ( tuningopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Tuning SVM parameters... ";

                std::string tfilename;

                while ( tuningopt.size() )
                {
                    currcommand = tuningopt(0);
                    tuningopt.remove(0);

                    if      ( currcommand(0) == "-cA"      ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).autosetCNKmean();                                                                                 }
                    else if ( currcommand(0) == "-bal"     ) { balc(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext));                                                                                            }
                    else if ( currcommand(0) == "-cB"      ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).autosetCNKmedian();                                                                               }
                    else if ( currcommand(0) == "-cAN"     ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).autosetCKmean();                                                                                  }
                    else if ( currcommand(0) == "-cBN"     ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).autosetCKmedian();                                                                                }
                    else if ( currcommand(0) == "-cX"      ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).autosetCscaled(safeatof(currcommand(1),argvariables));                                            }
                    else if ( currcommand(0) == "-cua"     ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).autosetOff();                                                                                     }
                    else if ( currcommand(0) == "-tkL"     ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(1,safeatof(currcommand(1),argvariables),1,0,nullptr); }
                    else if ( currcommand(0) == "-tkloo"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(2,safeatof(currcommand(1),argvariables),1,0,nullptr); }
                    else if ( currcommand(0) == "-tkrec"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(3,safeatof(currcommand(1),argvariables),1,0,nullptr); }
                    else if ( currcommand(0) == "-tcL"     ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(1,safeatof(currcommand(1),argvariables),0,1,nullptr); }
                    else if ( currcommand(0) == "-tcloo"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(2,safeatof(currcommand(1),argvariables),0,1,nullptr); }
                    else if ( currcommand(0) == "-tcrec"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(3,safeatof(currcommand(1),argvariables),0,1,nullptr); }
                    else if ( currcommand(0) == "-teL"     ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(1,safeatof(currcommand(1),argvariables),0,2,nullptr); }
                    else if ( currcommand(0) == "-teloo"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(2,safeatof(currcommand(1),argvariables),0,2,nullptr); }
                    else if ( currcommand(0) == "-terec"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(3,safeatof(currcommand(1),argvariables),0,2,nullptr); }
                    else if ( currcommand(0) == "-tceL"    ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(1,safeatof(currcommand(1),argvariables),0,3,nullptr); }
                    else if ( currcommand(0) == "-tceloo"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(2,safeatof(currcommand(1),argvariables),0,3,nullptr); }
                    else if ( currcommand(0) == "-tcerec"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(3,safeatof(currcommand(1),argvariables),0,3,nullptr); }
                    else if ( currcommand(0) == "-tkcL"    ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(1,safeatof(currcommand(1),argvariables),1,1,nullptr); }
                    else if ( currcommand(0) == "-tkcloo"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(2,safeatof(currcommand(1),argvariables),1,1,nullptr); }
                    else if ( currcommand(0) == "-tkcrec"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(3,safeatof(currcommand(1),argvariables),1,1,nullptr); }
                    else if ( currcommand(0) == "-tkeL"    ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(1,safeatof(currcommand(1),argvariables),1,2,nullptr); }
                    else if ( currcommand(0) == "-tkeloo"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(2,safeatof(currcommand(1),argvariables),1,2,nullptr); }
                    else if ( currcommand(0) == "-tkerec"  ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(3,safeatof(currcommand(1),argvariables),1,2,nullptr); }
                    else if ( currcommand(0) == "-tkceL"   ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(1,safeatof(currcommand(1),argvariables),1,3,nullptr); }
                    else if ( currcommand(0) == "-tkceloo" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(2,safeatof(currcommand(1),argvariables),1,3,nullptr); }
                    else if ( currcommand(0) == "-tkcerec" ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getML().tuneKernel(3,safeatof(currcommand(1),argvariables),1,3,nullptr); }
                    else if ( currcommand(0) == "-NlA"     ) { getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).autosetLinBiasForce(safeatof(currcommand(1),argvariables),safeatof(currcommand(2),argvariables)); }
                }

                time_used endtime = TIMECALL;
                tuningtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << tuningtime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Grid-search steps

            if ( gridopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Grid-search and direct SVM parameter selection... ";

                std::string interstring;
                std::string prestring;
                std::string midstring;

                // NB: these are rather large, so don't put them on the stack!

                GridOptions   *xgopts = nullptr;
                DIRectOptions *xdopts = nullptr;
                NelderOptions *xnopts = nullptr;
                BayesOptions  *xbopts = nullptr;

                MEMNEW(xgopts,GridOptions );
                MEMNEW(xdopts,DIRectOptions);
                MEMNEW(xnopts,NelderOptions);
                MEMNEW(xbopts,BayesOptions );

                gentype xfnis; xfnis.makeNull();

                int bayesModelNum = -1;
                int numOptReps    = 1;

                Vector<gentype> defxres;

                // RKHS kernel stuff

                int gphkernnum   = 0;
                int gphfirstcall = 1;

                int gPkernnum   = 0;
                int gPfirstcall = 1;

                int gkkernnum   = 0;
                int gkfirstcall = 1;

                int gekkernnum   = 0;
                int gekfirstcall = 1;

                int grkkernnum   = 0;
                int grkfirstcall = 1;

                int gkkernnum_rff   = 0;
                int gkfirstcall_rff = 1;

                int gekkernnum_rff   = 0;
                int gekfirstcall_rff = 1;

                int grkkernnum_rff   = 0;
                int grkfirstcall_rff = 1;

                int gskkernnum   = 0;
                int gskfirstcall = 1;

                int gsekkernnum   = 0;
                int gsekfirstcall = 1;

                int gsrkkernnum   = 0;
                int gsrkfirstcall = 1;


                std::string optname = "";

                int numtests = 0;
                {
                    std::stringstream ssj;
                    ssj << numtests;
                    optname = "sim "+ssj.str();
                }

                Vector<Vector<Vector<gentype> > > vecallxres;
                Vector<Vector<Vector<gentype> > > vecallrawxres;
                Vector<Vector<gentype> >          vecallfres;
                Vector<Vector<Vector<gentype> > > vecallcres;
                Vector<Vector<gentype> >          vecallmres;
                Vector<Vector<int> >              vecallires;
                Vector<Vector<gentype> >          vecallsres;
                Vector<Vector<double> >           vecallhres;
                Vector<Vector<double> >           vecsscore;

                Vector<gentype>          vecmeanfres;
                Vector<gentype>          vecmeanires;
                Vector<gentype>          vecmeantres;
                Vector<gentype>          vecmeanTres;
                Vector<Vector<gentype> > vecmeanallfres;
                Vector<Vector<gentype> > vecmeanallmres;

                Vector<gentype>          vecvarfres;
                Vector<gentype>          vecvarires;
                Vector<gentype>          vecvartres;
                Vector<gentype>          vecvarTres;
                Vector<Vector<gentype> > vecvarallfres;
                Vector<Vector<gentype> > vecvarallmres;

                Vector<std::string> vecnames;

                std::string optplotname = logfile+"_opt"; //.plot.pdf";
                std::string optplotdataname = logfile+"_optdat"; //.plot.pdat";
                std::string optplottitle = "";
                int optplotoutformat = 2;
                double optplotymin = 1;
                double optplotymax = 0;
                int plotoptres = 0;

                while ( gridopt.size() )
                {
                    currcommand = gridopt(0);
                    gridopt.remove(0);

                         if ( currcommand(0) == "-gy"   ) { (*xbopts).extgoptssingleobj.maxtraintime  = ( (*xnopts).maxtraintime  = ( (*xgopts).maxtraintime  = ( (*xdopts).maxtraintime  = ( (*xbopts).maxtraintime  = safeatof(currcommand(1),argvariables) ) ) ) ); }
                    else if ( currcommand(0) == "-gfm"  ) { (*xbopts).extgoptssingleobj.hardmin       = ( (*xnopts).hardmin       = ( (*xgopts).hardmin       = ( (*xdopts).hardmin       = ( (*xbopts).hardmin       = safeatof(currcommand(1),argvariables) ) ) ) ); }
                    else if ( currcommand(0) == "-gfM"  ) { (*xbopts).extgoptssingleobj.softmin       = ( (*xnopts).softmin       = ( (*xgopts).softmin       = ( (*xdopts).softmin       = ( (*xbopts).softmin       = safeatof(currcommand(1),argvariables) ) ) ) ); }
                    else if ( currcommand(0) == "-gfu"  ) { (*xbopts).extgoptssingleobj.hardmax       = ( (*xnopts).hardmax       = ( (*xgopts).hardmax       = ( (*xdopts).hardmax       = ( (*xbopts).hardmax       = safeatof(currcommand(1),argvariables) ) ) ) ); }
                    else if ( currcommand(0) == "-gfU"  ) { (*xbopts).extgoptssingleobj.softmax       = ( (*xnopts).softmax       = ( (*xgopts).softmax       = ( (*xdopts).softmax       = ( (*xbopts).softmax       = safeatof(currcommand(1),argvariables) ) ) ) ); }

                    else if ( currcommand(0) == "-gpln"  ) { optplotname      = currcommand(1);                        }
                    else if ( currcommand(0) == "-gpld"  ) { optplotdataname  = currcommand(1);                        }
                    else if ( currcommand(0) == "-gplT"  ) { optplottitle     = currcommand(1);                        }
                    else if ( currcommand(0) == "-gplt"  ) { optplotoutformat = safeatoi(currcommand(1),argvariables); (*xbopts).modeloutformat = optplotoutformat; }
                    else if ( currcommand(0) == "-gplm"  ) { optplotymin      = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gplM"  ) { optplotymax      = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gplot" ) { plotoptres       = 1;                                     }

                    else if ( currcommand(0) == "-gBy"  ) { (*xbopts).goptsmultiobj.maxtraintime = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gBfm" ) { (*xbopts).goptsmultiobj.hardmin      = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gBfN" ) { (*xbopts).goptsmultiobj.softmin      = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gBfU" ) { (*xbopts).goptsmultiobj.softmax      = safeatof(currcommand(1),argvariables); }

                    else if ( currcommand(0) == "-gpr"  ) { (*xnopts).randReproject = ( (*xgopts).randReproject = ( (*xdopts).randReproject = ( (*xbopts).randReproject = safeatoi(currcommand(1),argvariables) ) ) ); }
                    else if ( currcommand(0) == "-gnp"  ) { (*xnopts).isProjection  = ( (*xgopts).isProjection  = ( (*xdopts).isProjection  = ( (*xbopts).isProjection  = 0                                     ) ) ); }
                    else if ( currcommand(0) == "-gpb"  ) { (*xnopts).isProjection  = ( (*xgopts).isProjection  = ( (*xdopts).isProjection  = ( (*xbopts).isProjection  = 3                                     ) ) ); }
                    else if ( currcommand(0) == "-gpB"  ) { (*xnopts).isProjection  = ( (*xgopts).isProjection  = ( (*xdopts).isProjection  = ( (*xbopts).isProjection  = 4                                     ) ) );
                                                            (*xnopts).bernstart     = ( (*xgopts).bernstart     = ( (*xdopts).bernstart     = ( (*xbopts).bernstart     = safeatoi(currcommand(1),argvariables) ) ) ); }
                    else if ( currcommand(0) == "-gph"  ) { (*xnopts).isProjection  = ( (*xgopts).isProjection  = ( (*xdopts).isProjection  = ( (*xbopts).isProjection  = 5                                     ) ) ); }
                    else if ( currcommand(0) == "-gpd"  ) { (*xnopts).fnDim         = ( (*xgopts).fnDim         = ( (*xdopts).fnDim         = ( (*xbopts).fnDim         = safeatoi(currcommand(1),argvariables) ) ) ); }
                    else if ( currcommand(0) == "-gf"   ) { (*xnopts).useScalarFn   = ( (*xgopts).useScalarFn   = ( (*xdopts).useScalarFn   = ( (*xbopts).useScalarFn   = safeatoi(currcommand(1),argvariables) ) ) ); }
                    else if ( currcommand(0) == "-gc"   ) { (*xnopts).includeConst  = ( (*xgopts).includeConst  = ( (*xdopts).includeConst  = ( (*xbopts).includeConst  = safeatoi(currcommand(1),argvariables) ) ) ); }
                    else if ( currcommand(0) == "-gC"   ) { (*xnopts).whatConst     = ( (*xgopts).whatConst     = ( (*xdopts).whatConst     = ( (*xbopts).whatConst     = safeatoi(currcommand(1),argvariables) ) ) ); }
                    else if ( currcommand(0) == "-gns"  ) { (*xnopts).xNsamp        = ( (*xgopts).xNsamp        = ( (*xdopts).xNsamp        = ( (*xbopts).xNsamp        = safeatoi(currcommand(1),argvariables) ) ) ); }
                    else if ( currcommand(0) == "-gkm"  ) { (*xnopts).xSampSplit    = ( (*xgopts).xSampSplit    = ( (*xdopts).xSampSplit    = ( (*xbopts).xSampSplit    = safeatoi(currcommand(1),argvariables) ) ) ); }
                    else if ( currcommand(0) == "-gkt"  ) { (*xnopts).xSampType     = ( (*xgopts).xSampType     = ( (*xdopts).xSampType     = ( (*xbopts).xSampType     = safeatoi(currcommand(1),argvariables) ) ) ); }
                    else if ( currcommand(0) == "-gao"  ) { (*xnopts).usealtoptfn   = ( (*xgopts).usealtoptfn   = ( (*xdopts).usealtoptfn   = ( (*xbopts).usealtoptfn   = 1                                     ) ) );
                                                            (*xnopts).altoptfn      = ( (*xgopts).altoptfn      = ( (*xdopts).altoptfn      = ( (*xbopts).altoptfn      = safeatog(currcommand(1),argvariables) ) ) ); }
                    else if ( currcommand(0) == "-gan"  ) { (*xnopts).usealtoptfn   = ( (*xgopts).usealtoptfn   = ( (*xdopts).usealtoptfn   = ( (*xbopts).usealtoptfn   = 0                                     ) ) ); }

                    else if ( currcommand(0) == "-gp"  )
                    {
                        (*xnopts).isProjection = 1; (*xnopts).defrandDirtemplateVec.setoutfn(currcommand(1));
                        (*xgopts).isProjection = 1; (*xgopts).defrandDirtemplateVec.setoutfn(currcommand(1));
                        (*xdopts).isProjection = 1; (*xdopts).defrandDirtemplateVec.setoutfn(currcommand(1));
                        (*xbopts).isProjection = 1; (*xbopts).defrandDirtemplateVec.setoutfn(currcommand(1));
                    }

                    else if ( currcommand(0) == "-gP"  )
                    {
                        (*xnopts).isProjection = 2;
                        (*xgopts).isProjection = 2;
                        (*xdopts).isProjection = 2;
                        (*xbopts).isProjection = 2;

                        int kernnum = 0;

                        Vector<gentype> nkernRealConsts((*xnopts).defrandDirtemplateFnGP.getKernel().cRealConstants(kernnum));
                        Vector<gentype> gkernRealConsts((*xgopts).defrandDirtemplateFnGP.getKernel().cRealConstants(kernnum));
                        Vector<gentype> dkernRealConsts((*xdopts).defrandDirtemplateFnGP.getKernel().cRealConstants(kernnum));
                        Vector<gentype> bkernRealConsts((*xbopts).defrandDirtemplateFnGP.getKernel().cRealConstants(kernnum));

                        gentype temparg;

                        safeatowhatever(temparg,currcommand(1),argvariables);

                        nkernRealConsts("&",0) = temparg;
                        gkernRealConsts("&",0) = temparg;
                        dkernRealConsts("&",0) = temparg;
                        bkernRealConsts("&",0) = temparg;

                        (*xnopts).defrandDirtemplateFnGP.getKernel_unsafe().setRealConstants(nkernRealConsts,kernnum);
                        (*xgopts).defrandDirtemplateFnGP.getKernel_unsafe().setRealConstants(gkernRealConsts,kernnum);
                        (*xdopts).defrandDirtemplateFnGP.getKernel_unsafe().setRealConstants(dkernRealConsts,kernnum);
                        (*xbopts).defrandDirtemplateFnGP.getKernel_unsafe().setRealConstants(bkernRealConsts,kernnum);

                        (*xnopts).defrandDirtemplateFnGP.resetKernel(0);
                        (*xgopts).defrandDirtemplateFnGP.resetKernel(0);
                        (*xdopts).defrandDirtemplateFnGP.resetKernel(0);
                        (*xbopts).defrandDirtemplateFnGP.resetKernel(0);
                    }

                    else if ( ( currcommand(0).substr(0,4) == "-gPk" ) || ( currcommand(0) == "-gPmtb" ) || ( currcommand(0) == "-gPbmx" ) )
                    {
                        std::string currcommandis = "-" + ((currcommand(0)).substr(3));

                        ML_Base &kernML = (*xbopts).defrandDirtemplateFnGP;
                        MercerKernel &theKern = kernML.getKernel_unsafe();

                        processKernel(kernML,theKern,currcommandis,currcommand,0,argvariables,gPkernnum,gPfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        (*xnopts).defrandDirtemplateFnGP.getKernel_unsafe() = theKern;
                        (*xgopts).defrandDirtemplateFnGP.getKernel_unsafe() = theKern;
                        (*xdopts).defrandDirtemplateFnGP.getKernel_unsafe() = theKern;

                        gPfirstcall = 0;
                    }

                    else if ( ( currcommand(0).substr(0,5) == "-gphk" ) || ( currcommand(0) == "-gphmtb" ) || ( currcommand(0) == "-gphbmx" ) )
                    {
                        // Process RKHS projection kernel

                        std::string currcommandis = "-" + ((currcommand(0)).substr(4));

                        ML_Base dummyML;

                        processKernel(dummyML,(*xbopts).defrandDirtemplateFnRKHS.kern("&"),currcommandis,currcommand,2,argvariables,gphkernnum,gphfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        (*xnopts).defrandDirtemplateFnRKHS.kern("&") = (*xbopts).defrandDirtemplateFnRKHS.kern();
                        (*xgopts).defrandDirtemplateFnRKHS.kern("&") = (*xbopts).defrandDirtemplateFnRKHS.kern();
                        (*xdopts).defrandDirtemplateFnRKHS.kern("&") = (*xbopts).defrandDirtemplateFnRKHS.kern();

                        gphfirstcall = 0;
                    }

                    else if ( currcommand(0) == "-g+"  )
                    {
                        Vector<gentype> pterms;

                        safeatowhatever(pterms,currcommand(1),argvariables);

                        gentype penterm(0.0);
                        gentype makepos("max([ 0 x ])");

                        if ( pterms.size() )
                        {
                            int ij;

                            for ( ij = 0 ; ij < pterms.size() ; ++ij )
                            {
                                penterm += makepos(pterms(ij));
                            }
                        }

                        errstream() << "Setting p(x) penalty " << penterm << "\n";

                        (*xnopts).penterm = penterm;
                        (*xgopts).penterm = penterm;
                        (*xdopts).penterm = penterm;
                        (*xbopts).penterm = penterm;
                    }







                    else if ( currcommand(0) == "-ggm" ) { (*xgopts).numZooms = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-ggi" ) { (*xgopts).zoomFact = safeatof(currcommand(1),argvariables); }





                    else if ( currcommand(0) == "-gdc"  ) { (*xbopts).extgoptssingleobj.maxits            = ( (*xdopts).maxits            = safeatoi(currcommand(1),argvariables)                                      ); }
                    else if ( currcommand(0) == "-gdf"  ) { (*xbopts).extgoptssingleobj.maxevals          = ( (*xdopts).maxevals          = safeatoi(currcommand(1),argvariables)                                      ); }
                    else if ( currcommand(0) == "-gde"  ) { (*xbopts).extgoptssingleobj.eps               = ( (*xdopts).eps               = safeatof(currcommand(1),argvariables)                                      ); }
                    else if ( currcommand(0) == "-gda"  ) { (*xbopts).extgoptssingleobj.algorithm         = ( (*xdopts).algorithm         = safeatoi(currcommand(1),argvariables) ? DIRECT_ORIGINAL : DIRECT_GABLONSKY ); }
                    else if ( currcommand(0) == "-gdy"  ) { (*xbopts).extgoptssingleobj.traintimeoverride = ( (*xdopts).traintimeoverride = safeatof(currcommand(1),argvariables)                                      ); }

                    else if ( currcommand(0) == "-gBdc" ) { (*xbopts).goptsmultiobj.maxits            = safeatoi(currcommand(1),argvariables);                                      }
                    else if ( currcommand(0) == "-gBdf" ) { (*xbopts).goptsmultiobj.maxevals          = safeatoi(currcommand(1),argvariables);                                      }
                    else if ( currcommand(0) == "-gBde" ) { (*xbopts).goptsmultiobj.eps               = safeatof(currcommand(1),argvariables);                                      }
                    else if ( currcommand(0) == "-gBda" ) { (*xbopts).goptsmultiobj.algorithm         = safeatoi(currcommand(1),argvariables) ? DIRECT_ORIGINAL : DIRECT_GABLONSKY; }
                    else if ( currcommand(0) == "-gBdy" ) { (*xbopts).goptsmultiobj.traintimeoverride = safeatof(currcommand(1),argvariables);                                      }







                    else if ( currcommand(0) == "-gNa"  ) { (*xnopts).minf_max = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gNb"  ) { (*xnopts).ftol_rel = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gNc"  ) { (*xnopts).ftol_abs = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gNd"  ) { (*xnopts).xtol_rel = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gNg"  ) { (*xnopts).xtol_abs = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gNe"  ) { (*xnopts).maxeval  = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gNf"  ) { (*xnopts).method   = safeatoi(currcommand(1),argvariables); }







                    else if ( currcommand(0) == "-gmn"   ) { (*xbopts).sigmuseparate = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmt"   ) { (*xbopts).modeltype     = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmrff" ) { (*xbopts).modelrff      = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmq"   ) { (*xbopts).oracleMode    = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmw"   ) { ((*xbopts).extmuapprox).resize(1)   = &(getMLref(svmThreadOwner,svmbase,threadInd,( ( bayesModelNum = safeatoi(currcommand(1),argvariables) ) ),svmContext)); }
                    else if ( currcommand(0) == "-gmo"   ) { (*xbopts).ismoo         = 1; }
                    else if ( currcommand(0) == "-gms"   ) { (*xbopts).ismoo         = 0; }
                    else if ( currcommand(0) == "-gmr"   ) { (*xbopts).makenoise     = 1; }
                    else if ( currcommand(0) == "-gmR"   ) { (*xbopts).makenoise     = 0; }
                    else if ( currcommand(0) == "-gma"   ) { (*xbopts).moodim        = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmT"   ) { std::stringstream xstr(currcommand(1)); SparseVector<gentype> xt; streamItIn(xstr,xt,0); (*xbopts).xtemplate = xt; }
                    else if ( currcommand(0) == "-gmd"   ) { (*xbopts).default_model_setsigma(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-gmbgn" ) { (*xbopts).default_model_setvarApproxim(safeatoi(currcommand(1),argvariables));}
                    else if ( currcommand(0) == "-gmsbgn") { (*xbopts).default_modelaugx_setvarApproxim(safeatoi(currcommand(1),argvariables));}
                    else if ( currcommand(0) == "-gmgtbgn") { (*xbopts).default_modelcgt_setvarApproxim(safeatoi(currcommand(1),argvariables));}
                    else if ( currcommand(0) == "-gmg"   ) { gentype temp; safeatowhatever(temp,currcommand(1),argvariables); (*xbopts).default_model_setkernelg(temp); }
                    else if ( currcommand(0) == "-gmgg"  ) { Vector<gentype> xxscale; SparseVector<gentype> xscale; xscale = safeatowhatever(xxscale,currcommand(1),argvariables); (*xbopts).default_model_setkernelgg(xscale); }
                    else if ( currcommand(0) == "-gmma"  ) { (*xbopts).tunemu        = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmmb"  ) { (*xbopts).tunesigma     = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmmc"  ) { (*xbopts).tunesrcmod    = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmmd"  ) { (*xbopts).tunediffmod   = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmx"   ) { (*xbopts).tranmeth      = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmxa"  ) { (*xbopts).alpha0        = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmxb"  ) { (*xbopts).beta0         = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmy"   ) { (*xbopts).kernapprox    = &(getMLref(svmThreadOwner,svmbase,threadInd,safeatoi(currcommand(1),argvariables),svmContext)); }
                    else if ( currcommand(0) == "-gmya"  ) { (*xbopts).kxfnum        = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmyb"  ) { (*xbopts).kxfnorm       = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmsc"  ) { (*xbopts).usemodelaugx  = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmsn"  ) { (*xbopts).modelnaive    = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmsy"  ) { (*xbopts).ennornaive    = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmsa"  ) { (*xbopts).default_modelaugx_settspaceDim(safeatoi(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-gmsd"  ) { (*xbopts).default_modelaugx_setsigma(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-gmsg"  ) { gentype temp; safeatowhatever(temp,currcommand(1),argvariables); (*xbopts).default_modelaugx_setkernelg(temp); }
                    else if ( currcommand(0) == "-gmsgg" ) { Vector<gentype> xxscale; SparseVector<gentype> xscale; xscale = safeatowhatever(xxscale,currcommand(1),argvariables); (*xbopts).default_modelaugx_setkernelgg(xscale); }
                    else if ( currcommand(0) == "-gmsmd" ) { (*xbopts).tuneaugxmod   = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmgtc"  ) { (*xbopts).numcgt  = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmgtd"  ) { (*xbopts).default_modelcgt_setsigma(safeatof(currcommand(1),argvariables)); }
                    else if ( currcommand(0) == "-gmgtg"  ) { gentype temp; safeatowhatever(temp,currcommand(1),argvariables); (*xbopts).default_modelcgt_setkernelg(temp); }
                    else if ( currcommand(0) == "-gmgtgg" ) { Vector<gentype> xxscale; SparseVector<gentype> xscale; xscale = safeatowhatever(xxscale,currcommand(1),argvariables); (*xbopts).default_modelcgt_setkernelgg(xscale); }
                    else if ( currcommand(0) == "-gmgtmd" ) { (*xbopts).tunecgt      = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmLf"  ) { (*xbopts).plotfreq      = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmLF"  ) { (*xbopts).modeloutformat= safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gmLn"  ) { (*xbopts).modelname     = currcommand(1); }
                    else if ( currcommand(0) == "-gmhplb") { (*xbopts).modelbaseline = currcommand(1); }

                    else if ( currcommand(0) == "-gmsw" )
                    {
                        Vector<int> fxapproxInd;

                        safeatowhatever(fxapproxInd,currcommand(1),argvariables);

                        int ifr;

                        for ( ifr = 0 ; ifr < fxapproxInd.size() ; ++ifr )
                        {
                            (*xbopts).extfxapprox = &(getMLref(svmThreadOwner,svmbase,threadInd,fxapproxInd(ifr),svmContext));
                        }
                    }

                    else if ( ( currcommand(0).substr(0,5) == "-gmek" ) || ( currcommand(0) == "-gmemtb" ) || ( currcommand(0) == "-gmebmx" ) )
                    {
                        // output kernel

                        std::string currcommandis = "-" + ((currcommand(0)).substr(4));

                        {
                            ML_Base &kernML = (*xbopts).altmuapprox;
                            MercerKernel &theKern = kernML.getUUOutputKernel_unsafe();

                            processKernel(kernML,theKern,currcommandis,currcommand,1,argvariables,gekkernnum,gekfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                            gekfirstcall = 0;
                        }

                        {
                            ML_Base &kernML = (*xbopts).altmuapprox_rff;
                            MercerKernel &theKern = kernML.getUUOutputKernel_unsafe();

                            processKernel(kernML,theKern,currcommandis,currcommand,1,argvariables,gekkernnum_rff,gekfirstcall_rff,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                            gekfirstcall_rff = 0;
                        }
                    }

                    else if ( ( currcommand(0).substr(0,5) == "-gmrk" ) || ( currcommand(0) == "-gmrmtb" ) || ( currcommand(0) == "-gmrbmx" ) )
                    {
                        // RFF similarity space kernel

                        std::string currcommandis = "-" + ((currcommand(0)).substr(4));

                        {
                            ML_Base &kernML = (*xbopts).altmuapprox;
                            MercerKernel &theKern = kernML.getRFFKernel_unsafe();

                            processKernel(kernML,theKern,currcommandis,currcommand,3,argvariables,grkkernnum,grkfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                            grkfirstcall = 0;
                        }

                        {
                            ML_Base &kernML = (*xbopts).altmuapprox_rff;
                            MercerKernel &theKern = kernML.getRFFKernel_unsafe();

                            processKernel(kernML,theKern,currcommandis,currcommand,3,argvariables,grkkernnum_rff,grkfirstcall_rff,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                            grkfirstcall_rff = 0;
                        }
                    }

                    else if ( ( currcommand(0).substr(0,4) == "-gmk" ) || ( currcommand(0) == "-gmmtb" ) || ( currcommand(0) == "-gmbmx" ) )
                    {
                        std::string currcommandis = "-" + ((currcommand(0)).substr(3));

                        {
                            ML_Base &kernML = (*xbopts).altmuapprox;
                            MercerKernel &theKern = kernML.getKernel_unsafe();

                            processKernel(kernML,theKern,currcommandis,currcommand,0,argvariables,gkkernnum,gkfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                            gkfirstcall = 0;
                        }

                        {
                            ML_Base &kernML = (*xbopts).altmuapprox_rff;
                            MercerKernel &theKern = kernML.getKernel_unsafe();

                            processKernel(kernML,theKern,currcommandis,currcommand,0,argvariables,gkkernnum_rff,gkfirstcall_rff,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                            gkfirstcall_rff = 0;
                        }
                    }

                    else if ( ( currcommand(0).substr(0,6) == "-gmsek" ) || ( currcommand(0) == "-gmsemtb" ) || ( currcommand(0) == "-gmsebmx" ) )
                    {
                        // output kernel

                        std::string currcommandis = "-" + ((currcommand(0)).substr(5));

                        ML_Base &kernML = (*xbopts).altfxapprox;
                        MercerKernel &theKern = kernML.getUUOutputKernel_unsafe();

                        processKernel(kernML,theKern,currcommandis,currcommand,1,argvariables,gsekkernnum,gsekfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        gsekfirstcall = 0;
                    }

                    else if ( ( currcommand(0).substr(0,6) == "-gmsrk" ) || ( currcommand(0) == "-gmsrmtb" ) || ( currcommand(0) == "-gmsrbmx" ) )
                    {
                        // RFF similarity space kernel

                        std::string currcommandis = "-" + ((currcommand(0)).substr(5));

                        ML_Base &kernML = (*xbopts).altfxapprox;
                        MercerKernel &theKern = kernML.getRFFKernel_unsafe();

                        processKernel(kernML,theKern,currcommandis,currcommand,3,argvariables,gsrkkernnum,gsrkfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        gsrkfirstcall = 0;
                    }

                    else if ( ( currcommand(0).substr(0,5) == "-gmsk" ) || ( currcommand(0) == "-gmsmtb" ) || ( currcommand(0) == "-gmsbmx" ) )
                    {
                        std::string currcommandis = "-" + ((currcommand(0)).substr(4));

                        ML_Base &kernML = (*xbopts).altfxapprox;
                        MercerKernel &theKern = kernML.getKernel_unsafe();

                        processKernel(kernML,theKern,currcommandis,currcommand,0,argvariables,gskkernnum,gskfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

//errstream() << "phantomxyznlp " << theKern << "\n";
                        gskfirstcall = 0;
                    }

                    else if ( ( currcommand(0).substr(0,6) == "-gmgtek" ) || ( currcommand(0) == "-gmgtemtb" ) || ( currcommand(0) == "-gmgtebmx" ) )
                    {
                        // output kernel

                        std::string currcommandis = "-" + ((currcommand(0)).substr(6));

                        ML_Base &kernML = (*xbopts).altcgtapprox;
                        MercerKernel &theKern = kernML.getUUOutputKernel_unsafe();

                        processKernel(kernML,theKern,currcommandis,currcommand,1,argvariables,gsekkernnum,gsekfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        gsekfirstcall = 0;
                    }

                    else if ( ( currcommand(0).substr(0,6) == "-gmgtrk" ) || ( currcommand(0) == "-gmgtrmtb" ) || ( currcommand(0) == "-gmgtrbmx" ) )
                    {
                        // RFF similarity space kernel

                        std::string currcommandis = "-" + ((currcommand(0)).substr(6));

                        ML_Base &kernML = (*xbopts).altcgtapprox;
                        MercerKernel &theKern = kernML.getRFFKernel_unsafe();

                        processKernel(kernML,theKern,currcommandis,currcommand,3,argvariables,gsrkkernnum,gsrkfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        gsrkfirstcall = 0;
                    }

                    else if ( ( currcommand(0).substr(0,5) == "-gmgtk" ) || ( currcommand(0) == "-gmgtmtb" ) || ( currcommand(0) == "-gmgtbmx" ) )
                    {
                        std::string currcommandis = "-" + ((currcommand(0)).substr(5));

                        ML_Base &kernML = (*xbopts).altcgtapprox;
                        MercerKernel &theKern = kernML.getKernel_unsafe();

                        processKernel(kernML,theKern,currcommandis,currcommand,0,argvariables,gskkernnum,gskfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

//errstream() << "phantomxyznlp " << theKern << "\n";
                        gskfirstcall = 0;
                    }

                    else if ( currcommand(0) == "-gbH"   ) { (*xbopts).method              = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbs"   ) { (*xbopts).intrinbatch         = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbm"   ) { (*xbopts).intrinbatchmethod   = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbj"   ) { (*xbopts).startpoints         = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbTv"  ) { (*xbopts).sigma_cut           = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gbeu"  ) { (*xbopts).evaluse             = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gbTs"  ) { (*xbopts).TSsampType          = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gbTx"  ) { (*xbopts).TSxsampType         = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-gbTm"  ) { (*xbopts).TSmode              = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbTn"  ) { (*xbopts).TSNsamp             = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gba"   ) { (*xbopts).startseed           = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbb"   ) { (*xbopts).algseed             = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbt"   ) { (*xbopts).totiters            = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbe"   ) { (*xbopts).err                 = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbz"   ) { (*xbopts).ztol                = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbZ"   ) { (*xbopts).minstdev            = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbD"   ) { (*xbopts).delta               = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbzz"  ) { (*xbopts).zeta                = safeatof(currcommand(1),argvariables);                                                                   }

                    if ( currcommand(0) == "-gbk"   ) { (*xbopts).nu                  = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbx"   ) { (*xbopts).modD                = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbo"   ) { (*xbopts).a                   = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbB"   ) { (*xbopts).b                   = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbr"   ) { (*xbopts).r                   = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbRR"  ) { (*xbopts).R                   = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbBB"  ) { (*xbopts).B                   = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbu"   ) { (*xbopts).p                   = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbv"   ) { (*xbopts).betafn              = currcommand(1);                                                                                          }
                    else if ( currcommand(0) == "-gbim"  ) { (*xbopts).itcntmethod         = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbpd"  ) { (*xbopts).direcdim            = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbq"   ) { (*xbopts).impmeasu            = &(getMLref(svmThreadOwner,svmbase,threadInd,safeatoi(currcommand(1),argvariables),svmContext).getIMP()); }
                    else if ( currcommand(0) == "-gbpp"  ) { (*xbopts).direcpre            = &(getMLref(svmThreadOwner,svmbase,threadInd,safeatoi(currcommand(1),argvariables),svmContext).getML());  }
                    else if ( currcommand(0) == "-gbmm"  ) { (*xbopts).direcsubseqpre      = &(getMLref(svmThreadOwner,svmbase,threadInd,safeatoi(currcommand(1),argvariables),svmContext).getML());  }
                    else if ( currcommand(0) == "-gbG"   ) { (*xbopts).gridsource          = &(getMLref(svmThreadOwner,svmbase,threadInd,safeatoi(currcommand(1),argvariables),svmContext));          }
                    else if ( currcommand(0) == "-gbsp"  ) { (*xbopts).stabpmax            = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbsP"  ) { (*xbopts).stabpmin            = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbsA"  ) { (*xbopts).stabA               = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbsB"  ) { (*xbopts).stabB               = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbsF"  ) { (*xbopts).stabF               = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbsr"  ) { (*xbopts).stabbal             = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbsz"  ) { (*xbopts).stabZeroPt          = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbss"  ) { (*xbopts).stabUseSig          = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbst"  ) { (*xbopts).stabThresh          = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbuu"  ) { (*xbopts).unscentUse          = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbuk"  ) { (*xbopts).unscentK            = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbfid" ) { (*xbopts).numfids             = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbfn"  ) { (*xbopts).dimfid              = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbfb"  ) { (*xbopts).fidbudget           = safeatof(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbfp"  ) { (*xbopts).fidpenalty          = currcommand(1);                                                                                          }
                    else if ( currcommand(0) == "-gbfv"  ) { (*xbopts).fidvar              = currcommand(1);                                                                                          }
                    else if ( currcommand(0) == "-gbfo"  ) { (*xbopts).fidover             = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gBbj"  ) { (*xbopts).startpointsmultiobj = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gBbt"  ) { (*xbopts).totitersmultiobj    = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gBbH"  ) { (*xbopts).ehimethodmultiobj   = safeatoi(currcommand(1),argvariables);                                                                   }
                    else if ( currcommand(0) == "-gbpl"  ) { safeatowhatever((*xbopts).direcmin,currcommand(1),argvariables);                                                                         }
                    else if ( currcommand(0) == "-gbpu"  ) { safeatowhatever((*xbopts).direcmax,currcommand(1),argvariables);                                                                         }
                    else if ( currcommand(0) == "-gbuS"  ) { safeatowhatever((*xbopts).unscentSqrtSigma,currcommand(1),argvariables);                                                                 }

                    else if ( currcommand(0) == "-gbp"  )
                    {
                        Vector<int> penaltyrefs;

                        safeatowhatever(penaltyrefs,currcommand(1),argvariables);

                        ((*xbopts).penalty).resize(penaltyrefs.size());

                        if ( penaltyrefs.size() )
                        {
                            int ij;

                            for ( ij = 0 ; ij < penaltyrefs.size() ; ++ij )
                            {
                                ((*xbopts).penalty)("&",ij) = &(getMLref(svmThreadOwner,svmbase,threadInd,penaltyrefs(ij),svmContext));
                            }
                        }
                    }








                    else if ( currcommand(0) == "-gxs"  ) { std::stringstream xstr(currcommand(1)); streamItIn(xstr,defxres,0); }
                    else if ( currcommand(0) == "-gtp"  ) { prestring   = currcommand(1);                                       }
                    else if ( currcommand(0) == "-gtP"  ) { midstring   = currcommand(1);                                       }
                    else if ( currcommand(0) == "-gbts" ) { interstring = currcommand(1);                                       }
                    else if ( currcommand(0) == "-gtx"  ) { xfnis       = currcommand(1);                                       }
                    else if ( currcommand(0) == "-gr"   ) { numOptReps  = safeatoi(currcommand(1),argvariables);                }
                    else if ( currcommand(0) == "-gref" ) { optname     = currcommand(1);                                       }
                    else if ( currcommand(0) == "-gpd"  ) { optname = ( (*xnopts).optname = ( (*xgopts).optname = ( (*xdopts).optname = ( (*xbopts).optname = currcommand(1) ) ) ) ); }

                    else if ( ( currcommand(0) == "-g"  ) ||
                              ( currcommand(0) == "-gd" ) ||
                              ( currcommand(0) == "-gN" ) ||
                              ( currcommand(0) == "-gb" )    )
                    {
                        std::string opttypestring(currcommand(0));

                        GlobalOptions &xopts = *(   ( currcommand(0) == "-gd" ) ? &static_cast<GlobalOptions &>((*xdopts)) :
                                                  ( ( currcommand(0) == "-gb" ) ? &static_cast<GlobalOptions &>((*xbopts)) :
                                                  ( ( currcommand(0) == "-gN" ) ? &static_cast<NelderOptions &>((*xnopts)) :  &static_cast<GlobalOptions &>((*xgopts)) ) ) );

                        {
                            // 1. Construct vector of arguments
                            // 2. Run through vector and fill in results
                            // 3. Write results to file
                            // 4. Find optimal element of vector
                            // 5. Run set argument

                            int nargst = ((currcommand.size())-3)/5; // total number of arguments

                            if ( nargst )
                            {
                                Vector<std::string> type(nargst);     // type
                                Vector<int>         argtype(nargst);  // range (0 lin, 1 log, 2 exp, 3 rand)
                                Vector<int>         argvart(nargst);  // variable type (0 int, 1 double)
                                Vector<int>         argnums(nargst);  // arg num
                                Vector<int>         incrtot(nargst);  // total increments
                                Vector<gentype>     startval(nargst); // start var
                                Vector<gentype>     endval(nargst);   // end intvar

                                for ( i = 0 ; i < nargst ; ++i )
                                {
                                    type("&",i)    = currcommand(3+(i*5));
                                    argnums("&",i) = safeatoi(currcommand(4+(i*5)),argvariables);
                                    incrtot("&",i) = safeatoi(currcommand(7+(i*5)),argvariables);

                                    int donesomething = 0; // because fucking visual fucking studio (aka mex) whinges about blocks nested too deeply if I just use standard fucking else fucking if

                                    if ( type(i) == "zb" )
                                    {
                                        // integer linear

                                        argtype("&",i)              = 0;
                                        argvart("&",i)              = 0;
                                        startval("&",i).force_int() = safeatoi(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_int()   = safeatoi(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "zl" )
                                    {
                                        // integer logarithmic

                                        argtype("&",i)              = 1;
                                        argvart("&",i)              = 0;
                                        startval("&",i).force_int() = safeatoi(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_int()   = safeatoi(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "za" )
                                    {
                                        // integer antilogarithmic

                                        argtype("&",i)              = 2;
                                        argvart("&",i)              = 0;
                                        startval("&",i).force_int() = safeatoi(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_int()   = safeatoi(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "zr" )
                                    {
                                        // integer random

                                        argtype("&",i)              = 3;
                                        argvart("&",i)              = 0;
                                        startval("&",i).force_int() = safeatoi(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_int()   = safeatoi(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "zc" )
                                    {
                                        // integer random

                                        argtype("&",i)              = 4;
                                        argvart("&",i)              = 0;
                                        startval("&",i).force_int() = safeatoi(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_int()   = safeatoi(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "fb" )
                                    {
                                        // float linear

                                        argtype("&",i)                 = 0;
                                        argvart("&",i)                 = 1;
                                        startval("&",i).force_double() = safeatof(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_double()   = safeatof(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "fl" )
                                    {
                                        // float logarithmic

                                        argtype("&",i)                 = 1;
                                        argvart("&",i)                 = 1;
                                        startval("&",i).force_double() = safeatof(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_double()   = safeatof(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "fa" )
                                    {
                                        // float antilogarithmic

                                        argtype("&",i)                 = 2;
                                        argvart("&",i)                 = 1;
                                        startval("&",i).force_double() = safeatof(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_double()   = safeatof(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "fr" )
                                    {
                                        // float random

                                        argtype("&",i)                 = 3;
                                        argvart("&",i)                 = 1;
                                        startval("&",i).force_double() = safeatof(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_double()   = safeatof(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( type(i) == "fc" )
                                    {
                                        // float random

                                        argtype("&",i)                 = 4;
                                        argvart("&",i)                 = 1;
                                        startval("&",i).force_double() = safeatof(currcommand(5+(i*5)),argvariables);
                                        endval("&",i).force_double()   = safeatof(currcommand(6+(i*5)),argvariables);

                                        donesomething = 1;
                                    }

                                    if ( !donesomething )
                                    {
                                        STRTHROW("Error: "+type(i)+" is not a known -g type.");
                                    }
                                }

                                Vector<gentype> xres(defxres);
                                Vector<gentype> rawxres(defxres);
                                gentype fres;
                                Vector<gentype> cres;
                                int ires = 0;
                                int mInd = 0;
                                Vector<int> muInd;
                                Vector<int> augxInd;
                                Vector<int> cgtInd;
                                int sigInd = 0;
                                int srcmodInd = 0;
                                int diffmodInd = 0;
                                gentype sres;
                                double hres = 0.0;
                                int rawitcnt = 0;
                                double optstarttime = TIMEABSSEC(TIMECALL);





                                vecallxres.add(numtests);
                                vecallrawxres.add(numtests);
                                vecallfres.add(numtests);
                                vecallcres.add(numtests);
                                vecallmres.add(numtests);
                                vecallires.add(numtests);
                                vecallsres.add(numtests);
                                vecallhres.add(numtests);
                                vecsscore.add(numtests);

                                vecmeanfres.add(numtests);
                                vecmeanires.add(numtests);
                                vecmeantres.add(numtests);
                                vecmeanTres.add(numtests);
                                vecmeanallfres.add(numtests);
                                vecmeanallmres.add(numtests);

                                vecvarfres.add(numtests);
                                vecvarires.add(numtests);
                                vecvartres.add(numtests);
                                vecvarTres.add(numtests);
                                vecvarallfres.add(numtests);
                                vecvarallmres.add(numtests);

                                vecnames.add(numtests);

                                Vector<Vector<gentype> > &allxres    = vecallxres("&",numtests);
                                Vector<Vector<gentype> > &allrawxres = vecallrawxres("&",numtests);
                                Vector<gentype>          &allfres    = vecallfres("&",numtests);
                                Vector<Vector<gentype> > &allcres    = vecallcres("&",numtests);
                                Vector<gentype>          &allmres    = vecallmres("&",numtests);
                                Vector<int>              &allires    = vecallires("&",numtests);
                                Vector<gentype>          &allsres    = vecallsres("&",numtests);
                                Vector<double>           &allhres    = vecallhres("&",numtests);
                                Vector<double>           &sscore     = vecsscore("&",numtests);

                                gentype         &meanfres    = vecmeanfres("&",numtests);
                                gentype         &meanires    = vecmeanires("&",numtests);
                                gentype         &meantres    = vecmeantres("&",numtests);
                                gentype         &meanTres    = vecmeanTres("&",numtests);
                                Vector<gentype> &meanallfres = vecmeanallfres("&",numtests);
                                Vector<gentype> &meanallmres = vecmeanallmres("&",numtests);

                                gentype         &varfres    = vecvarfres("&",numtests);
                                gentype         &varires    = vecvarires("&",numtests);
                                gentype         &vartres    = vecvartres("&",numtests);
                                gentype         &varTres    = vecvarTres("&",numtests);
                                Vector<gentype> &varallfres = vecvarallfres("&",numtests);
                                Vector<gentype> &varallmres = vecvarallmres("&",numtests);

                                vecnames("&",numtests) = optname;

                                ++numtests;
                                {
                                    std::stringstream ssj;
                                    ssj << numtests;
                                    optname = "sim "+ssj.str();
                                }




//NB: if you change this you'll also need to change it in globalopt.h!
                                Vector<int> MLnumbers(9); // 0 = ML model (mu), -1 if none
                                                          // 1 = ML model (sigma), -1 if none or same as mu
                                                          // 2 = functional analysis model, -1 if not relevant
                                                          // 3 = random direction model (GPR or distribution)
                                                          // 4 = source model (env-GP,diff-GP)
                                                          // 5 = difference model (diff-GP)
                                                          // 6 = augx model, -1 if not defined
                                                          // 7 = cgt model, -1 if not defined

                                MLnumbers("&",0) = bayesModelNum;
                                MLnumbers("&",1) = -1;
                                MLnumbers("&",2) = -1;
                                MLnumbers("&",3) = -1;
                                MLnumbers("&",4) = -1;
                                MLnumbers("&",5) = -1;
                                MLnumbers("&",6) = -1;
                                MLnumbers("&",7) = -1;

                                void *fnarg[21];

                                fnarg[0]  = (void *) &(currcommand("&",1));
                                fnarg[1]  = (void *) &svmContext;
                                fnarg[2]  = (void *) &verblevel;
                                fnarg[3]  = (void *) &finalresult;
                                fnarg[4]  = (void *) &logfile;
                                fnarg[5]  = (void *) &depthin;
                                fnarg[6]  = (void *) &argvariables;
                                fnarg[7]  = (void *) &threadInd;
                                fnarg[8]  = (void *) &globargvariables;
                                fnarg[9]  = (void *) &argnums;
                                fnarg[10] = (void *) getsetExtVar;
                                fnarg[11] = (void *) &svmbase;
                                fnarg[12] = (void *) &svmThreadOwner;
                                fnarg[13] = (void *) &interstring;
                                fnarg[14] = (void *) &xfnis;
                                fnarg[15] = (void *) &MLnumbers; // IMPORTANT: this number must be fixed!
                                fnarg[16] = (void *) &prestring;
                                fnarg[17] = (void *) &midstring;
                                fnarg[18] = (void *) &rawitcnt;
                                fnarg[19] = (void *) &optstarttime;

                                int dummy = 0;

                                (*xgopts).numPts = incrtot;
                                xopts.MLregfn = gridelmMLreg;

                                xopts.MLdefined = 1;

                                xopts.reset();

                                int optres = xopts.optim(nargst,
                                                          xres,
                                                          rawxres,
                                                          fres,
                                                          ires,
                                                          mInd,
                                                          muInd,
                                                          augxInd,
                                                          cgtInd,
                                                          sigInd,
                                                          srcmodInd,
                                                          diffmodInd,
                                                          allxres,
                                                          allrawxres,
                                                          allfres,
                                                          allcres,
                                                          allmres,
                                                          allsres,
                                                          sscore,
                                                          startval,
                                                          endval,
                                                          argtype,
                                                          argvart,
                                                          &gridelmrun,
                                                          (void *) fnarg,
                                                          dummy,
                                                          numOptReps,
                                                          meanfres,varfres,
                                                          meanires,varires,
                                                          meantres,vartres,
                                                          meanTres,varTres,
                                                          meanallfres,varallfres,
                                                          meanallmres,varallmres);


                                retVector<double> tmpva;
                                retVector<int>    tmpvb;

                                allires = cntintvec(allfres.size(),tmpvb);
                                allhres = zerodoublevec(allfres.size(),tmpva);

                                errstream() << "Optim return code: " << optres << " - ";

                                xfnis.makeNull();

                                if ( opttypestring == "-g" )
                                {
                                    errstream() << " (" << ires << ")\n";
                                }

                                else if ( opttypestring == "-gN" )
                                {
                                    switch ( optres )
                                    {
                                        case 0:    { errstream() << "success\n";                          break; }
                                        case 1:    { errstream() << "success\n";                          break; }
                                        case 2:    { errstream() << "success: stopval reached\n";         break; }
                                        case 3:    { errstream() << "success: ftol reached\n";            break; }
                                        case 4:    { errstream() << "success: xtol reached\n";            break; }
                                        case 5:    { errstream() << "success: max evaluations reached\n"; break; }
                                        case 6:    { errstream() << "success: maxtime exceeded\n";        break; }
                                        case -1:   { errstream() << "generic fail\n";                     break; }
                                        case -2:   { errstream() << "invalid arguments\n";                break; }
                                        case -3:   { errstream() << "out of memory\n";                    break; }
                                        case -4:   { errstream() << "roundoff limited\n";                 break; }
                                        case -5:   { errstream() << "forced stop\n";                      break; }
                                        default:   { errstream() << "Unknown\n";                          break; }
                                    }
                                }

                                else if ( opttypestring == "-gd" )
                                {
                                    switch ( optres )
                                    {
                                        case 0:    { errstream() << "success\n";                         break; }
                                        case 1:    { errstream() << "success: maxfeval exceeded\n";      break; }
                                        case 2:    { errstream() << "success: maxiter exceeded\n";       break; }
                                        case 3:    { errstream() << "success: global found\n";           break; }
                                        case 4:    { errstream() << "success: voltol something\n";       break; }
                                        case 5:    { errstream() << "success: sigmatol something\n";     break; }
                                        case 6:    { errstream() << "success: maxtime exceeded\n";       break; }
                                        case -1:   { errstream() << "invalid bounds\n";                  break; }
                                        case -2:   { errstream() << "maxfeval too big\n";                break; }
                                        case -3:   { errstream() << "init failed\n";                     break; }
                                        case -4:   { errstream() << "sample-points failed\n";            break; }
                                        case -5:   { errstream() << "function sample failed\n";          break; }
                                        case -6:   { errstream() << "hyper-rectangle addition failed\n"; break; }
                                        case -100: { errstream() << "out of memory\n";                   break; }
                                        case -101: { errstream() << "invalid args\n";                    break; }
                                        case -102: { errstream() << "forced stop\n";                     break; }
                                        default:   { errstream() << "Unknown\n";                         break; }
                                    }
                                }

                                else if ( opttypestring == "-gb" )
                                {
                                    errstream() << " (" << ires << ")\n";
                                }

                                // Secondary analysis.

                                Vector<int> optvarind(allxres.size() ? 1 : 0);

                                xopts.analyse(allxres,allmres,allcres,allhres,optvarind,1);

                                if ( allfres.size() && ( allfres(0).size() > 1 ) )
                                {
                                    // Multiobjective optimisation - find the
                                    // Pareto set (xres,fres,ires currently
                                    // ill-defined, so ve must fix zat).
                                    //
                                    // In this case  we need to set xres,
                                    // fres, ires to some representative of
                                    // the complete pareto set.  This is
                                    // arbitrarily chosen to be element
                                    // zero.

                                    xres = allxres(optvarind(0));
                                    fres = allfres(optvarind(0));
                                    cres = allcres(optvarind(0));
                                    ires = allires(optvarind(0));
                                    sres = allsres(optvarind(0));
                                    hres = allhres(optvarind(0));
                                }

                                else if ( allfres.size() && ( ires >= 0 ) )
                                {
                                    optvarind.resize(1) = ires;

errstream() << "phantomabc ires = " << ires << "\n";
                                    // Still need to retieve supplementary results!

                                    xres = allxres(optvarind(0)); // Important to do this: xres is non-converted, allxres is
                                    // fres = allfres(optvarind(0));
                                    // cres = allcres(optvarind(0));
                                    // ires = allires(optvarind(0));
                                    sres = allsres(optvarind(0));
                                    hres = allhres(optvarind(0));
                                }

                                else if ( ires >= 0 )
                                {
                                    optvarind.resize(1) = ires;
                                }

                                if ( optvarind.size() )
                                {
                                    sres = allsres(optvarind(0));
                                    // Set result (we now know that it is defined)

                                    errstream() << "Setting result " << xres << "\n";

                                    {
                                        //SparseVector<SparseVector<gentype> > gridargvars(argvariables);
                                        SparseVector<SparseVector<gentype> > &gridargvars = argvariables;

                                        // Not necessarily true NiceAssert( xres.size() == argnums.size() );

                                        int i;

                                        if ( xres.size() )
                                        {
                                            for ( i = 0 ; i < xres.size() ; ++i )
                                            {
                                                if ( i < argnums.size() )
                                                {
                                                    gridargvars("&",0)("&",argnums(i)) = xres(i);
                                                }

                                                gridargvars("&",50)("&",i) = xres(i);
                                            }
                                        }

                                        if ( sres.size() )
                                        {
                                            for ( i = 0 ; i < sres.size() ; ++i )
                                            {
                                                gridargvars("&",53)("&",i) = sres(i);
                                            }
                                        }

                                        gridargvars("&",51)("&",0) = fres;
                                        gridargvars("&",52)("&",0) = ires;
                                        gridargvars("&",54)("&",0) = hres;

                                        gridargvars("&",55)("&",0) = meanfres; gridargvars("&",55)("&",65536) = varfres;
                                        gridargvars("&",57)("&",0) = meanires; gridargvars("&",57)("&",65536) = varires;
                                        gridargvars("&",58)("&",0) = meantres; gridargvars("&",58)("&",65536) = vartres;
                                        gridargvars("&",59)("&",0) = meanTres; gridargvars("&",59)("&",65536) = varTres;

                                        for ( i = 0 ; i < optvarind.size() ; ++i )
                                        {
                                            gridargvars("&",60)("&",i) = allxres(optvarind(i));
                                            gridargvars("&",61)("&",i) = allfres(optvarind(i));
                                            //FIXME gridargvars("&",61)("&",i) = allcres(optvarind(i));
                                            gridargvars("&",62)("&",i) = allires(optvarind(i));
                                            gridargvars("&",63)("&",i) = allsres(optvarind(i));
                                            gridargvars("&",64)("&",i) = allhres(optvarind(i));

                                            gridargvars("&",65)("&",i) = meanallfres(optvarind(i)); gridargvars("&",65)("&",65536+i) = varallfres(optvarind(i));
                                            gridargvars("&",66)("&",i) = meanallmres(optvarind(i)); gridargvars("&",66)("&",65536+i) = varallmres(optvarind(i));
                                        }

                                        for ( i = 0 ; i < allxres.size() ; ++i )
                                        {
                                            gridargvars("&",70)("&",i) = allxres(i);
                                            gridargvars("&",71)("&",i) = allfres(i);
                                            //FIXME gridargvars("&",71)("&",i) = allcres(i);
                                            gridargvars("&",72)("&",i) = allires(i);
                                            gridargvars("&",73)("&",i) = allsres(i);
                                            gridargvars("&",74)("&",i) = allhres(i);

                                            gridargvars("&",75)("&",i) = meanallfres(i); gridargvars("&",75)("&",65536+i) = varallfres(i);
                                            gridargvars("&",76)("&",i) = meanallmres(i); gridargvars("&",76)("&",65536+i) = varallmres(i);
                                        }

                                        gridargvars("&",80)("&",0) = allxres;
                                        gridargvars("&",81)("&",0) = allfres;
                                        //FIXME gridargvars("&",81)("&",0) = allcres;
                                        gridargvars("&",82)("&",0) = allires;
                                        gridargvars("&",83)("&",0) = allsres;
                                        gridargvars("&",84)("&",0) = allhres;

                                        gridargvars("&",85)("&",0) = meanallfres; gridargvars("&",75)("&",65536) = varallfres;
                                        gridargvars("&",86)("&",0) = meanallmres; gridargvars("&",76)("&",65536) = varallmres;

                                        gridargvars("&",90)("&",0)         = muInd;
                                        gridargvars("&",90)("&",1)         = sigInd;
                                        gridargvars("&",90)("&",2)         = mInd;
                                        //gridargvars("&",90)("&",3)         = randDirtemplateInd; // Not relevant here!
                                        //gridargvars("&",90)("&",4)         = itnum; // Not relevant here!
                                        gridargvars("&",90)("&",5)         = srcmodInd;
                                        gridargvars("&",90)("&",6)         = diffmodInd;
                                        //gridargvars("&",90)("&",7)         = raw iteration count not relevant here!
                                        //gridargvars("&",90)("&",8)         = raw start time not relevant here!
                                        gridargvars("&",90)("&",9)         = augxInd;
                                        gridargvars("&",90)("&",10)        = cgtInd;

//phantomxyzxyz
                                        int locverblevel = verblevel;

                                        std::stringstream *tmpcommand;
                                        MEMNEW(tmpcommand,std::stringstream(currcommand(2)));
                                        awarestream *gridbox;
                                        MEMNEW(gridbox,awarestream(tmpcommand,1));
                                        Stack<awarestream *> *gridcommstack;
                                        MEMNEW(gridcommstack,Stack<awarestream *>);
                                        gridcommstack->push(gridbox);
                                        std::string loclogfile = logfile+"_gridfileopt";

                                        gentype dummyres;

                                        callsvm(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,gridargvars,locverblevel,dummyres,loclogfile);

                                        MEMDEL(gridcommstack);
                                    }

//phantomxyzxyzxyzxyz
                                    // Save results

                                    finalresult = fres;

                                    if ( verblevel && ( logfile.length() > 0 ) )
                                    {
                                        retVector<Vector<gentype> > tmpva;
                                        retVector<gentype>          tmpvb;
                                        retVector<gentype>          tmpvc;
                                        retVector<Vector<gentype> > tmpvq;
                                        retVector<double>           tmpvd;
                                        retVector<int>              tmpve;

                                        errstream() << "Writing gridfile... ";

                                        std::string xgridfilenamefull = logfile+"_xgrid"; // x values
                                        std::string fgridfilenamefull = logfile+"_fgrid"; // results
                                        std::string cgridfilenamefull = logfile+"_cgrid"; // results
                                        std::string mgridfilenamefull = logfile+"_mgrid"; // results modified
                                        std::string igridfilenamefull = logfile+"_igrid"; // indices
                                        std::string sgridfilenamefull = logfile+"_sgrid"; // suplementary information
                                        std::string hgridfilenamefull = logfile+"_hgrid"; // hypervolume information
                                        std::string qgridfilenamefull = logfile+"_qgrid"; // stability scores

                                        std::string meanfgridfilenamefull = logfile+"_fgrid_mean"; // mean results
                                        std::string meanmgridfilenamefull = logfile+"_mgrid_mean"; // mean results modified

                                        std::string varfgridfilenamefull = logfile+"_fgrid_var"; // var results
                                        std::string varmgridfilenamefull = logfile+"_mgrid_var"; // var results modified

                                        std::string xygridfilenamefull  = logfile+"_xygrid";  // y and x ready to train another model
                                        std::string xytgridfilenamefull = logfile+"_xytgrid"; // y and x ready to train another model

                                        writeLog(allxres,xgridfilenamefull,getsetExtVar);
                                        writeLog(allfres,fgridfilenamefull,getsetExtVar);
                                        writeLog(allcres,cgridfilenamefull,getsetExtVar);
                                        writeLog(allmres,mgridfilenamefull,getsetExtVar);
                                        writeLog(allires,igridfilenamefull,getsetExtVar);
                                        writeLog(allsres,sgridfilenamefull,getsetExtVar);
                                        writeLog(allhres,hgridfilenamefull,getsetExtVar);
                                        writeLog(sscore ,qgridfilenamefull,getsetExtVar);

                                        writeLog(meanallfres,meanfgridfilenamefull,getsetExtVar);
                                        writeLog(meanallmres,meanmgridfilenamefull,getsetExtVar);

                                        writeLog(varallfres,varfgridfilenamefull,getsetExtVar);
                                        writeLog(varallmres,varmgridfilenamefull,getsetExtVar);

                                        writeLog(allfres,allxres,xygridfilenamefull);
                                        writeLog(allfres,allsres,allxres,xytgridfilenamefull);

                                        errstream() << "Writing paretofile... " << optvarind;

                                        std::string xparetofilenamefull = logfile+"_xpareto";
                                        std::string fparetofilenamefull = logfile+"_fpareto";
                                        std::string cparetofilenamefull = logfile+"_cpareto";
                                        std::string mparetofilenamefull = logfile+"_mpareto";
                                        std::string iparetofilenamefull = logfile+"_ipareto";
                                        std::string sparetofilenamefull = logfile+"_spareto";
                                        std::string hparetofilenamefull = logfile+"_hpareto";
                                        std::string qparetofilenamefull = logfile+"_qpareto";

                                        std::string meanfparetofilenamefull = logfile+"_fpareto_mean";
                                        std::string meanmparetofilenamefull = logfile+"_mpareto_mean";

                                        std::string varfparetofilenamefull = logfile+"_fpareto_var";
                                        std::string varmparetofilenamefull = logfile+"_mpareto_var";

                                        std::string xyparetofilenamefull  = logfile+"_xypareto";
                                        std::string xytparetofilenamefull = logfile+"_xytpareto";

                                        writeLog(allxres(optvarind,tmpva),xparetofilenamefull,getsetExtVar);
                                        writeLog(allfres(optvarind,tmpvc),fparetofilenamefull,getsetExtVar);
                                        writeLog(allcres(optvarind,tmpvq),cparetofilenamefull,getsetExtVar);
                                        writeLog(allmres(optvarind,tmpvc),mparetofilenamefull,getsetExtVar);
                                        writeLog(allires(optvarind,tmpve),iparetofilenamefull,getsetExtVar);
                                        writeLog(allsres(optvarind,tmpvc),sparetofilenamefull,getsetExtVar);
                                        writeLog(allhres(optvarind,tmpvd),hparetofilenamefull,getsetExtVar);
                                        writeLog( sscore(optvarind,tmpvd),qparetofilenamefull,getsetExtVar);

                                        writeLog(meanallfres(optvarind,tmpvc),meanfparetofilenamefull,getsetExtVar);
                                        writeLog(meanallmres(optvarind,tmpvc),meanmparetofilenamefull,getsetExtVar);

                                        writeLog(varallfres(optvarind,tmpvc),varfparetofilenamefull,getsetExtVar);
                                        writeLog(varallmres(optvarind,tmpvc),varmparetofilenamefull,getsetExtVar);

                                        writeLog(allfres(optvarind,tmpvc),allxres(optvarind,tmpva),xyparetofilenamefull);
                                        writeLog(allfres(optvarind,tmpvc),allsres(optvarind,tmpvb),allxres(optvarind,tmpva),xytparetofilenamefull);

                                        retVector<gentype> tmpvcc;
                                        const Vector<gentype> &paretofres = allfres(optvarind,tmpvcc);

                                        if ( plotoptres && paretofres.size() && ( paretofres(0).size() == 2 ) )
                                        {
                                            // Multi-objective, 2-d, so we can plot the Pareto fronts

                                            Vector<double> fxval(paretofres.size());
                                            Vector<double> fyval(paretofres.size());

                                            int iij;

                                            double xmin = -1;
                                            double xmax = 0;
                                            double ymin = -1;
                                            double ymax = 0;

                                            for ( iij = 0 ; iij < paretofres.size() ; iij++ )
                                            {
                                                fxval("&",iij) = ((const Vector<double> &) paretofres(iij))(0);
                                                fyval("&",iij) = ((const Vector<double> &) paretofres(iij))(1);
                                            }

                                            std::string fname(optplotname);     fname += "_pareto_scatter";
                                            std::string dname(optplotdataname); dname += "_pareto_scatter";

                                            scatterplot2d(fxval,fyval,xmin,xmax,ymin,ymax,fname,dname,optplotoutformat);
                                        }

                                        if ( plotoptres && allfres.size() && ( allfres(0).size() == 2 ) )
                                        {
                                            // Multi-objective, 2-d, so we can plot the all fronts

                                            Vector<double> fxval(allfres.size());
                                            Vector<double> fyval(allfres.size());

                                            int iij;

                                            double xmin = -1;
                                            double xmax = 0;
                                            double ymin = -1;
                                            double ymax = 0;

                                            for ( iij = 0 ; iij < allfres.size() ; iij++ )
                                            {
                                                fxval("&",iij) = ((const Vector<double> &) allfres(iij))(0);
                                                fyval("&",iij) = ((const Vector<double> &) allfres(iij))(1);
                                            }

                                            std::string fname(optplotname);     fname += "_grid_scatter";
                                            std::string dname(optplotdataname); dname += "_grid_scatter";

                                            scatterplot2d(fxval,fyval,xmin,xmax,ymin,ymax,fname,dname,optplotoutformat);
                                        }

                                        errstream() << " done.\n";
                                    }
                                }
                            }
                        }

                        // Plot comparison of different grids if selected

                        if ( numtests && plotoptres )
                        {
                            multiplot2d(vecmeanallmres,vecvarallmres,vecnames,optplotymin,optplotymax,optplotname,optplotdataname,optplotoutformat,optplottitle);
                        }
//
//FIXMEFIXME FIXME in the 2-objective case we should plot the Pareto front
//(also in the 3-objective case too, ideally)
//
//                                        writeLog(allfres(optvarind,tmpvc),fparetofilenamefull,getsetExtVar);
//./svmheavyv7.exe -qw 10 -z rls -ia 0   -Zx -gbq 10 -gmd 0.01 -gmLf 5 -L testrls -gbH 0 -gmo -gma 2 -gb 2 "-ft 123 2 2 [ y z ] -tM var(0,123)" "-echo y -echo z" fb 1 0 1 1 fb 2 0 1 1                }
                    }
                }

                MEMDEL(xgopts);
                MEMDEL(xdopts);
                MEMDEL(xnopts);
                MEMDEL(xbopts);

                time_used endtime = TIMECALL;
                gridtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << gridtime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Transfer kernel learning steps

            if ( xferopt.size() )
            {
                time_used begintime = TIMECALL;

                int maxiter = 20;
                int randtype = 2;
                double randvari = -1; // default to copy actual data variance
                double maxtime = 0;
                double soltol = 0.01;
                double llrr = -1;
                int method = 3;
                double mlCval = 1.0;
                int regtype = 1;
                int alphaRange = 0;
                int useH01 = 0;

                errstream() << "Setting kernel transfer selection parameters... ";

                while ( xferopt.size() )
                {
                    currcommand = xferopt(0);
                    xferopt.remove(0);

                         if ( currcommand(0) == "-xi"  ) { maxiter    = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-xr"  ) { randtype   = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-xrv" ) { randvari   = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-xt"  ) { maxtime    = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-xC"  ) { mlCval     = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-xR"  ) { regtype    = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-xs"  ) { soltol     = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-xl"  ) { llrr       = safeatof(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-xa"  ) { alphaRange = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-xo"  ) { method     = safeatoi(currcommand(1),argvariables); }
                    else if ( currcommand(0) == "-xh"  ) { useH01     = safeatoi(currcommand(1),argvariables); }

                    else if ( currcommand(0) == "-x" )
                    {
                        int n = safeatoi(currcommand(1),argvariables);
                        Vector<int> mln; safeatowhatever(mln,currcommand(2),argvariables);
                        Vector<double> caseweight; safeatowhatever(caseweight,currcommand(3),argvariables);

                        NiceAssert( mln.size() == caseweight.size() );

                        SVM_Generic &core = dynamic_cast<SVM_Generic &>(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getSVM());
                        Vector<ML_Base *> cases(mln.size());

                        for ( i = 0 ; i < mln.size() ; ++i )
                        {
                            cases("&",i) = &(getMLref(svmThreadOwner,svmbase,threadInd,( mln(i) < 0 ) ? -mln(i) : mln(i),svmContext).getML());
                        }

                        xferMLtrain(thread_killswitch,core,cases,n,maxiter,maxtime,soltol,caseweight,( llrr < 0 ) ? 1 : 0,( llrr < 0 ) ? 0 : llrr,randtype,method,mlCval,regtype,randvari,alphaRange,useH01);
//errstream() << "phantomxyz -42: " << core << "\n";
//errstream() << "phantomxyz -43: " << getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext) << "\n";
//for ( i = 0 ; i < mln.size() ; ++i ) { errstream() << "phantomxyz -43b: " << *(cases(i)) << "\n"; }
                    }
                }

                time_used endtime = TIMECALL;
                xfertime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << xfertime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Feature selection steps

            if ( featureopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Setting Feature selection parameters... ";

                int useDescent = 0;

                Vector<SparseVector<gentype> > xtest;
                Vector<gentype> ytest;
                int startdirty = 0;
                int traverse = 0;

                while ( featureopt.size() )
                {
                    currcommand = featureopt(0);
                    featureopt.remove(0);

                    if ( currcommand(0)[2] == 'S' ) { useDescent = 1; }
                    else                            { useDescent = 0; }

                    currcommand("&",0)[2] = 's';

                         if ( currcommand(0) == "-fsd" ) { startdirty = 1;                                   }
                    else if ( currcommand(0) == "-fsD" ) { startdirty = 0;                                   }
                    else if ( currcommand(0) == "-fss" ) { traverse = safeatoi(currcommand(1),argvariables); }

                    else
                    {
                        Vector<int> usedfeats;

                        int numreps = 1;
                        int randcross = 0;
                        int numfolds = 0;
                        double bestres = 1e6;

                        std::string subcom = (currcommand(0)).substr(4,((currcommand(0)).length())-4);  // Contains suffixes only
                        int isANtype = ( (currcommand(0)).substr(0,4) == "-fsF" );                              // Set if i j {k} suffixes present
                        int fileargpos = isANtype ? 3 : ( ( (currcommand(0)).substr(0,3) == "-fsf" ) ? 1 : 0 ); // position of filename/number
                        int setibase = 0;                                                                               // set if ibase (k) present

                        int reverse = 0;              // set 1 if -AAe used.
                        int ignoreStart = 0;          // number to ignore at start
                        int imax = -1;                // max number to add, or -1 if no limit
                        int ibase = -1;               // where to start adding points, or -1 if end.
                        int uselinesvector = 0;       // if 1 then use linesread vector
                        int israw = 0;                // set if output is to be saved in raw format
                        int startpoint = 0;           // set if reoptimisation should start clean-slate
                        int coercetosingle = 0;       // if 1 then class label / target is read but disgarded and
                        int coercefromsingle = 0;     // if 1 then class label / target is given and file is assumed unlabelled
                        gentype fromsingletarget;     // see above
                        std::string trainfile;        // name of training file
                        Vector<int> linesread;        // vector containing lines to be read (if uselinesvector is set)

                        xlateDataSourceSuffixes(isANtype,fileargpos,setibase,currcommand,subcom,argvariables,filevariables,reverse,ignoreStart,imax,ibase,uselinesvector,israw,startpoint,coercetosingle,coercefromsingle,fromsingletarget,trainfile,linesread);

                        if ( uselinesvector )
                        {
                            std::string indexfilename = logfile+"_index_"+((currcommand(0)).substr(1,((currcommand(0)).length())-1));

                            writeLog(linesread,indexfilename,getsetExtVar);
                        }

                        if ( (currcommand(0)).substr(0,4) == "-fsx" )
                        {
                            bestres = optFeatHillClimb(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),-1,0,0,usedfeats,errstream(),useDescent,xtest,ytest,startpoint,traverse,startdirty);

                            errstream() << "Hill climbing best error: " << bestres << "\n";
                        }

                        else if ( (currcommand(0)).substr(0,4) == "-fsr" )
                        {
                            bestres = optFeatHillClimb(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),-2,0,0,usedfeats,errstream(),useDescent,xtest,ytest,startpoint,traverse,startdirty);

                            errstream() << "Hill climbing best error: " << bestres << "\n";
                        }

                        else if ( ( (currcommand(0)).substr(0,4) == "-fsc" ) || ( (currcommand(0)).substr(0,4) == "-fsC" ) )
                        {
                            numreps    = ( (currcommand(0)).substr(0,4) == "-fsC" ) ? safeatoi(currcommand(1),argvariables) : 1;
                            randcross  = ( (currcommand(0)).substr(0,4) == "-fsC" ) ? 1 : 0;
                            numfolds   = ( (currcommand(0)).substr(0,4) == "-fsC" ) ? safeatoi(currcommand(2),argvariables) : safeatoi(currcommand(1),argvariables);

                            bestres = optFeatHillClimb(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),numfolds,numreps,randcross,usedfeats,errstream(),useDescent,xtest,ytest,startpoint,traverse,startdirty);

                            errstream() << "Hill climbing best error: " << bestres << "\n";
                        }

                        else if ( ( (currcommand(0)).substr(0,3) == "-fsf" ) || ( (currcommand(0)).substr(0,3) == "-fsF" ) )
                        {
                            trainfile = currcommand(1);

                            loadFileForHillClimb(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xtemplate,trainfile,reverse,ignoreStart,imax,coercetosingle,coercefromsingle,fromsingletarget,binaryRelabel,singleDrop,uselinesvector,linesread,xtest,ytest);

                            bestres = optFeatHillClimb(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),-3,1,0,usedfeats,errstream(),useDescent,xtest,ytest,startpoint,traverse,startdirty);

                            errstream() << "Hill climbing best error: " << bestres << "\n";
                        }

                        {
                            std::string featsfilename = logfile+"_feats_"+((currcommand(0)).substr(1,((currcommand(0)).length())-1));

                            writeLog(usedfeats,featsfilename,getsetExtVar);
                        }

                        argvariables("&",1)("&",1) = bestres;
                    }
                }

                time_used endtime = TIMECALL;
                featuretime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << featuretime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Fuzzy SVM steps

            if ( fuzzyopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Setting fuzzy SVM parameters... ";

                gentype tfuzzfn("1");
                int tdistkernstarted = 0;
                MercerKernel tdistkern;
                double tfuzzf  = 1;
                double tfuzzm  = 1;
                double tfuzznu = 0.5;
                int tkernnum = 0;
                int tkernfirstcall = 1;
                int dotfuzz = 0;

                gentype sfuzzfn("1");
                int sdistkernstarted = 0;
                MercerKernel sdistkern;
                double sfuzzf  = 1;
                double sfuzzm  = 1;
                double sfuzznu = 0.5;
                int skernnum = 0;
                int skernfirstcall = 1;
                int dosfuzz = 0;

                while ( fuzzyopt.size() )
                {
                    currcommand = fuzzyopt(0);
                    fuzzyopt.remove(0);

                    if ( currcommand(0) == "-fzt" )
                    {
                        tfuzzfn = currcommand(1);

                        if ( !tdistkernstarted )
                        {
                            tdistkernstarted = 1;
                            tdistkern = getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel();
                        }

                        dotfuzz = 1;
                    }

                    else if ( currcommand(0) == "-fzs" )
                    {
                        sfuzzfn = currcommand(1);

                        if ( !sdistkernstarted )
                        {
                            sdistkernstarted = 1;
                            sdistkern = getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel();
                        }

                        dosfuzz = 1;
                    }

                    else if ( currcommand(0) == "-fztf" )
                    {
                        tfuzzf = safeatof(currcommand(1),argvariables);
                    }

                    else if ( currcommand(0) == "-fztm" )
                    {
                        tfuzzm = safeatof(currcommand(1),argvariables);
                    }

                    else if ( currcommand(0) == "-fztNlA" )
                    {
                        tfuzznu = safeatof(currcommand(1),argvariables);
                    }

                    else if ( ( currcommand(0).substr(0,5) == "-fztk" ) || ( currcommand(0) == "-fztmtb" ) || ( currcommand(0) == "-fztbmx" ) )
                    {
                        std::string currcommandis = "-" + ((currcommand(0)).substr(4));

                        if ( !tdistkernstarted )
                        {
                            tdistkernstarted = 1;
                            tdistkern = getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel();
                        }

                        ML_Base dummyML;

                        processKernel(dummyML,tdistkern,currcommandis,currcommand,2,argvariables,tkernnum,tkernfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        tkernfirstcall = 0;
                    }

                    else if ( currcommand(0) == "-fzsf" )
                    {
                        sfuzzf = safeatof(currcommand(1),argvariables);
                    }

                    else if ( currcommand(0) == "-fzsm" )
                    {
                        sfuzzm = safeatof(currcommand(1),argvariables);
                    }

                    else if ( currcommand(0) == "-fzsNlA" )
                    {
                        sfuzznu = safeatof(currcommand(1),argvariables);
                    }

                    else if ( ( currcommand(0).substr(0,5) == "-fzsk" ) || ( currcommand(0) == "-fzsmtb" ) || ( currcommand(0) == "-fzsbmx" ) )
                    {
                        std::string currcommandis = "-" + ((currcommand(0)).substr(4));

                        if ( !sdistkernstarted )
                        {
                            sdistkernstarted = 1;
                            sdistkern = getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel();
                        }

                        ML_Base dummyML;

                        processKernel(dummyML,sdistkern,currcommandis,currcommand,2,argvariables,skernnum,skernfirstcall,svmThreadOwner,svmbase,threadInd,svmInd,svmContext);

                        skernfirstcall = 0;
                    }
                }

                if ( dotfuzz )
                {
                    if ( calcFuzzML(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),tfuzzfn,argvariables,tdistkern,tfuzzf,tfuzzm,tfuzznu,1) )
                    {
                        STRTHROW("Unknown error during "+currcommand(0)+" operation.");
                    }
                }

                if ( dosfuzz )
                {
                    if ( calcFuzzML(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),sfuzzfn,argvariables,sdistkern,sfuzzf,sfuzzm,sfuzznu,0) )
                    {
                        STRTHROW("Unknown error during "+currcommand(0)+" operation.");
                    }
                }

                time_used endtime = TIMECALL;
                fuzzytime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << fuzzytime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Bootstrap running steps

            if ( bootopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Running bootstraps... ";

                while ( bootopt.size() )
                {
                    currcommand = bootopt(0);
                    bootopt.remove(0);

                    if ( currcommand(0) == "-boot" )
                    {
                         bootstrapML(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext));
                    }
                }

                time_used endtime = TIMECALL;
                boottime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << boottime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Training

            {
                errstream() << "Training... ";

                time_used begintime = TIMECALL;

                // Optimise the SVM

                int res = 0;

                if ( isSVM(getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext)) )
                {
                    errstream() << "Start pretraining...";
                    getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).pretrain();
                    errstream() << " done, proceed with training if required....";
                }

                if ( doopt && !bgTrainOn )
                {
                    errstream() << "Training SVM... ";

                    res = 0;
                    getMLref(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).train(res,thread_killswitch);
                }

                time_used endtime = TIMECALL;
                optimtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << optimtime << " sec (return " << res << ")\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Just in case there is no logfile name yet set it now

            if ( logfile.length() == 0 )
            {
                logfile = DEFAULTLOGFILE;
                argvariables("&",1)("&",12).makeString(logfile);
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Performance evaluation steps

            if ( performopt.size() )
            {
                time_used begintime = TIMECALL;

                errstream() << "Testing SVM... ";

                gentype resfilter("var(1,2)");
                int recordres   = 0;
                int logres      = 1;
                int recordxvar  = 0;
                int useThreads  = 0;

                while ( performopt.size() )
                {
                    currcommand = performopt(0);
                    performopt.remove(0);

                    if      ( currcommand(0) == "-tm"    ) { resfilter   = currcommand(1);                        }
                    else if ( currcommand(0) == "-tQ"    ) { recordres   = 1;                                     }
                    else if ( currcommand(0) == "-tnQ"   ) { recordres   = 0;                                     }
                    else if ( currcommand(0) == "-tQx"   ) { logres      = 1;                                     }
                    else if ( currcommand(0) == "-tnQx"  ) { logres      = 0;                                     }
                    else if ( currcommand(0) == "-tvar"  ) { recordxvar  = 1;                                     }
                    else if ( currcommand(0) == "-tT"    ) { useThreads  = safeatoi(currcommand(1),argvariables); }

                    else if ( currcommand(0) == "-tn" )
                    {
                        int errsel = safeatoi(currcommand(1),argvariables);

                             if ( errsel == 0 ) { resfilter = "var(1,2)"; }
                        else if ( errsel == 1 ) { resfilter = "1-var(1,38)"; }
                        else if ( errsel == 2 ) { resfilter = "1-var(1,39)"; }
                        else if ( errsel == 3 ) { resfilter = "1-var(1,40)"; }
                        else if ( errsel == 4 ) { resfilter = "1-var(1,41)"; }
                        else if ( errsel == 5 ) { resfilter = "1-var(1,42)"; }
                    }

                    else if ( currcommand(0) == "-tM" )
                    {
                        resfilter = currcommand(1);
                        finalresult = resfilter(argvariables);
                        finalresult.finalise();
                    }

                    else if ( currcommand(0) == "-tU" )
                    {
                        resfilter = currcommand(1);
                        gentype midresult = resfilter(argvariables);
                        midresult.finalise();

                        gentype feedfilter("var(1,2)");

                        feedfilter = currcommand(2);
                        gentype feedresult = feedfilter(argvariables);
                        feedresult.finalise();

                        argvariables("&",1)("&",1) = midresult;

                        std::stringstream promptbuff;

                        promptbuff << feedresult << " (output: u = " << midresult << ")" << ": "; // NOTE: don't log, bypass suppression
                        gentype midfilter;
                        promptstream(promptbuff.str()) >> midfilter;
                        finalresult = midfilter(argvariables);
                        finalresult.finalise();

//                        promptbuff.str("");
//                        promptbuff << "Evaluated to: " << finalresult << "\n";
//                        promptstream(promptbuff.str());
                    }

                    else if ( currcommand(0) == "-tMd" )
                    {
                        int dim = safeatoi(currcommand(1),argvariables);
                        gentype f,g;

                        safeatowhatever(f,currcommand(2),argvariables); // must be a vector with infsize()
                        safeatowhatever(g,currcommand(3),argvariables);

                        NiceAssert( f.isValVector() );
                        NiceAssert( f.infsize() );

                        finalresult = calcL2distsq(f.cast_vector(),g,dim);
                    }

                    else if ( currcommand(0) == "-tMD" )
                    {
                        int dim = safeatoi(currcommand(1),argvariables);
                        gentype f,g;

                        safeatowhatever(f,currcommand(2),argvariables);
                        safeatowhatever(g,currcommand(3),argvariables);

                        finalresult = calcL2distsq(f,g,dim);
                    }

                    else if ( currcommand(0) == "-tMv" )
                    {
                        Set<gentype> setres;

                        gentype tmpg;

                        safeatowhatever(tmpg,currcommand(1),argvariables);

                        setres.add(finalresult);
                        setres.add(tmpg);

                        finalresult = setres;
                    }

                    else if ( currcommand(0) == "-tMx" )
                    {
                        Set<gentype> setres;

                        Vector<gentype> tmphv;

                        safeatowhatever(tmphv,currcommand(1),argvariables);

                        gentype tmpg(0);
                        gentype tmph(tmphv);

                        setres.add(finalresult);
                        setres.add(tmpg);
                        setres.add(tmph);

                        finalresult = setres;
                    }

                    else if ( currcommand(0) == "-tMvx" )
                    {
                        Set<gentype> setres;

                        Vector<gentype> tmphv;

                        safeatowhatever(tmphv,currcommand(2),argvariables);

                        gentype tmpg(0);
                        gentype tmph(tmphv);

                        safeatowhatever(tmpg,currcommand(1),argvariables);

                        setres.add(finalresult);
                        setres.add(tmpg);
                        setres.add(tmph);

                        finalresult = setres;
                    }

                    else if ( currcommand(0) == "-tMMv" )
                    {
                        resfilter = currcommand(1);
                        finalresult = resfilter(argvariables);
                        finalresult.finalise();

                        Set<gentype> setres;

                        gentype tmpg;

                        safeatowhatever(tmpg,currcommand(2),argvariables);

                        setres.add(finalresult);
                        setres.add(tmpg);

                        finalresult = setres;
                    }

                    else if ( currcommand(0) == "-tMMx" )
                    {
                        resfilter = currcommand(1);
                        finalresult = resfilter(argvariables);
                        finalresult.finalise();

                        Set<gentype> setres;

                        Vector<gentype> tmphv;

                        safeatowhatever(tmphv,currcommand(2),argvariables);

                        gentype tmpg(0);
                        gentype tmph(tmphv);

                        setres.add(finalresult);
                        setres.add(tmpg);
                        setres.add(tmph);

                        finalresult = setres;
                    }

                    else if ( currcommand(0) == "-tMMvx" )
                    {
                        resfilter = currcommand(1);
                        finalresult = resfilter(argvariables);
                        finalresult.finalise();

                        Set<gentype> setres;

                        Vector<gentype> tmphv;

                        safeatowhatever(tmphv,currcommand(3),argvariables);

                        gentype tmpg(0);
                        gentype tmph(tmphv);

                        safeatowhatever(tmpg,currcommand(2),argvariables);

                        setres.add(finalresult);
                        setres.add(tmpg);
                        setres.add(tmph);

                        finalresult = setres;
                    }

                    else if ( currcommand(0) == "-tV"  )
                    {
                        int firstsum = 1;

                        std::stringstream dstr(currcommand(1));
                        std::stringstream xstr(currcommand(2));

                        Vector<gentype> dz;
                        Vector<SparseVector<gentype> > x;

                        dstr >> dz;
                        streamItIn(xstr,x,0);

                        NiceAssert( dz.size() == x.size() );

                        addtemptox(x,xtemplate);

                        testTest(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),x,dz,firstsum,finalresult,resfilter,argvariables,recordres,logres,recordxvar,getsetExtVar,0,0,useThreads);

                        argvariables("&",1)("&",1) = finalresult;
                    }

                    else if ( currcommand(0) == "-tW"  )
                    {
                        int firstsum = 1;
                        Vector<gentype> dz;
                        Vector<SparseVector<gentype> > x;
                        gentype temp;

                        loadDataFromMatlab(currcommand(2),currcommand(1),x,dz,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).targType(),getsetExtVar);

                        addtemptox(x,xtemplate);

                        testTest(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),x,dz,firstsum,finalresult,resfilter,argvariables,recordres,logres,recordxvar,getsetExtVar,0,0,useThreads);

                        argvariables("&",1)("&",1) = finalresult;
                    }

                    else if ( currcommand(0) == "-tb" )
                    {
                        int firstsum = 1;

                        int minbad = safeatoi(currcommand(1),argvariables);
                        int maxbad = safeatoi(currcommand(2),argvariables);
                        double nmean = safeatof(currcommand(3),argvariables);
                        double nvar  = safeatof(currcommand(4),argvariables);

                        testSparSens(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),firstsum,minbad,maxbad,nmean,nvar,0,finalresult,resfilter,argvariables,recordres,logres,recordxvar,getsetExtVar,useThreads);
                    }

                    else if ( ( currcommand(0) == "-tg" ) || ( currcommand(0) == "-tG" ) || ( currcommand(0) == "-tgc" ) || ( currcommand(0) == "-tGc" ) )
                    {
                        int firstsum = 1;

                        int N = safeatoi(currcommand(1),argvariables);
                        int d = safeatoi(currcommand(2),argvariables);
                        gentype f(currcommand(3)); // No processing, deliberately
                        double v = safeatof(currcommand(4),argvariables);
                        double nadd = 0;
                        int gorG = ( currcommand(0) == "-Ag" ) ? 0 : 1;

                        std::string cfn("1");

                        if ( ( currcommand(0) == "-tgc" ) || ( currcommand(0) == "-tGc" ) )
                        {
                            cfn = currcommand(5);
                        }

                        gentype cf(cfn);


                        int jj,kk;
                        Vector<SparseVector<gentype> > xdata(N);
                        Vector<gentype> ydata(N);
                        SparseVector<SparseVector<gentype> > z;
                        Vector<double> Qweight(N);

                        Qweight = 1.0;

                        errstream() << "Generated test...\n";

                        // Generate x data

                        for ( jj = 0 ; jj < N ; ++jj )
                        {
                            for ( kk = 0 ; kk < d ; ++kk )
                            {
                                if ( !gorG )
                                {
                                    randnfill(xdata("&",jj)("&",kk)); // Gaussian, zero mean, unit variance.
                                }

                                else
                                {
                                    randufill(xdata("&",jj)("&",kk)); // Uniform 0-1
                                }
                            }

                            z("&",0) = xdata(jj);

                            randnfill(nadd);

                            ydata("&",jj) = f(z) + (nadd*v);

                            if ( ( (int) cf(z) ) != 1 )
                            {
                                --jj;
                            }
                        }

                        addtemptox(xdata,xtemplate);

                        testTest(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xdata,ydata,firstsum,finalresult,resfilter,argvariables,recordres,logres,recordxvar,getsetExtVar,0,0,useThreads);

                        argvariables("&",1)("&",1) = finalresult;
                    }

                    else if ( currcommand(0) == "-tMpy"   ) { gentype sf; safeatowhatever(sf,currcommand(2),argvariables); Vector<gentype> fv;                                                     pyorexeeval(1,0,2,currcommand(1),sf,fv,finalresult); }
                    else if ( currcommand(0) == "-tMpyv"  ) { gentype fv; safeatowhatever(fv,currcommand(2),argvariables); gentype sf; NiceAssert( fv.isValVector() ); NiceAssert( fv.infsize() ); pyorexeeval(0,1,2,currcommand(1),sf,fv.cast_vector(),finalresult); }
                    else if ( currcommand(0) == "-tMpyf"  ) { gentype sf; safeatowhatever(sf,currcommand(2),argvariables); Vector<gentype> fv;                                                     pyorexeeval(0,0,2,currcommand(1),sf,fv,finalresult); }

                    else if ( currcommand(0) == "-tMexe"  ) { gentype sf; safeatowhatever(sf,currcommand(2),argvariables); Vector<gentype> fv;                                                     pyorexeeval(1,0,0,currcommand(1),sf,fv,finalresult); }
                    else if ( currcommand(0) == "-tMexev" ) { gentype fv; safeatowhatever(fv,currcommand(2),argvariables); gentype sf; NiceAssert( fv.isValVector() ); NiceAssert( fv.infsize() ); pyorexeeval(0,1,0,currcommand(1),sf,fv.cast_vector(),finalresult); }
                    else if ( currcommand(0) == "-tMexef" ) { gentype sf; safeatowhatever(sf,currcommand(2),argvariables); Vector<gentype> fv;                                                     pyorexeeval(0,0,0,currcommand(1),sf,fv,finalresult); }

                    else
                    {
                        int firstsum = 1;
                        int numreps = 1;
                        int randcross = 0;
                        int numfolds = 0;

                        std::string subcom = (currcommand(0)).substr(3,((currcommand(0)).length())-3); // Contains suffixes only
                        int isANtype = ( (currcommand(0)).substr(0,3) == "-tF" );                              // Set if i j {k} suffixes present
                        int fileargpos = isANtype ? 3 : ( ( (currcommand(0)).substr(0,3) == "-tf" ) ? 1 : 0 ); // position of filename/number
                        int setibase = 0;                                                                              // set if ibase (k) present

                        int reverse = 0;              // set 1 if -AAe used.
                        int ignoreStart = 0;          // number to ignore at start
                        int imax = -1;                // max number to add, or -1 if no limit
                        int ibase = -1;               // where to start adding points, or -1 if end.
                        int uselinesvector = 0;       // if 1 then use linesread vector
                        int israw = 0;                // set if output is to be saved in raw format
                        int startpoint = 0;           // set if reoptimisation should start clean-slate
                        int coercetosingle = 0;       // if 1 then class label / target is read but disgarded and
                        int coercefromsingle = 0;     // if 1 then class label / target is given and file is assumed unlabelled
                        gentype fromsingletarget;     // see above
                        std::string trainfile;        // name of training file
                        Vector<int> linesread;        // vector containing lines to be read (if uselinesvector is set)

                        xlateDataSourceSuffixes(isANtype,fileargpos,setibase,currcommand,subcom,argvariables,filevariables,reverse,ignoreStart,imax,ibase,uselinesvector,israw,startpoint,coercetosingle,coercefromsingle,fromsingletarget,trainfile,linesread);

                        if ( uselinesvector )
                        {
                            std::string indexfilename = logfile+"_index_"+((currcommand(0)).substr(1,((currcommand(0)).length())-1));

                            writeLog(linesread,indexfilename,getsetExtVar);
                        }

                        if ( (currcommand(0)).substr(0,3) == "-tx" )
                        {
                            testLOO(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),firstsum,startpoint,finalresult,resfilter,argvariables,recordres,logres,recordxvar,getsetExtVar,useThreads);
                        }

                        else if ( (currcommand(0)).substr(0,3) == "-tl" )
                        {
                            testnegloglike(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),firstsum,finalresult,resfilter,argvariables,getsetExtVar,useThreads);
                        }

                        else if ( (currcommand(0)).substr(0,4) == "-tmg" )
                        {
                            testmaxinfogain(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),firstsum,finalresult,resfilter,argvariables,getsetExtVar,useThreads);
                        }

                        else if ( (currcommand(0)).substr(0,6) == "-ta" )
                        {
                            testRKHSnorm(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),firstsum,finalresult,resfilter,argvariables,getsetExtVar,useThreads);
                        }

                        else if ( (currcommand(0)).substr(0,3) == "-tr" )
                        {
                            testRecall(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),firstsum,finalresult,resfilter,argvariables,recordres,logres,recordxvar,getsetExtVar,useThreads);
                        }

                        else if ( ( (currcommand(0)).substr(0,3) == "-tc" ) || ( (currcommand(0)).substr(0,3) == "-tC" ) )
                        {
                            numreps    = ( (currcommand(0)).substr(0,3) == "-tC" ) ? safeatoi(currcommand(1),argvariables) : 1;
                            randcross  = ( (currcommand(0)).substr(0,3) == "-tC" ) ? 1 : 0;
                            numfolds   = ( (currcommand(0)).substr(0,3) == "-tC" ) ? safeatoi(currcommand(2),argvariables) : safeatoi(currcommand(1),argvariables);

                            testCross(logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),firstsum,numreps,startpoint,randcross,numfolds,finalresult,resfilter,argvariables,recordres,logres,recordxvar,getsetExtVar,useThreads);
                        }

                        else if ( ( (currcommand(0)).substr(0,3) == "-tf" ) || ( (currcommand(0)).substr(0,3) == "-tF" ) )
                        {
                            testFileVectors(binaryRelabel,singleDrop,logfile,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),trainfile,reverse,ignoreStart,imax,firstsum,coercetosingle,coercefromsingle,fromsingletarget,finalresult,uselinesvector,linesread,resfilter,argvariables,recordres,logres,recordxvar,getsetExtVar,xtemplate);
                        }
                    }

                    outstream() << "Error: " << finalresult << "\n";

                    argvariables("&",1)("&",1) = finalresult;
                }

                time_used endtime = TIMECALL;
                performtime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << performtime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }

            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Reporting steps

            if ( reportopt.size() )
            {
                errstream() << "Reporting and logging... ";

                time_used begintime = TIMECALL;

                std::string plotname = logfile+"_plot_pdf";
                std::string plotdataname = logfile+"_plot_pdat";
                int plotoutformat = 2;
                int plotoutsquare = 0;
                int plotincdata = 1;
                int plotincvar = 1;
                gentype plotbaseline;
                SparseVector<gentype> plotxtemplate;
                double plotymin = 1;
                double plotymax = 0;

                while ( reportopt.size() )
                {
                    currcommand = reportopt(0);
                    reportopt.remove(0);

                    if ( currcommand(0) == "-echo" )
                    {
                        std::stringstream evalx;
                        gentype echoval; safeatowhatever(echoval,currcommand(1),argvariables);
                        evalx << echoval << "\n";
                        stopnow = puttylump(evalx.str(),commstack);
                        outstream() << currcommand(1) << " = " << evalx.str();
                    }

                    if ( currcommand(0) == "-ECHO" )
                    {
                        std::stringstream evalx;
                        gentype echoval; safeatowhatever(echoval,currcommand(1),argvariables);
                        echoval.finalise(2); // First globals (leaving possible distributions in place)
                        echoval.finalise(1); // Then randoms that remain
                        echoval.finalise();  // Then just in case
                        evalx << echoval << "\n";
                        stopnow = puttylump(evalx.str(),commstack);
                        outstream() << currcommand(1) << " = " << evalx.str();
                    }

                    else if ( currcommand(0) == "-ak" )
                    {
                        std::stringstream xastr(currcommand(1));
                        SparseVector<gentype> xa;

                        streamItIn(xastr,xa,0);
                        addtemptox(xa,xtemplate);

                        int ii;

                        gentype sumtot(0.0);

                        for ( ii = 0 ; ii < getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N() ; ii++ )
                        {
                            gentype Kxx,Kii,Kxi;

                            getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).K2(Kxx,xa,xa);
                            getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).K2(Kii,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).x(ii),getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).x(ii));
                            getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).K2(Kxi,xa,getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).x(ii));

                            sumtot += ((Kxx*Kii)-(Kxi*Kxi));

                            errstream() << "K2(x,x).K2(xi,xi) - K2(x,xi).K2(x,xi) = " << ((Kxx*Kii)-(Kxi*Kxi)) << "\t" << sumtot << "\t" << (sumtot/((double) ii)) << "\t" << ii << "\n";
                        }
                    }

                    else if ( currcommand(0) == "-hU" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        gentype resh;
                        gentype resg;

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        streamItIn(xstr,x,0);

                        addtemptox(x,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).gh(resh,resg,x);

                        argvariables("&",1)("&",8) = resh;
                        argvariables("&",1)("&",9) = resg;

                        errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                        errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                    }

                    else if ( currcommand(0) == "-hhU" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        gentype resh;
                        gentype resg;

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        streamItIn(xstr,x,0);

                        addtemptox(x,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).gh(resh,resg,x,2);

                        argvariables("&",1)("&",8) = resh;
                        argvariables("&",1)("&",9) = resg;

                        errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                        errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                    }

                    else if ( currcommand(0) == "-hUe" )
                    {
                        std::stringstream xstr(currcommand(2));
                        SparseVector<gentype> x;
                        gentype y;

                        safeatowhatever(y,currcommand(1),argvariables);
                        streamItIn(xstr,x,0);

                        addtemptox(x,xtemplate);

                        double res = getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).e(y,x);

                        errstream() << "error(x) = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-hUE" )
                    {
                        std::stringstream xstr(currcommand(2));
                        SparseVector<gentype> x;
                        gentype y;

                        safeatowhatever(y,currcommand(1),argvariables);
                        streamItIn(xstr,x,0);

                        addtemptox(x,xtemplate);

                        gentype res;

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).dedg(res,y,x);

                        errstream() << "error gradient(x) = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-hP" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        double res;

                        streamItIn(xstr,x,0);

                        int p = safeatoi(currcommand(2),argvariables);
                        double mu = safeatof(currcommand(3),argvariables);
                        double B = safeatof(currcommand(4),argvariables);
                        double pnrm = safeatof(currcommand(5),argvariables);
                        int rot = 0;

                        addtemptox(x,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).stabProb(res,x,p,pnrm,rot,mu,B);

                        errstream() << "Pr(x) = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-hp" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        double res;

                        streamItIn(xstr,x,0);

                        int p = safeatoi(currcommand(2),argvariables);
                        double mu = safeatof(currcommand(3),argvariables);
                        double B = safeatof(currcommand(4),argvariables);
                        double pnrm = 1;
                        int rot = 1;

                        addtemptox(x,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).stabProb(res,x,p,pnrm,rot,mu,B);

                        errstream() << "Pr(x) = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-hPi" )
                    {
                        double res;

                        int i = safeatoi(currcommand(1),argvariables);
                        int p = safeatoi(currcommand(2),argvariables);
                        double mu = safeatof(currcommand(3),argvariables);
                        double B = safeatof(currcommand(4),argvariables);
                        double pnrm = safeatof(currcommand(5),argvariables);
                        int rot = 0;

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).stabProbTrainingVector(res,i,p,pnrm,rot,mu,B);

                        errstream() << "Pr(x) = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-hpi" )
                    {
                        double res;

                        int i = safeatoi(currcommand(1),argvariables);
                        int p = safeatoi(currcommand(2),argvariables);
                        double mu = safeatof(currcommand(3),argvariables);
                        double B = safeatof(currcommand(4),argvariables);
                        double pnrm = 1;
                        int rot = 1;

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).stabProbTrainingVector(res,i,p,pnrm,rot,mu,B);

                        errstream() << "Pr(x) = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-plotfn" )
                    {
                        double xmin   = safeatof(currcommand(1),argvariables);
                        double xmax   = safeatof(currcommand(2),argvariables);
                        double omin   = safeatof(currcommand(3),argvariables);
                        double omax   = safeatof(currcommand(4),argvariables);
                        gentype fn    = safeatog(currcommand(5),argvariables);
                        int usevar    = safeatoi(currcommand(6),argvariables);
                        std::string fname      = currcommand(7);
                        int outformat = safeatoi(currcommand(8),argvariables);
                        std::string dname = fname+"_data";

                        plotfn2d(xmin,xmax,omin,omax,fname,dname,outformat,fn,usevar);
                    }

                    else if ( currcommand(0) == "-surffn" )
                    {
                        double xmin   = safeatof(currcommand(1),argvariables);
                        double xmax   = safeatof(currcommand(2),argvariables);
                        double ymin   = safeatof(currcommand(3),argvariables);
                        double ymax   = safeatof(currcommand(4),argvariables);
                        double omin   = safeatof(currcommand(5),argvariables);
                        double omax   = safeatof(currcommand(6),argvariables);
                        gentype fn    = safeatog(currcommand(7),argvariables);
                        int xusevar   = safeatoi(currcommand(8),argvariables);
                        int yusevar   = safeatoi(currcommand(9),argvariables);
                        std::string fname      = currcommand(10);
                        int outformat = safeatoi(currcommand(11),argvariables);
                        std::string dname = fname+"_data";

                        surffn(xmin,xmax,ymin,ymax,omin,omax,fname,dname,outformat,fn,xusevar,yusevar);
                    }

                    else if ( currcommand(0) == "-K0" )
                    {
                        gentype res;

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).K0(res);

                        errstream() << "K0() = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-K1" )
                    {
                        std::stringstream xastr(currcommand(1));

                        SparseVector<gentype> xa;

                        gentype res;

                        streamItIn(xastr,xa,0);

                        addtemptox(xa,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).K1(res,xa);

                        errstream() << "K1(x) = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-phi2" )
                    {
                        std::stringstream xastr(currcommand(1));

                        SparseVector<gentype> xa;

                        Vector<gentype> res;

                        streamItIn(xastr,xa,0);

                        addtemptox(xa,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).phi2(res,xa);

                        errstream() << "phi2(x) = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-K2" )
                    {
                        std::stringstream xastr(currcommand(1));
                        std::stringstream xbstr(currcommand(2));

                        SparseVector<gentype> xa;
                        SparseVector<gentype> xb;

                        gentype res;

                        streamItIn(xastr,xa,0);
                        streamItIn(xbstr,xb,0);

                        addtemptox(xa,xtemplate);
                        addtemptox(xb,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).K2(res,xa,xb);

                        errstream() << "K2(x,y) = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-K3" )
                    {
                        std::stringstream xastr(currcommand(1));
                        std::stringstream xbstr(currcommand(2));
                        std::stringstream xcstr(currcommand(3));

                        SparseVector<gentype> xa;
                        SparseVector<gentype> xb;
                        SparseVector<gentype> xc;

                        gentype res;

                        streamItIn(xastr,xa,0);
                        streamItIn(xbstr,xb,0);
                        streamItIn(xcstr,xc,0);

                        addtemptox(xa,xtemplate);
                        addtemptox(xb,xtemplate);
                        addtemptox(xc,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).K3(res,xa,xb,xc);

                        errstream() << "K3(x,y,u) = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-K4" )
                    {
                        std::stringstream xastr(currcommand(1));
                        std::stringstream xbstr(currcommand(2));
                        std::stringstream xcstr(currcommand(3));
                        std::stringstream xdstr(currcommand(4));

                        SparseVector<gentype> xa;
                        SparseVector<gentype> xb;
                        SparseVector<gentype> xc;
                        SparseVector<gentype> xd;

                        gentype res;

                        streamItIn(xastr,xa,0);
                        streamItIn(xbstr,xb,0);
                        streamItIn(xcstr,xc,0);
                        streamItIn(xdstr,xd,0);

                        addtemptox(xa,xtemplate);
                        addtemptox(xb,xtemplate);
                        addtemptox(xc,xtemplate);
                        addtemptox(xd,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).K4(res,xa,xb,xc,xd);

                        errstream() << "K4(x,y,u,v) = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-Km" )
                    { 
                        int ii,m = safeatoi(currcommand(1),argvariables);

                        Vector<SparseVector<gentype> > xx(m);

                        for ( ii = 0 ; ii < m ; ++ii )
                        {
                            std::stringstream xstr(currcommand(ii+2));
                            streamItIn(xstr,xx("&",ii),0);
                        }

                        gentype res;

                        addtemptox(xx,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).Km(res,xx);

                        errstream() << "Km(...) = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-hY" )
                    {
                        std::stringstream xstr(currcommand(1)); 
                        SparseVector<gentype> x; 
                        gentype resh; 
                        gentype resg;
                        gentype tmpg;

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        x = safeatowhatever(tmpg,currcommand(1),argvariables).cast_vector(1);

                        addtemptox(x,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).gh(resh,resg,x);

                        argvariables("&",1)("&",8) = resh;
                        argvariables("&",1)("&",9) = resg;

                        errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                        errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                    }

                    else if ( currcommand(0) == "-hhY" )
                    {
                        std::stringstream xstr(currcommand(1)); 
                        SparseVector<gentype> x; 
                        gentype resh; 
                        gentype resg;
                        gentype tmpg;

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        x = safeatowhatever(tmpg,currcommand(1),argvariables).cast_vector(1);

                        addtemptox(x,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).gh(resh,resg,x,2);

                        argvariables("&",1)("&",8) = resh;
                        argvariables("&",1)("&",9) = resg;

                        errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                        errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                    }

                    else if ( currcommand(0) == "-hYe" )
                    {
                         SparseVector<gentype> x;
                         gentype y;
                         gentype tmpg;

                         safeatowhatever(y,currcommand(1),argvariables);
                         x = safeatowhatever(tmpg,currcommand(2),argvariables).cast_vector(1);

                         addtemptox(x,xtemplate);

                         double res = getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).e(y,x);

                         errstream() << "error(x) = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-hYE" )
                    {
                         SparseVector<gentype> x;
                         gentype y;
                         gentype tmpg;

                         safeatowhatever(y,currcommand(1),argvariables);
                         x = safeatowhatever(tmpg,currcommand(2),argvariables).cast_vector(1);

                         addtemptox(x,xtemplate);

                         gentype res;

                         getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).dedg(res,y,x);

                         errstream() << "error gradient(x) = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-hZ" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        gentype resh;
                        gentype resg;
                        gentype tmpg;
                        int nInd = safeatoi(currcommand(2),argvariables);

                        if ( nInd < 0 )
                        {
                            STRTHROW("Negative SVM index "+currcommand(2)+" in -hZ");
                        }

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        x = safeatowhatever(tmpg,currcommand(1),argvariables).cast_vector(1);

                        addtemptox(x,xtemplate);

                        getMLref(svmThreadOwner,svmbase,threadInd,nInd,svmContext).gh(resh,resg,x);

                        argvariables("&",1)("&",8) = resh;
                        argvariables("&",1)("&",9) = resg;

                        errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                        errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                    }

                    else if ( currcommand(0) == "-hhZ" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        gentype resh;
                        gentype resg;
                        gentype tmpg;
                        int nInd = safeatoi(currcommand(2),argvariables);

                        if ( nInd < 0 )
                        {
                            STRTHROW("Negative SVM index "+currcommand(2)+" in -hZ");
                        }

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        x = safeatowhatever(tmpg,currcommand(1),argvariables).cast_vector(1);

                        addtemptox(x,xtemplate);

                        getMLref(svmThreadOwner,svmbase,threadInd,nInd,svmContext).gh(resh,resg,x,2);

                        argvariables("&",1)("&",8) = resh;
                        argvariables("&",1)("&",9) = resg;

                        errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                        errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                    }

                    else if ( currcommand(0) == "-hV" )
                    {
                        std::stringstream xstr(currcommand(1));
                        Vector<SparseVector<gentype> > x;
                        Vector<gentype> resh;
                        Vector<gentype> resg;
                        int i;

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        streamItIn(xstr,x,0);

                        addtemptox(x,xtemplate);

                        if ( x.size() )
                        {
                            for ( i = 0 ; i < x.size() ; ++i )
                            {
                                getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).gh(resh("&",i),resg("&",i),x("&",i));
                            }

                            argvariables("&",1)("&",8) = resh;
                            argvariables("&",1)("&",9) = resg;

                            errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                            errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                        }
                    }

                    else if ( currcommand(0) == "-hhV" )
                    {
                        std::stringstream xstr(currcommand(1));
                        Vector<SparseVector<gentype> > x;
                        Vector<gentype> resh;
                        Vector<gentype> resg;
                        int i;

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        streamItIn(xstr,x,0);

                        addtemptox(x,xtemplate);

                        if ( x.size() )
                        {
                            for ( i = 0 ; i < x.size() ; ++i )
                            {
                                getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).gh(resh("&",i),resg("&",i),x("&",i),2);
                            }

                            argvariables("&",1)("&",8) = resh;
                            argvariables("&",1)("&",9) = resg;

                            errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                            errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                        }
                    }

                    else if ( currcommand(0) == "-hW" )
                    {
                        int i = safeatoi(currcommand(1),argvariables);
                        gentype resh;
                        gentype resg;

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).ghTrainingVector(resh,resg,i);

                        argvariables("&",1)("&",8) = resh;
                        argvariables("&",1)("&",9) = resg;

                        errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                        errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                    }

                    else if ( currcommand(0) == "-hhW" )
                    {
                        int i = safeatoi(currcommand(1),argvariables);
                        gentype resh;
                        gentype resg;

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).ghTrainingVector(resh,resg,i,2);

                        argvariables("&",1)("&",8) = resh;
                        argvariables("&",1)("&",9) = resg;

                        errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                        errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                    }

                    else if ( currcommand(0) == "-hWe" )
                    {
                        int i = safeatoi(currcommand(1),argvariables);

                        double res = getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).eTrainingVector(i);

                        errstream() << "error(x) = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-hWE" )
                    {
                        int i = safeatoi(currcommand(1),argvariables);

                        gentype res;

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).dedgTrainingVector(res,i);

                        errstream() << "error gradient(x) = " << res << "\n";
                    }

                    else if ( currcommand(0) == "-hX" )
                    {
                        int i;
                        Vector<gentype> resh(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N());
                        Vector<gentype> resg(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N());

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        if ( getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N() )
                        {
                            for ( i = 0 ; i < getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N() ; ++i )
                            {
                                getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).ghTrainingVector(resh("&",i),resg("&",i),i);
                            }
                        }

                        argvariables("&",1)("&",8) = resh;
                        argvariables("&",1)("&",9) = resg;

                        errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                        errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                    }

                    else if ( currcommand(0) == "-hhX" )
                    {
                        int i;
                        Vector<gentype> resh(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N());
                        Vector<gentype> resg(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N());

                        argvariables("&",1)("&",8).makeNull();
                        argvariables("&",1)("&",9).makeNull();

                        if ( getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N() )
                        {
                            for ( i = 0 ; i < getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N() ; ++i )
                            {
                                getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).ghTrainingVector(resh("&",i),resg("&",i),i,2);
                            }
                        }

                        argvariables("&",1)("&",8) = resh;
                        argvariables("&",1)("&",9) = resg;

                        errstream() << "h(x) = " << argvariables("&",1)("&",8) << "\n";
                        errstream() << "g(x) = " << argvariables("&",1)("&",9) << "\n";
                    }

                    else if ( currcommand(0) == "-hUc" )
                    {
                        std::stringstream xastr(currcommand(1));
                        std::stringstream xbstr(currcommand(2));
                        SparseVector<gentype> xa;
                        SparseVector<gentype> xb;
                        gentype resh,dummy;

                        streamItIn(xastr,xa,0);
                        streamItIn(xbstr,xb,0);

                        addtemptox(xa,xtemplate);
                        addtemptox(xb,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).cov(resh,dummy,xa,xb);

                        errstream() << "cov(x,y) = " << resh << "\n";
                    }

                    else if ( currcommand(0) == "-hYc" )
                    {
                        std::stringstream xastr(currcommand(1));
                        std::stringstream xbstr(currcommand(2));
                        SparseVector<gentype> xa;
                        SparseVector<gentype> xb;
                        gentype resh,dummy,tmpga,tmpgb;

                        xa = safeatowhatever(tmpga,currcommand(1),argvariables).cast_vector(1);
                        xb = safeatowhatever(tmpgb,currcommand(2),argvariables).cast_vector(1);

                        addtemptox(xa,xtemplate);
                        addtemptox(xb,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).cov(resh,dummy,xa,xb);

                        errstream() << "cov(x,y) = " << resh << "\n";
                    }

                    else if ( currcommand(0) == "-hZc" )
                    {
                        std::stringstream xastr(currcommand(1));
                        std::stringstream xbstr(currcommand(2));
                        SparseVector<gentype> xa;
                        SparseVector<gentype> xb;
                        gentype resh,dummy,tmpga,tmpgb;
                        int nInd = safeatoi(currcommand(3),argvariables);

                        if ( nInd < 0 )
                        {
                            STRTHROW("Negative SVM index "+currcommand(2)+" in -hZv");
                        }

                        xa = safeatowhatever(tmpga,currcommand(1),argvariables).cast_vector(1);
                        xb = safeatowhatever(tmpgb,currcommand(1),argvariables).cast_vector(1);

                        addtemptox(xa,xtemplate);
                        addtemptox(xb,xtemplate);

                        getMLref(svmThreadOwner,svmbase,threadInd,nInd,svmContext).cov(resh,dummy,xa,xb);

                        errstream() << "cov(x,y) = " << resh << "\n";
                    }

                    else if ( currcommand(0) == "-hWc" )
                    {
                        int ia = safeatoi(currcommand(1),argvariables);
                        int ib = safeatoi(currcommand(2),argvariables);
                        gentype resh,dummy;

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).covTrainingVector(resh,dummy,ia,ib);

                        errstream() << "cov(x,y) = " << resh << "\n";
                    }

                    else if ( currcommand(0) == "-hUv" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        gentype resh,dummy;

                        streamItIn(xstr,x,0);

                        addtemptox(x,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).var(resh,dummy,x);

                        errstream() << "var(x) = " << resh << "\n";
                    }

                    else if ( currcommand(0) == "-hYv" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        gentype resh,dummy,tmpga;

                        x = safeatowhatever(tmpga,currcommand(1),argvariables).cast_vector(1);

                        addtemptox(x,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).var(resh,dummy,x);

                        errstream() << "var(x) = " << resh << "\n";
                    }

                    else if ( currcommand(0) == "-hYvn" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        gentype resh,dummy,tmpga;

                        x = safeatowhatever(tmpga,currcommand(1),argvariables).cast_vector(1);

                        addtemptox(x,xtemplate);

                        std::stringstream xvstr(currcommand(1));
                        SparseVector<gentype> xv;

                        xv = safeatowhatever(tmpga,currcommand(2),argvariables).cast_vector(1);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).noisevar(resh,dummy,x,xv);

                        errstream() << "noisevar(x) = " << resh << "\n";
                    }

                    else if ( currcommand(0) == "-hZv" )
                    {
                        std::stringstream xstr(currcommand(1));
                        SparseVector<gentype> x;
                        gentype resh,dummy,tmpga;
                        int nInd = safeatoi(currcommand(2),argvariables);

                        if ( nInd < 0 )
                        {
                            STRTHROW("Negative SVM index "+currcommand(2)+" in -hZv");
                        }

                        x = safeatowhatever(tmpga,currcommand(1),argvariables).cast_vector(1);

                        addtemptox(x,xtemplate);

                        getMLref(svmThreadOwner,svmbase,threadInd,nInd,svmContext).var(resh,dummy,x);

                        errstream() << "var(x) = " << resh << "\n";
                    }

                    else if ( currcommand(0) == "-hWv" )
                    {
                        int i = safeatoi(currcommand(1),argvariables);
                        gentype resh,dummy;

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).varTrainingVector(resh,dummy,i);

                        errstream() << "var(x) = " << resh << "\n";
                    }

                    else if ( currcommand(0) == "-hVv" )
                    {
                        std::stringstream xstr(currcommand(1));
                        Vector<SparseVector<gentype> > x;
                        Vector<gentype> resh;
                        int i;
                        gentype dummy;

                        streamItIn(xstr,x,0);

                        addtemptox(x,xtemplate);

                        if ( x.size() )
                        {
                            for ( i = 0 ; i < x.size() ; ++i )
                            {
                                getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).var(resh("&",i),dummy,x("&",i));
                            }

                            errstream() << "var(x) = " << resh(i) << "\n";
                        }
                    }

                    else if ( currcommand(0) == "-hVV" )
                    {
                        std::stringstream xstr(currcommand(1));
                        Vector<SparseVector<gentype> > x;
                        Matrix<gentype> resh;

                        streamItIn(xstr,x,0);

                        addtemptox(x,xtemplate);

                        getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).covar(resh,x);

                        errstream() << "covar(x) = " << resh << "\n";
                    }

                    else if ( currcommand(0) == "-hXv" )
                    {
                        int i;
                        gentype dummy;
                        Vector<gentype> resh(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N());

                        if ( getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N() )
                        {
                            for ( i = 0 ; i < getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N() ; ++i )
                            {
                                getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).varTrainingVector(resh("&",i),dummy,i);
                            }
                        }

                        errstream() << "var(x) = " << resh << "\n";
                    }

                    else if ( currcommand(0) == "-hM" )
                    {
                        // Save data to matlab

                        gentype srcvar; safeatowhatever(srcvar,currcommand(2),argvariables);
                        gentype resvar;

                        resvar.makeString(currcommand(1));

                        (*getsetExtVar)(resvar,srcvar,-2);
                    }

                    else if ( currcommand(0) == "-hN" )
                    {
                        // Load data from matlab

                        int nn = safeatoi(currcommand(1),argvariables);

                        const gentype srcvar = argvariables("&",0)("&",nn);
                        gentype &resvar = argvariables("&",0)("&",nn);

                        resvar.makeString(currcommand(2));

                        (*getsetExtVar)(resvar,srcvar,-1);
                    }

                    else if ( currcommand(0) == "-a" )
                    {
                        NiceAssert( isSVM(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext)) );

                        argvariables("&",1)("&",18).makeString(currcommand(1));

                        writeLog(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).alpha(),currcommand(1),getsetExtVar);
                    }

                    else if ( currcommand(0) == "-b" )
                    {
                        NiceAssert( isSVM(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext)) );

                        argvariables("&",1)("&",19).makeString(currcommand(1));

                        Vector<gentype> biasfill(1);

                        biasfill("&",0) = getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).bias();

                        writeLog(biasfill,currcommand(1),getsetExtVar);
                    }

                    else if ( currcommand(0) == "-s" )
                    {
                        std::string svmfile = currcommand(1);

                        argvariables("&",1)("&",17).makeString(svmfile);

                        std::ofstream sfile;
                        sfile.open(svmfile.c_str(),std::ofstream::out);

                        if ( !sfile.is_open() )
                        {
                            STRTHROW("Unable to open svm file "+currcommand(0)+" "+svmfile);
                        }

                        sfile << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext);
                        sfile.close();
                    }

                    else if ( currcommand(0) == "-hpln" ) { plotname      = currcommand(1);                             }
                    else if ( currcommand(0) == "-hpld" ) { plotdataname  = currcommand(1);                             }
                    else if ( currcommand(0) == "-hpls" ) { plotoutsquare = safeatoi(currcommand(1),argvariables);      }
                    else if ( currcommand(0) == "-hplt" ) { plotoutformat = safeatoi(currcommand(1),argvariables);      }
                    else if ( currcommand(0) == "-hplD" ) { plotincdata   = safeatoi(currcommand(1),argvariables);      }
                    else if ( currcommand(0) == "-hplv" ) { plotincvar    = safeatoi(currcommand(1),argvariables);      }
                    else if ( currcommand(0) == "-hplb" ) { plotbaseline  = safeatog(currcommand(1),argvariables);      }
                    else if ( currcommand(0) == "-hplm" ) { plotymin      = safeatof(currcommand(1),argvariables);      }
                    else if ( currcommand(0) == "-hplM" ) { plotymax      = safeatof(currcommand(1),argvariables);      }
                    else if ( currcommand(0) == "-hplx" ) { safeatowhatever(plotxtemplate,currcommand(1),argvariables); }

                    else if ( currcommand(0) == "-plot" )
                    {
                        int index = safeatoi(currcommand(1),argvariables);

                        double xmin = safeatof(currcommand(2),argvariables);
                        double xmax = safeatof(currcommand(3),argvariables);

                        plotml(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),index,xmin,xmax,plotymin,plotymax,plotname,plotdataname,plotoutformat,plotincdata,plotbaseline,plotincvar,0,plotxtemplate,plotoutsquare);
                    }

                    else if ( currcommand(0) == "-surf" )
                    {
                        int xindex = safeatoi(currcommand(1),argvariables);

                        double xmin = safeatof(currcommand(2),argvariables);
                        double xmax = safeatof(currcommand(3),argvariables);

                        int yindex = safeatoi(currcommand(4),argvariables);

                        double ymin = safeatof(currcommand(5),argvariables);
                        double ymax = safeatof(currcommand(6),argvariables);

                        plotml(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext),xindex,yindex,xmin,xmax,ymin,ymax,plotymin,plotymax,plotname,plotdataname,plotoutformat,plotincdata,plotbaseline,plotincvar,0,1,plotxtemplate,plotoutsquare);
                    }
                }

                time_used endtime = TIMECALL;
                reporttime = TIMEDIFFSEC(endtime,begintime);

                errstream() << " done in " << reporttime << " sec\n";
                errstream() << "*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n";
            }


            // ---------------------------------------------------------------------------------------------------------------------------------------------------------------------

            // Write logfile, SVM file, alpha file etc

            // DISABLED
            if ( 0 )
            {
                errstream() << "Logging and saving...\n";

                currcommand = "**writing logfile "+logfile+"**";

                if ( verblevel && ( logfile.length() > 0 ) )
                {
                    errstream() << "Writing logfile... ";

                    std::string logfilenamefull = logfile+"_log";
                    std::ofstream loggerfile;
                    loggerfile.open(logfilenamefull.c_str(),std::ofstream::out);

                    if ( !loggerfile.is_open() )
                    {
                        STRTHROW("Unable to open log file "+currcommand(0)+" "+logfilenamefull);
                    }

                    loggerfile << "Logging setup time (sec):          " << loggingtime     << "\n";
                    loggerfile << "Multiple SVM setup time (sec):     " << multiruntime    << "\n";
                    loggerfile << "Setup time (sec):                  " << svmsetuptime    << "\n";
                    loggerfile << "Preload time (sec):                " << preloadtime     << "\n";
                    loggerfile << "Load time (sec):                   " << loadtime        << "\n";
                    loggerfile << "Postload time (sec):               " << postloadtime    << "\n";
                    loggerfile << "Learning setup time (sec):         " << learningtime    << "\n";
                    loggerfile << "Kernel setup time (sec):           " << kerneltime      << "\n";
                    loggerfile << "Tuning time (sec):                 " << tuningtime      << "\n";
                    loggerfile << "Grid time (sec):                   " << gridtime        << "\n";
                    loggerfile << "Kernel transfer time (sec):        " << xfertime        << "\n";
                    loggerfile << "Feature selection time (sec):      " << featuretime     << "\n";
                    loggerfile << "Fuzzy selection time (sec):        " << fuzzytime     << "\n";
                    loggerfile << "Optimisation time (sec):           " << optimtime       << "\n";
                    loggerfile << "Performance evaluation time (sec): " << performtime     << "\n";
                    loggerfile << "Report time (sec):                 " << reporttime      << "\n\n";

                    loggerfile << "Trained: " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).isTrained() << "\n\n";

                    loggerfile << "N:       " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).N()     << "\n";
                    loggerfile << "NNC(0):  " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).NNC(0)  << "\n\n";

                    loggerfile << "Target space dimension: " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).tspaceDim()  << "\n";
                    loggerfile << "Classes:                " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).numClasses() << "\n";
                    loggerfile << "SVM type:               " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).type()       << "\n\n";

                    loggerfile << "Class labels:         " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).ClassLabels() << "\n";

                    loggerfile << "Zero tolerance:      " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).zerotol()      << "\n";
                    loggerfile << "Optimal tolerance:   " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).Opttol()       << "\n";
                    loggerfile << "Max iterations:      " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).maxitcnt()     << "\n";
                    loggerfile << "Max training time:   " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).maxtraintime() << "\n\n";

                    loggerfile << "Underlying scalar:    " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).isUnderlyingScalar() << "\n";
                    loggerfile << "Underlying vectorial: " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).isUnderlyingVector() << "\n\n";
                    loggerfile << "Underlying anionic:   " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).isUnderlyingAnions() << "\n\n";

                    loggerfile << "Kernel dictionary size: " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().size()      << "\n";
                    loggerfile << "Kernel indexed:         " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().isIndex()   << "\n";
                    loggerfile << "Kernel indices:         " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().cIndexes()  << "\n\n";

                    if ( getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().size() )
                    {
                        for ( i = 0 ; i < getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().size() ; ++i )
                        {
                            loggerfile << "Kernel type       (" << i << "): " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().cType(i)          << "\n";
                            loggerfile << "Kernel weight     (" << i << "): " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().cWeight(i)        << "\n";
                            loggerfile << "Normalised        (" << i << "): " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().isNormalised(i)   << "\n";
                            loggerfile << "Integer constants (" << i << "): " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().cIntConstants(i)  << "\n";
                            loggerfile << "Real constants    (" << i << "): " << getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getKernel().cRealConstants(i) << "\n\n";
                        }

                        loggerfile << "\n";
                    }

                    if ( isSVM(getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext)) )
                    {
                        const SVM_Generic &locsvmref = getMLrefconst(svmThreadOwner,svmbase,threadInd,svmInd,svmContext).getSVMconst();

                        loggerfile << "SVM specifics:\n\n";

                        loggerfile << "NS: " << locsvmref.NS()  << "\n";
                        loggerfile << "NZ: " << locsvmref.NZ()  << "\n\n";

                        loggerfile << "Class representation: " << locsvmref.ClassRep()    << "\n\n";

                        loggerfile << "Linear cost:    " << locsvmref.isLinearCost()    << "\n";
                        loggerfile << "Quadratic cost: " << locsvmref.isQuadraticCost() << "\n";
                        loggerfile << "1-norm cost:    " << locsvmref.is1NormCost()     << "\n\n";

                        loggerfile << "Active set optimisation: " << locsvmref.isOptActive() << "\n";
                        loggerfile << "SMO type optimisation:   " << locsvmref.isOptSMO()    << "\n";
                        loggerfile << "D2C type optimisation:   " << locsvmref.isOptD2C()    << "\n\n";
                        loggerfile << "Grad type optimisation:  " << locsvmref.isOptGrad()   << "\n\n";

                        loggerfile << "Margin norm: " << locsvmref.m() << "\n\n";

                        loggerfile << "C:   " << locsvmref.C()   << "\n";
                        loggerfile << "eps: " << locsvmref.eps() << "\n\n";

                        loggerfile << "Kernel cache size:   " << locsvmref.memsize()      << "\n";
                        loggerfile << "Outer learning rate: " << locsvmref.outerlr()      << "\n";
                        loggerfile << "Outer momentum:      " << locsvmref.outermom()     << "\n";
                        loggerfile << "Outer optimal tol.:  " << locsvmref.outertol()     << "\n\n";

                        loggerfile << "Fuzzy weights on:     " << locsvmref.usefuzzt()     << "\n";
                        loggerfile << "Max fuzzy iterations: " << locsvmref.maxiterfuzzt() << "\n";
                        loggerfile << "Fuzzy learning rate:  " << locsvmref.lrfuzzt()      << "\n";
                        loggerfile << "Fuzzy zero tolerance: " << locsvmref.ztfuzzt()      << "\n";
                        loggerfile << "Fuzzy cost function:  " << locsvmref.costfnfuzzt()  << "\n\n";

                        loggerfile << "Fixed tube:     " << locsvmref.isFixedTube()  << "\n";
                        loggerfile << "Shrinking tube: " << locsvmref.isShrinkTube() << "\n\n";

                        loggerfile << "Epsilon restricted positive: " << locsvmref.isRestrictEpsPos() << "\n";
                        loggerfile << "Epsilon restricted negative: " << locsvmref.isRestrictEpsNeg() << "\n\n";

                        loggerfile << "Linear shrinking factor:    " << locsvmref.nu()     << "\n";
                        loggerfile << "Quadratic shrinking factor: " << locsvmref.nuQuad() << "\n\n";

                        loggerfile << "Classify via SVR: " << locsvmref.isClassifyViaSVR() << "\n";
                        loggerfile << "Classify via SVC: " << locsvmref.isClassifyViaSVM() << "\n\n";

                        loggerfile << "Multiclass via 1vsA:               " << locsvmref.is1vsA()    << "\n";
                        loggerfile << "Multiclass via 1vs1:               " << locsvmref.is1vs1()    << "\n";
                        loggerfile << "Multiclass via DAGSVM:             " << locsvmref.isDAGSVM()  << "\n";
                        loggerfile << "Multiclass via MOC:                " << locsvmref.isMOC()     << "\n";
                        loggerfile << "Multiclass via max wins:           " << locsvmref.ismaxwins() << "\n";
                        loggerfile << "Multiclass via recursive division: " << locsvmref.isrecdiv()  << "\n\n";

                        loggerfile << "SVM at-once:             " << locsvmref.isatonce() << "\n";
                        loggerfile << "SVM reduction-to-binary: " << locsvmref.isredbin() << "\n\n";

                        loggerfile << "Anomaly detection on:    " << locsvmref.isanomalyOn()  << "\n";
                        loggerfile << "Anomaly detection off:   " << locsvmref.isanomalyOff() << "\n";
                        loggerfile << "Anomaly detection nu:    " << locsvmref.anomalyNu()    << "\n";
                        loggerfile << "Anomaly detection class: " << locsvmref.anomalyClass() << "\n\n";

                        loggerfile << "Parameter autoset off:                 " << locsvmref.isautosetOff()          << "\n";
                        loggerfile << "Parameter autoset C/N scaling:         " << locsvmref.isautosetCscaled()      << "\n";
                        loggerfile << "Parameter autoset CK mean scaling:     " << locsvmref.isautosetCKmean()       << "\n";
                        loggerfile << "Parameter autoset CK median scaling:   " << locsvmref.isautosetCKmedian()     << "\n";
                        loggerfile << "Parameter autoset CK/N mean scaling:   " << locsvmref.isautosetCNKmean()      << "\n";
                        loggerfile << "Parameter autoset CK/N median scaling: " << locsvmref.isautosetCNKmedian()    << "\n";
                        loggerfile << "Parameter autoset CS++ scaling:        " << locsvmref.isautosetLinBiasForce() << "\n\n";

                        loggerfile << "Parameter autoset C value:  " << locsvmref.autosetCval()  << "\n";
                        loggerfile << "Parameter autoset nu value: " << locsvmref.autosetnuval() << "\n\n";

                        if ( isSVMMultiC(locsvmref) )
                        {
                            int Ndim = ( locsvmref.isatonce() ? locsvmref.numClasses() : locsvmref.tspaceDim() );
                            int d,q,qd;

                            if ( Ndim )
                            {
                                for ( q = 0 ; q < Ndim ; ++q )
                                {
                                    qd = locsvmref.isatonce() ? (locsvmref.ClassLabels())(q) : q;

                                    loggerfile << "NF  (" << qd << ") = " << locsvmref.NF (qd) << "\n";
                                    loggerfile << "NC  (" << qd << ") = " << locsvmref.NC (qd) << "\n";
                                    loggerfile << "NLB (" << qd << ") = " << locsvmref.NLB(qd) << "\n";
                                    loggerfile << "NLF (" << qd << ") = " << locsvmref.NLF(qd) << "\n";
                                    loggerfile << "NUF (" << qd << ") = " << locsvmref.NUF(qd) << "\n";
                                    loggerfile << "NUB (" << qd << ") = " << locsvmref.NUB(qd) << "\n\n";

                                    loggerfile << "Variable bias (" << qd << "):               " << locsvmref.isVarBias(qd)        << "\n";
                                    loggerfile << "Positive bias (" << qd << "):               " << locsvmref.isPosBias(qd)        << "\n";
                                    loggerfile << "Negative bias (" << qd << "):               " << locsvmref.isNegBias(qd)        << "\n";
                                    loggerfile << "Fixed bias (" << qd << "):                  " << locsvmref.isFixedBias(qd)      << "\n";
                                    loggerfile << "Linear bias forcing term (" << qd << "):    " << locsvmref.LinBiasForce(qd)  << "\n";
                                    loggerfile << "Quadratic bias forcing term (" << qd << "): " << locsvmref.QuadBiasForce(qd) << "\n\n";
                                }
                            }

                            if ( locsvmref.numClasses() )
                            {
                                for ( i = 0 ; i < locsvmref.numClasses() ; ++i )
                                {
                                    d = locsvmref.ClassLabels()(i);

                                    loggerfile << "NNC(" << d << "): " << locsvmref.NNC(d)      << "\n";
                                    loggerfile << "C(" << d << "):   " << locsvmref.Cclass(d)   << "\n";
                                    loggerfile << "eps(" << d << "): " << locsvmref.epsclass(d) << "\n\n";
                                }
                            }
                        }

                        else
                        {
                            loggerfile << "NF:      " << locsvmref.NF()    << "\n";
                            loggerfile << "NC:      " << locsvmref.NC()    << "\n";
                            loggerfile << "NLB:     " << locsvmref.NLB()   << "\n";
                            loggerfile << "NLF:     " << locsvmref.NLF()   << "\n";
                            loggerfile << "NUF:     " << locsvmref.NUF()   << "\n";
                            loggerfile << "NUB:     " << locsvmref.NUB()   << "\n\n";

                            loggerfile << "Bias variable:               " << locsvmref.isVarBias()        << "\n";
                            loggerfile << "Bias positive:               " << locsvmref.isPosBias()        << "\n";
                            loggerfile << "Bias negative:               " << locsvmref.isNegBias()        << "\n";
                            loggerfile << "Bias fixed:                  " << locsvmref.isFixedBias()      << "\n";
                            loggerfile << "Linear bias forcing term:    " << locsvmref.LinBiasForce()  << "\n";
                            loggerfile << "Quadratic bias forcing term: " << locsvmref.QuadBiasForce() << "\n\n";

                            if ( isSVMScalar(locsvmref) )
                            {
                                loggerfile << "C+:      " << locsvmref.Cclass(-1)   << "\n";
                                loggerfile << "C-:      " << locsvmref.Cclass(+1)   << "\n";
                                loggerfile << "C=:      " << locsvmref.Cclass(2)    << "\n";
                                loggerfile << "eps+:    " << locsvmref.epsclass(-1) << "\n";
                                loggerfile << "eps-:    " << locsvmref.epsclass(+1) << "\n";
                                loggerfile << "eps=:    " << locsvmref.epsclass(2)  << "\n";
                                loggerfile << "NNC(-1): " << locsvmref.NNC(-1)      << "\n";
                                loggerfile << "NNC(+1): " << locsvmref.NNC(+1)      << "\n";
                                loggerfile << "NNC(2):  " << locsvmref.NNC(2)          << "\n\n";
                            }

                            else
                            {
                                loggerfile << "C+:      " << locsvmref.Cclass(-1)   << "\n";
                                loggerfile << "C-:      " << locsvmref.Cclass(+1)   << "\n";
                                loggerfile << "eps+:    " << locsvmref.epsclass(-1) << "\n";
                                loggerfile << "eps-:    " << locsvmref.epsclass(+1) << "\n";
                                loggerfile << "NNC(-1): " << locsvmref.NNC(-1)      << "\n";
                                loggerfile << "NNC(+1): " << locsvmref.NNC(+1)      << "\n\n";
                            }
                        }
                    }








                    loggerfile.close();

                    errstream() << " done.\n";
                }
            }

            // Acknowledge block boundary if not finished

            if ( !stopnow )
            {
                errstream() << "=======================================================================\n";
            }
          }

          catch ( const char *error )
          {
            errstream() << "Error thrown during " << currcommand << " operation: " << error << "\n";
            retval  = 301;
            stopnow = 1;
          }

          catch ( const std::string &error )
          {
            errstream() << "Error thrown during " << currcommand << " operation: " << error << "\n";
            retval  = 302;
            stopnow = 1;
          }

          //catch ( ... )
          //{
          //  errstream() << "Unknown error during " << currcommand << " operation.\n";
          //  retval  = 303;
          //  stopnow = 1;
          //}
        }
    }

    emptycommstack(commstack);

    --depthin;

    if ( !depthin )
    {
        controlThreadInd = -1; // This signals that the thread is no longer operating
    }

    return retval;
}











































//
// Callback function used by Bayesian optimiser
//

int gridelmMLreg(int ind, ML_Mutable *MLreg, void *arg)
{
//    std::string &commandstr                                            = *((std::string                           *) ((void **) arg)[0] );
    SparseVector<SVMThreadContext *> &svmContext                       = *((SparseVector<SVMThreadContext *>      *) ((void **) arg)[1] );
//    int &verblevel                                                     = *((int                                   *) ((void **) arg)[2] );
//    gentype &finalresult                                               = *((gentype                               *) ((void **) arg)[3] );
//    std::string &logfile                                               = *((std::string                           *) ((void **) arg)[4] );
//    int &depthin                                                       = *((int                                   *) ((void **) arg)[5] );
//    SparseVector<SparseVector<gentype> > &argvariables                 = *((SparseVector<SparseVector<gentype> >  *) ((void **) arg)[6] );
    int &threadInd                                                     = *((int                                   *) ((void **) arg)[7] );
//    svmvolatile SparseVector<SparseVector<gentype> > &globargvariables = *((SparseVector<SparseVector<gentype> >  *) ((void **) arg)[8] );
//    Vector<int> &argnums                                               = *((Vector<int>                           *) ((void **) arg)[9] );
//    int (*getsetExtVar)(gentype &, const gentype &, int)               = ((int (*)(gentype &, const gentype &, int)) ((void **) arg)[10]);
    SparseVector<ML_Mutable *> &svmbase                                = *((SparseVector<ML_Mutable *>            *) ((void **) arg)[11]);
    SparseVector<int> &svmThreadOwner                                  = *((SparseVector<int>                     *) ((void **) arg)[12]);
//    std::string &interstring                                           = *((std::string                           *) ((void **) arg)[13]);
//    gentype &xfnis                                                     = *((gentype                               *) ((void **) arg)[14]);
//    Vector<int> &MLnumbers                                             = *((Vector<int>                           *) ((void **) arg)[15]);
//    std::string &prestring                                             = *((std::string                           *) ((void **) arg)[16]);
//    std::string &midstring                                             = *((std::string                           *) ((void **) arg)[17]);

    return regsvm(svmThreadOwner,svmbase,threadInd,ind,svmContext,MLreg);
}

void gridelmrun(gentype &res, Vector<gentype> &x, void *arg)
{
    std::string &commandstr                                            = *((std::string                           *) ((void **) arg)[0] );
    SparseVector<SVMThreadContext *> &svmContext                       = *((SparseVector<SVMThreadContext *>      *) ((void **) arg)[1] );
    int &verblevel                                                     = *((int                                   *) ((void **) arg)[2] );
    gentype &finalresult                                               = *((gentype                               *) ((void **) arg)[3] );
    std::string &logfile                                               = *((std::string                           *) ((void **) arg)[4] );
    int &depthin                                                       = *((int                                   *) ((void **) arg)[5] );
    SparseVector<SparseVector<gentype> > &argvariables                 = *((SparseVector<SparseVector<gentype> >  *) ((void **) arg)[6] );
    int &threadInd                                                     = *((int                                   *) ((void **) arg)[7] );
    svmvolatile SparseVector<SparseVector<gentype> > &globargvariables = *((SparseVector<SparseVector<gentype> >  *) ((void **) arg)[8] );
    Vector<int> &argnums                                               = *((Vector<int>                           *) ((void **) arg)[9] );
    int (*getsetExtVar)(gentype &, const gentype &, int)               = ((int (*)(gentype &, const gentype &, int)) ((void **) arg)[10]);
    SparseVector<ML_Mutable *> &svmbase                                = *((SparseVector<ML_Mutable *>            *) ((void **) arg)[11]);
    SparseVector<int> &svmThreadOwner                                  = *((SparseVector<int>                     *) ((void **) arg)[12]);
    std::string &interstring                                           = *((std::string                           *) ((void **) arg)[13]);
    gentype &xfnis                                                     = *((gentype                               *) ((void **) arg)[14]);
    Vector<int> &MLnumbers                                             = *((Vector<int>                           *) ((void **) arg)[15]);
    std::string &prestring                                             = *((std::string                           *) ((void **) arg)[16]);
    std::string &midstring                                             = *((std::string                           *) ((void **) arg)[17]);
    int &rawitcnt                                                      = *((int                                   *) ((void **) arg)[18]);
    double &rawstarttime                                               = *((double                                *) ((void **) arg)[19]);

    (void) verblevel;
    (void) logfile;
    (void) finalresult;
    (void) depthin;

    // NB: res can be used to pass an integer *into* the code.  This is the itnum

    int itnum = 0;

    if ( res.isValInteger() )
    {
        itnum = (int) res;
    }

    int itnumalt = (-itnum)/10000; // relevant if itnum < 0
    int itnumid  = (-itnum)%10000; // relevant if itnum < 0

    SparseVector<SparseVector<gentype> > gridargvars(argvariables);

    NiceAssert( ( x.size() >= argnums.size() ) || !x.size() );

    int i;

    // MLnumbers is a curious case.  It is *shared* with the optimiser, so these variables
    // can (are) used to pass arguments back to the grid run!

    gridargvars("&",90)("&",0)  = MLnumbers(0);
    gridargvars("&",90)("&",1)  = MLnumbers(1);
    gridargvars("&",90)("&",2)  = MLnumbers(2);
    gridargvars("&",90)("&",3)  = MLnumbers(3);
    gridargvars("&",90)("&",4)  = ( itnum >= 0 ) ? itnum : itnumalt;
    gridargvars("&",90)("&",5)  = MLnumbers(4);
    gridargvars("&",90)("&",6)  = MLnumbers(5);
    gridargvars("&",90)("&",7)  = ++rawitcnt;
    gridargvars("&",90)("&",8)  = rawstarttime;
    gridargvars("&",90)("&",9)  = MLnumbers(6);
    gridargvars("&",90)("&",10) = MLnumbers(7);

    if ( x.size() )
    {
        // standard evaluation with argument x

        for ( i = 0 ; i < x.size() ; ++i )
        {
            if ( i < argnums.size() )
            {
                gridargvars("&",0)("&",argnums(i)) = x(i);
            }

            else
            {
                // This is bayes on grid, result in ML is stored in x(dim),
                // so load into gridargvars(0,0)

                gridargvars("&",5)("&",5) = x(i);
            }
        }

        int locverblevel = verblevel;
        int adim = ( x.size() < argnums.size() ) ? x.size() : argnums.size();

        retVector<gentype> tmpva;

        errstream() << "Running grid " << x(0,1,adim-1,tmpva) << " ... ";

        if ( argnums.size() < x.size() )
        {
            errstream() << "Grid test value recall = " << gridargvars(0)(0) << "\n";
        }

        // Bookkeeping stuff.  Not entirely sure what this does - probably
        // should have documented it better when I first wrote it.

        std::stringstream *tmpcommand;
        MEMNEW(tmpcommand,std::stringstream(commandstr));
        awarestream *gridbox;
        MEMNEW(gridbox,awarestream(tmpcommand,1));
        Stack<awarestream *> *gridcommstack;
        MEMNEW(gridcommstack,Stack<awarestream *>);
        gridcommstack->push(gridbox);
        std::string loclogfile = logfile+"_gridlog";

        callsvm(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,gridargvars,locverblevel,res,loclogfile);

        MEMDEL(gridcommstack);
    }

    else if ( ( itnum == 0 ) && interstring.length() )
    {
        int locverblevel = verblevel;

        errstream() << "Running intermediate evaluation ... ";

        // Bookkeeping stuff.  Not entirely sure what this does - probably
        // should have documented it better when I first wrote it.

        std::stringstream *tmpcommand;
        MEMNEW(tmpcommand,std::stringstream(interstring));
        awarestream *gridbox;
        MEMNEW(gridbox,awarestream(tmpcommand,1));
        Stack<awarestream *> *gridcommstack;
        MEMNEW(gridcommstack,Stack<awarestream *>);
        gridcommstack->push(gridbox);
        std::string loclogfile = logfile+"_gridlog";

        // Call runsvm to get test result and save result

        callsvm(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,gridargvars,locverblevel,res,loclogfile);

        MEMDEL(gridcommstack);
    }

    else if ( ( itnum < 0 ) && ( itnumid == 1 ) && prestring.length() )
    {
        int locverblevel = verblevel;

        errstream() << "Running pre-optimisation setup operation (" << prestring << ") ... ";

        // Bookkeeping stuff.  Not entirely sure what this does - probably
        // should have documented it better when I first wrote it.

        std::stringstream *tmpcommand;
        MEMNEW(tmpcommand,std::stringstream(prestring));
        awarestream *gridbox;
        MEMNEW(gridbox,awarestream(tmpcommand,1));
        Stack<awarestream *> *gridcommstack;
        MEMNEW(gridcommstack,Stack<awarestream *>);
        gridcommstack->push(gridbox);
        std::string loclogfile = logfile+"_gridlog";

        // Call runsvm to get test result and save result

        callsvm(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,gridargvars,locverblevel,res,loclogfile);

        MEMDEL(gridcommstack);
    }

    else if ( ( itnum < 0 ) && ( itnumid == 2 ) && midstring.length() )
    {
        int locverblevel = verblevel;

        errstream() << "Running intermediate setup operation ... ";

        // Bookkeeping stuff.  Not entirely sure what this does - probably
        // should have documented it better when I first wrote it.

        std::stringstream *tmpcommand;
        MEMNEW(tmpcommand,std::stringstream(midstring));
        awarestream *gridbox;
        MEMNEW(gridbox,awarestream(tmpcommand,1));
        Stack<awarestream *> *gridcommstack;
        MEMNEW(gridcommstack,Stack<awarestream *>);
        gridcommstack->push(gridbox);
        std::string loclogfile = logfile+"_gridlog";

        // Call runsvm to get test result and save result

        callsvm(threadInd,svmContext,svmbase,svmThreadOwner,gridcommstack,globargvariables,getsetExtVar,gridargvars,locverblevel,res,loclogfile);

        MEMDEL(gridcommstack);
    }

    // Update x if relevant

    if ( !xfnis.isValNull() )
    {
        x = (const Vector<gentype> &) xfnis(gridargvars);
    }

    // Exit - we're done

    errstream() << "Grid raw error: " << res << "\n";

    return;
}

































void emptycommstack(Stack<awarestream *> &commstack)
{
    while ( commstack.size() )
    {
        MEMDEL(commstack.accessTop());

        commstack.pop();
    }

    return;
}

int grabnextarg(Stack<awarestream *> &commstack, std::string &currentarg)
{
    int stopnow = 0;

    while ( !stopnow )
    {
        if ( commstack.size() == 0 )
        {
            // Stack is empty, no more inputs available.

            stopnow = 1;
        }

        else if ( !((commstack.accessTop())->good()) )
        {
            // Top of stack is exhausted.  Delete (which
            // will close file streams) the variable on
            // top of the stack and, pop it off and try
            // again.

            // see also below

            MEMDEL(commstack.accessTop());

            commstack.pop();
        }

        else
        {
            // Stream off next argument.

            currentarg = "";

            if ( (commstack.accessTop())->skim(currentarg) )
            {
                // See above

                MEMDEL(commstack.accessTop());

                commstack.pop();
            }

            else if ( currentarg.length() )
            {
#ifdef MANGLE_UNDERSCORES
                int i;

                for ( i = 0 ; i < (int) currentarg.length() ; ++i )
                {
                    if ( currentarg[i] == '_' )
                    {
                        currentarg[i] = ' ';
                    }
                }
#endif

                break;
            }
        }
    }

    return stopnow;
}

int grabargs(int num, Vector<Vector<std::string> > &destlist, Stack<awarestream *> &commstack, std::string &currentarg, int allowvetor)
{
    int i,j;
    size_t k;
    int quotelevel;
    int escchar;

    if ( num > 0 )
    {
        std::string oldarg = currentarg;
        std::string innerarg;

        destlist.add(j = destlist.size());
        // BE CAREFUL!!!  Because of the way vectors work, it is quite
        // possible that the contents of the cell just added will actually
        // be the preserved contents of whatever was in this cell previously
        // (prior to it being deleted), if there were contents previously.
        // This actually can result in bugs.  Specifically, if it had
        // contents (a vector of strings), these were "removed", and then
        // this line re-adds them, then the contents of detlist(j) will be
        // the old arguments, and the new arguments will be concatenated onto
        // these!
        destlist("&",j).resize(0);

        for ( i = 0 ; i < num ; ++i )
        {
            if ( i )
            {
                // If allowvector then the arguments may be a vector.  To
                // allow for this we have a parenthesis stack.

                Stack<char> parstack;
                quotelevel = 0;
                currentarg = "";
                escchar = 0;

                do
                {
                    if ( grabnextarg(commstack,innerarg) )
                    {
                        errstream() << "Syntax error: " << oldarg << " requires " << num << " arguments\n";
                        return 1;
                    }

                    // Parenthesise traversed using stack.  {[( are put on
                    // stack, )]} cancel their pair off the top of the stack,
                    // quotes are modal.

                    // Note that escaped characters are skipped

                    if ( allowvetor && innerarg.length() )
                    {
                        escchar = 0;

                        for ( k = 0 ; k < innerarg.length() ; ++k )
                        {
                            if ( !escchar && ( innerarg[k] == '\\' ) )
                            {
                                escchar = 1;
                                innerarg.erase(k,1);
                                --k;
                            }

                            else if ( escchar )
                            {
                                switch ( innerarg[k] )
                                {
                                    case 'a': { innerarg[k] = '\a'; break; }
                                    case 'b': { innerarg[k] = '\b'; break; }
                                    case 'f': { innerarg[k] = '\f'; break; }
                                    case 'n': { innerarg[k] = '\n'; break; }
                                    case 'r': { innerarg[k] = '\r'; break; }
                                    case 't': { innerarg[k] = '\t'; break; }
                                    case 'v': { innerarg[k] = '\v'; break; }
                                    default: { break; }
                                }

                                // default interpretted as "just keep it"

                                escchar = 0;
                            }

                            else if ( !quotelevel )
                            {
                                if ( ( innerarg[k] == '[' ) ||
                                     ( innerarg[k] == '{' ) ||
                                     ( innerarg[k] == '(' )    )
                                {
                                    parstack.push(innerarg[k]);
                                }

                                else if ( innerarg[k] == ')' )
                                {
                                    if ( parstack.isempty() )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }

                                    else if ( parstack.accessTop() != '(' )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }

                                    else if ( parstack.pop() )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }
                                }

                                else if ( innerarg[k] == ']' )
                                {
                                    if ( parstack.isempty() )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }

                                    else if ( parstack.accessTop() != '[' )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }

                                    else if ( parstack.pop() )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }
                                }

                                else if ( innerarg[k] == '}' )
                                {
                                    if ( parstack.isempty() )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }

                                    else if ( parstack.accessTop() != '{' )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }

                                    else if ( parstack.pop() )
                                    {
                                        errstream() << "Unpaired brackets error.\n";
                                        return 1;
                                    }
                                }

                                else if ( innerarg[k] == '\"' )
                                {
                                    quotelevel = 1;
                                }
                            }

                            else if ( innerarg[k] == '\"' )
                            {
                                quotelevel = 0;
                            }
                        }
                    }

                    currentarg += innerarg;

                    if ( quotelevel || parstack.size() )
                    {
                        currentarg += " ";
                    }
                }
                while ( quotelevel || parstack.size() );
            }

            stripquotes(currentarg);

            destlist("&",j).add(i);
            destlist("&",j)("&",i) = currentarg;
        }
    }

    return 0;
}

int puttylump(const std::string &src, Stack<awarestream *> &commstack)
{
    int stopnow = 0;

    while ( !stopnow )
    {
        if ( commstack.size() == 0 )
        {
            // Stack is empty, no more outputs available.

            stopnow = 1;
        }

        else
        {
            // Push out src

            if ( (commstack.accessTop())->vogon(src) )
            {
                MEMDEL(commstack.accessTop());

                commstack.pop();
            }

            else
            {
                break;
            }
        }
    }

    return stopnow;
}

void stripcurlybrackets(std::string &evalarg)
{
    int repcnt = 0;

    while ( evalarg.length() && ( ( ( evalarg[0] == '{' ) && !repcnt ) || isspace(evalarg[0]) ) )
    {
        evalarg = evalarg.substr(1,evalarg.length()-1);

        if ( !evalarg.length() )
        {
            break;
        }

        ++repcnt;
    }

    repcnt = 0;

    while ( evalarg.length() && ( ( ( evalarg[evalarg.length()-1] == '}' ) && !repcnt ) || isspace(evalarg[evalarg.length()-1]) ) )
    {
        evalarg = evalarg.substr(0,evalarg.length()-1);

        if ( !evalarg.length() )
        {
            break;
        }

        ++repcnt;
    }

    return;
}

void stripquotes(std::string &evalarg)
{
    int repcnt = 0;

    while ( evalarg.length() && ( ( ( evalarg[0] == '\"' ) && !repcnt ) || isspace(evalarg[0]) ) )
    {
        evalarg = evalarg.substr(1,evalarg.length()-1);

        if ( !evalarg.length() )
        {
            break;
        }

        ++repcnt;
    }

    repcnt = 0;

    while ( evalarg.length() && ( ( ( evalarg[evalarg.length()-1] == '\"' ) && !repcnt ) || isspace(evalarg[evalarg.length()-1]) ) )
    {
        evalarg = evalarg.substr(0,evalarg.length()-1);

        if ( !evalarg.length() )
        {
            break;
        }

        ++repcnt;
    }

    return;
}

int &safeatowhatever(int &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    return res = safeatoi(src,argvariables);
}

double &safeatowhatever(double &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    return res = safeatof(src,argvariables);
}

gentype &safeatowhatever(gentype &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    return res = safeatog(src,argvariables);
}

template <class T> Vector<T> &safeatowhatever(Vector<T> &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    gentype temp(safeatog(src,argvariables));

    temp.morph_vector();

    int res_size = temp.size();

    res.resize(res_size);

    if ( res_size )
    {
        int i;

        const Vector<gentype> &ghgh = (const Vector<gentype> &) temp;

        for ( i = 0 ; i < res_size ; ++i )
        {
            res("&",i) = (T) ghgh(i);
        }
    }

    return res;
}

template <class T> SparseVector<T> &safeatowhatever(SparseVector<T> &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    // reset target

    res.zero();

    // convert string to sparse vector of gentypes using stream operators

    SparseVector<gentype> tempval;

    std::stringstream tmpstore(src);
    tmpstore >> tempval;

    // Substitute and cast to target

    int i;

    for ( i = 0 ; i < tempval.indsize() ; ++i )
    {
        ((tempval.direref(i)).substitute(argvariables));
        res("&",tempval.ind(i)) = (T) tempval.direcref(i);
    }

    return res;
}

template <class T> Matrix<T> &safeatowhatever(Matrix<T> &res, const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    std::string xsrc("M:");

    xsrc = xsrc + src;

    gentype temp(safeatog(xsrc,argvariables));

    temp.morph_matrix();

    int res_nrows = temp.numRows();
    int res_ncols = temp.numCols();

    res.resize(res_nrows,res_ncols);

    if ( res_nrows && res_ncols )
    {
        int i,j;

        const Matrix<gentype> &ghgh = (const Matrix<gentype> &) temp;

        for ( i = 0 ; i < res_nrows ; ++i )
        {
            for ( j = 0 ; j < res_ncols ; ++j )
            {
                res("&",i,j) = (T) ghgh(i,j);
            }
        }
    }

    return res;
}

int safeatoi(const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    gentype srceqn(src);
    int res;

    srceqn.substitute(argvariables);

    if ( srceqn.isCastableToIntegerWithoutLoss() )
    {
        res = (int) srceqn;
    }

    else
    {
        std::string errstring;
        errstring = "Syntax error: "+src+" does not evaluate as integer.";
        NiceThrow(errstring);
    }

    return res;
}

double safeatof(const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    gentype srceqn(src);
    double res;

    srceqn.substitute(argvariables);

    if ( srceqn.isCastableToRealWithoutLoss() )
    {
        res = (double) srceqn;
    }

    else
    {
        std::string errstring;
        errstring = "Syntax error: "+src+" does not evaluate as double.";
        NiceThrow(errstring);
    }

    return res;
}

gentype safeatog(const std::string &src, SparseVector<SparseVector<gentype> > &argvariables)
{
    gentype res(src);

    res.substitute(argvariables);

    return res;
}

void xlateDataSourceSuffixes(int isANtype, int fileargpos, int setibase, const Vector<std::string> &currcommand, const std::string &subcom, SparseVector<SparseVector<gentype> > &argvariables, SparseVector<ofiletype> &filevariables,
     int &reverse, int &ignoreStart, int &imax, int &ibase, int &uselinesvector, int &israw, int &startpoint, int &coercetosingle, int &coercefromsingle, gentype &fromsingletarget, std::string &trainfile, Vector<int> &linesread)
{
    int randorder = 0; // set 1 for random ordering
    int filenum = -1;  // file number in -AA...i... type commands
    int uselinesa = 0;
    int uselinesb = 0;

    trainfile = currcommand(fileargpos);

    reverse          = ( subcom.find("e") != std::string::npos );
    ignoreStart      = isANtype ? safeatoi(currcommand(1),argvariables) : 0;
    imax             = isANtype ? safeatoi(currcommand(2),argvariables) : -1;
    ibase            = ( isANtype && setibase ) ? safeatoi(currcommand(3),argvariables) : -1;
    randorder        = ( subcom.find("r") != std::string::npos ) || ( subcom.find("R") != std::string::npos );
    uselinesa        = ( subcom.find("r") != std::string::npos ) || ( subcom.find("i") != std::string::npos );
    uselinesb        = ( subcom.find("R") != std::string::npos ) || ( subcom.find("I") != std::string::npos );
    uselinesvector   = uselinesa ? 1 : ( uselinesb ? 2 : 0 );
    israw            = ( subcom.find("B") != std::string::npos );
    startpoint       = ( subcom.find("z") != std::string::npos ) ? 1 : 0;
    startpoint       = ( subcom.find("f") != std::string::npos ) ? 4 : 0;
    startpoint      |= ( subcom.find("Z") != std::string::npos ) ? 2 : 0;
    startpoint      |= ( subcom.find("F") != std::string::npos ) ? 2 : 0;
    coercetosingle   = ( subcom.find("u") != std::string::npos );
    coercefromsingle = ( subcom.find("l") != std::string::npos );
    filenum          = uselinesvector ? safeatoi(trainfile,argvariables) : 0;

    // make -foe work correctly!
    reverse = filenum ? filevariables("&",filenum).gettargpos() : reverse;

    fromsingletarget = 0;

    if ( coercefromsingle )
    {
        safeatowhatever(fromsingletarget,currcommand(fileargpos+1),argvariables);
    }

    if ( uselinesvector )
    {
        preExtractLinesFromFile(filevariables("&",filenum),argvariables("&",0)("&",filenum),trainfile,ignoreStart,imax,linesread,randorder);

        ignoreStart = 0;
        imax = -1;
        ibase = -1;
    }

    return;
}

void preExtractLinesFromFile(ofiletype &filelines, gentype &linesleft, std::string &trainfile, int ignoreStart, int imax, Vector<int> &linesread, int randorder)
{
    if ( filelines.getlinecnt() < ignoreStart )
    {
        std::string errstring;
        errstring = "Error: cannot ignore more lines than are left in a file.";
        NiceThrow(errstring);
    }

    if ( imax == -1 )
    {
        imax = filelines.getlinecnt();
    }

    imax = ( (filelines.getlinecnt())-ignoreStart <= imax ) ? (filelines.getlinecnt())-ignoreStart : imax;

    trainfile = filelines.getfilename();

    int jjj = 0;
    int kkk;

    // Grab line numbers

    linesread.resize(imax);

    while ( imax )
    {
        //kkk = randorder ? (svm_rand()%((filelines.getlinecnt())-ignoreStart))+ignoreStart : ignoreStart;
        kkk = randorder ? (rand()%((filelines.getlinecnt())-ignoreStart))+ignoreStart : ignoreStart;
        linesread("&",jjj) = filelines.pullline(kkk);

        ++jjj;
        --imax;
    }

    // Sort vector into order

    int cii,cjj,ckk;

    if ( linesread.size() > 1 )
    {
        for ( cii = 0 ; cii < linesread.size()-1 ; ++cii )
        {
            for ( cjj = cii+1 ; cjj < linesread.size() ; ++cjj )
            {
                if ( linesread(cii) > linesread(cjj) )
                {
                    ckk                 = linesread(cii);
                    linesread("&",cii) = linesread(cjj);
                    linesread("&",cjj) = ckk;
                }
            }
        }
    }

    // Fix line count variable

    linesleft = filelines.getlinecnt();

    return;
}






















































void processKernel(ML_Base &kernML, MercerKernel &theKern, const std::string &currcommandis, const Vector<std::string> &currcommand, int ktype, SparseVector<SparseVector<gentype> > &argvariables, int &kernnum, int firstcall, SparseVector<int> &svmThreadOwner, SparseVector<ML_Mutable *> &svmbase, int threadInd, int svmInd, SparseVector<SVMThreadContext *> &svmContext)
{
    (void) svmInd;

    // Process this first

    if ( ( ktype == 0 ) && ( currcommand(0) == "-ktk" ) )
    {
        if ( isMLM(kernML) )
        {
            if ( kernML.isMutable() )
            {
                ML_Mutable &kernMLmut = dynamic_cast<ML_Mutable &>(kernML);
                MLM_Generic &kernMLM = kernMLmut.getMLM();

                kernMLM.setknum(safeatoi(currcommand(1),argvariables));
            }

            else
            {
                MLM_Generic &kernMLM = dynamic_cast<MLM_Generic &>(kernML);

                kernMLM.setknum(safeatoi(currcommand(1),argvariables));
            }
        }

        firstcall = 1; // trigger kernnum re-calculation
    }

    // Calculate multi-layer kernel offset

    int keroffset = 0;

    if ( ( ktype == 0 ) && isMLM(kernML) )
    {
        if ( kernML.isMutable() )
        {
            ML_Mutable &kernMLmut = dynamic_cast<ML_Mutable &>(kernML);
            MLM_Generic &kernMLM = kernMLmut.getMLM();

            keroffset = ( kernMLM.tsize() && kernMLM.knum() ) ? 1 : 0;
        }

        else
        {
            MLM_Generic &kernMLM = dynamic_cast<MLM_Generic &>(kernML);

            keroffset = ( kernMLM.tsize() && kernMLM.knum() ) ? 1 : 0;
        }
    }

    // Calculate kernnum on firstcall

    if ( firstcall )
    {
        kernnum = keroffset;
    }

    // Update kernnum as required

    if ( currcommandis == "-ki" )
    {
        kernnum = keroffset + safeatoi(currcommand(1),argvariables);

        if ( kernnum < 0 )
        {
            STRTHROW("Error: kernel number must be non-negative or zero");
        }

        else if ( kernnum >= theKern.size() )
        {
            STRTHROW("Error: kernel number out of dictionary range.");
        }
    }

    // Prepare kernel if required

    if ( ( currcommandis == "-ks"  ) || ( currcommandis == "-kn"  ) || ( currcommandis == "-ku"   ) ||
         ( currcommandis == "-knn" ) || ( currcommandis == "-kuu" ) || ( currcommandis == "-krn"  ) ||
         ( currcommandis == "-kss" ) || ( currcommandis == "-kus" ) ||
         ( currcommandis == "-ku"  ) || ( currcommandis == "-ka"  ) || ( currcommandis == "-kp"   ) ||
         ( currcommandis == "-knp" ) ||
         ( currcommandis == "-kb"  ) || ( currcommandis == "-ke"  ) || ( currcommandis == "-kc"   ) ||
         ( currcommandis == "-kuc" ) || ( currcommandis == "-kS"  ) || ( currcommandis == "-kA"   ) || ( currcommandis == "-kuS"  ) || 
         ( currcommandis == "-kMA" ) || ( currcommandis == "-kMS" ) || ( currcommandis == "-kMuS" ) ||
         ( currcommandis == "-km"  ) || ( currcommandis == "-kum" ) || ( currcommandis == "-kI"   ) ||
         ( currcommandis == "-kw"  ) || ( currcommandis == "-kt"  ) || ( currcommandis == "-kan"  ) ||
         ( currcommandis == "-kg"  ) || ( currcommandis == "-kr"  ) || ( currcommandis == "-kd"   ) ||
         ( currcommandis == "-kG"  ) || ( currcommandis == "-kf"  ) || ( currcommandis == "-kv"   ) ||
         ( currcommandis == "-kV"  ) || ( currcommandis == "-kgg" ) || ( currcommandis == "-kx"   )    )
    {
        if ( ktype == 0 )
        {
            kernML.prepareKernel();
        }
    }

    // Process the command

                        SparseVector<int> newrealover;
                        SparseVector<int> newintover;
                        Vector<int> tempresa;
                        Vector<gentype> tempresb; 

                             if ( currcommandis == "-ks"   ) { theKern.resize(safeatoi(currcommand(1),argvariables)); }
                        else if ( currcommandis == "-kn"   ) { theKern.setNormalised(kernnum); }
                        else if ( currcommandis == "-kp"   ) { theKern.setProd(); }
                        else if ( currcommandis == "-knp"  ) { theKern.setnonProd(); }
                        else if ( currcommandis == "-ku"   ) { theKern.setUnNormalised(kernnum); }
                        else if ( currcommandis == "-kss"  ) { theKern.setSymmSet(); }
                        else if ( currcommandis == "-kus"  ) { theKern.setNoSymmSet(); }
                        else if ( currcommandis == "-knn"  ) { theKern.setFullNorm(); }
                        else if ( currcommandis == "-kuu"  ) { theKern.setNoFullNorm(); }
                        else if ( currcommandis == "-krn"  ) { theKern.setrankType(safeatoi(currcommand(1),argvariables)); }
                        else if ( currcommandis == "-koz"  ) { theKern.setRealOverwrite(newrealover,kernnum); }
                        else if ( currcommandis == "-kOz"  ) { theKern.setIntOverwrite(newintover,kernnum); }
                        else if ( currcommandis == "-ku"   ) { theKern.setUnNormalised(kernnum); }
                        else if ( currcommandis == "-ka"   ) { theKern.setnumSamples(safeatoi(currcommand(1),argvariables)); }
                        else if ( currcommandis == "-kb"   ) { theKern.setSampleIndices(safeatowhatever(tempresa,currcommand(1),argvariables)); }
                        else if ( currcommandis == "-ke"   ) { theKern.setSampleDistribution(safeatowhatever(tempresb,currcommand(1),argvariables)); }
                        else if ( currcommandis == "-kc"   ) { theKern.setChained(kernnum); }
                        else if ( currcommandis == "-kuc"  ) { theKern.setUnChained(kernnum); }
                        else if ( currcommandis == "-kS"   ) { theKern.setSplit(kernnum); }
                        else if ( currcommandis == "-kA"   ) { theKern.setSplitAdd(kernnum); }
                        else if ( currcommandis == "-kuS"  ) { theKern.setUnSplit(kernnum); }
                        else if ( currcommandis == "-kMS"  ) { theKern.setMulSplit(kernnum); }
                        else if ( currcommandis == "-kMA"  ) { theKern.setAddSplit(kernnum); }
                        else if ( currcommandis == "-kMuS" ) { theKern.setUnMulSplit(kernnum); }
                        else if ( currcommandis == "-km"   ) { theKern.setMagTerm(kernnum); }
                        else if ( currcommandis == "-kum"  ) { theKern.setUnMagTerm(kernnum); }
                        else if ( currcommandis == "-kU"   ) { theKern.setUnIndex(); }
                        else if ( currcommandis == "-kw"   ) { gentype tmpg; safeatowhatever(tmpg,currcommand(1),argvariables); theKern.setWeight(tmpg,kernnum); }
                        else if ( currcommandis == "-kwlb"   ) { gentype tmpg; safeatowhatever(tmpg,currcommand(1),argvariables); theKern.setWeightLB(tmpg,kernnum); }
                        else if ( currcommandis == "-kwub"   ) { gentype tmpg; safeatowhatever(tmpg,currcommand(1),argvariables); theKern.setWeightUB(tmpg,kernnum); }
                        else if ( currcommandis == "-kt"   ) { theKern.setType(safeatoi(currcommand(1),argvariables),kernnum); }
                        else if ( currcommandis == "-mtb"  ) { theKern.setsuggestXYcache(1); }
                        else if ( currcommandis == "-bmx"  ) { theKern.setsuggestXYcache(0); }
//                        else if ( currcommandis == "-kcy " ) { theKern.setChurnInner(1); }
//                        else if ( currcommandis == "-kcn"  ) { theKern.setChurnInner(0); }
                        else if ( currcommandis == "-kan"  ) { theKern.setAltDiff(safeatoi(currcommand(1),argvariables)); }
                        else if ( currcommandis == "-ktx"  ) { theKern.setAltCall((getMLref(svmThreadOwner,svmbase,threadInd,safeatoi(currcommand(1),argvariables),svmContext)).MLid(),kernnum); }

                        else if ( currcommandis == "-kI" ) 
                        { 
                            Vector<int> vectInd; 

                            safeatowhatever(vectInd,currcommand(1),argvariables); 

                            theKern.setIndexes(vectInd);
                        }

                        else if ( currcommandis == "-ko" )
                        {
                            int destind = safeatoi(currcommand(1),argvariables)+1;
                            int srcind  = safeatoi(currcommand(2),argvariables);

                            if ( destind < 0 )
                            {
                                STRTHROW("Error: destination index must be >= -1 in -ko");
                            }

                            if ( srcind < 0 )
                            {
                                STRTHROW("Error: source index must be >= 0 in -ko");
                            }

                            SparseVector<int> newrealover(theKern.cRealOverwrite());

                            newrealover("&",destind) = srcind;

                            theKern.setRealOverwrite(newrealover,kernnum);
                        }

                        else if ( currcommandis == "-kO" )
                        {
                            int destind = safeatoi(currcommand(1),argvariables);
                            int srcind  = safeatoi(currcommand(2),argvariables);

                            if ( destind < 0 )
                            {
                                STRTHROW("Error: destination index must be >= 0 in -kO");
                            }

                            if ( srcind < 0 )
                            {
                                STRTHROW("Error: source index must be >= 0 in -kO");
                            }

                            SparseVector<int> newintover(theKern.cIntOverwrite());

                            newintover("&",destind) = srcind;

                            theKern.setIntOverwrite(newintover,kernnum);
                        }

                        else if ( currcommandis == "-kgg" )
                        {
                            Vector<gentype> xxscale;
                            SparseVector<gentype> xscale;

                            xscale = safeatowhatever(xxscale,currcommand(1),argvariables);

errstream() << "set xscale: " << xscale << "\n";
                            theKern.setScale(xscale);
                        }

                        else if ( currcommandis == "-kr" )
                        {
                            Vector<gentype> kernRealConsts(theKern.cRealConstants(kernnum));
                            gentype temparg;

                            if ( kernRealConsts.size() <= 1 )
                            {
                                STRTHROW("Error: constant r1 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernRealConsts("&",1) = temparg;

                            theKern.setRealConstants(kernRealConsts,kernnum);
                        }

                        else if ( currcommandis == "-kg" )
                        {
                            Vector<gentype> kernRealConsts(theKern.cRealConstants(kernnum));
                            gentype temparg;

                            if ( kernRealConsts.size() <= 0 )
                            {
                                STRTHROW("Error: constant r0 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernRealConsts("&",0) = temparg;

                            theKern.setRealConstants(kernRealConsts,kernnum);
                        }

                        else if ( currcommandis == "-kd" )
                        {
                            Vector<int> kernIntConsts(theKern.cIntConstants(kernnum));
                            int temparg;

                            if ( kernIntConsts.size() <= 0 )
                            {
                                STRTHROW("Error: constant i0 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernIntConsts("&",0) = temparg;

                            theKern.setIntConstants(kernIntConsts,kernnum);
                        }

                        else if ( currcommandis == "-kG" )
                        {
                            Vector<int> kernIntConsts(theKern.cIntConstants(kernnum));
                            int temparg;

                            if ( kernIntConsts.size() <= 0 )
                            {
                                STRTHROW("Error: constant i0 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernIntConsts("&",0) = temparg;

                            theKern.setIntConstants(kernIntConsts,kernnum);
                        }








                        else if ( currcommandis == "-krlb" )
                        {
                            Vector<gentype> kernRealConstsLB(theKern.cRealConstantsLB(kernnum));
                            gentype temparg;

                            if ( kernRealConstsLB.size() <= 1 )
                            {
                                STRTHROW("Error: constant r1 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernRealConstsLB("&",1) = temparg;

                            theKern.setRealConstantsLB(kernRealConstsLB,kernnum);
                        }

                        else if ( currcommandis == "-kglb" )
                        {
                            Vector<gentype> kernRealConstsLB(theKern.cRealConstantsLB(kernnum));
                            gentype temparg;

                            if ( kernRealConstsLB.size() <= 0 )
                            {
                                STRTHROW("Error: constant r0 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernRealConstsLB("&",0) = temparg;

                            theKern.setRealConstantsLB(kernRealConstsLB,kernnum);
                        }

                        else if ( currcommandis == "-kdlb" )
                        {
                            Vector<int> kernIntConstsLB(theKern.cIntConstantsLB(kernnum));
                            int temparg;

                            if ( kernIntConstsLB.size() <= 0 )
                            {
                                STRTHROW("Error: constant i0 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernIntConstsLB("&",0) = temparg;

                            theKern.setIntConstantsLB(kernIntConstsLB,kernnum);
                        }

                        else if ( currcommandis == "-kGlb" )
                        {
                            Vector<int> kernIntConstsLB(theKern.cIntConstantsLB(kernnum));
                            int temparg;

                            if ( kernIntConstsLB.size() <= 0 )
                            {
                                STRTHROW("Error: constant i0 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernIntConstsLB("&",0) = temparg;

                            theKern.setIntConstantsLB(kernIntConstsLB,kernnum);
                        }









                        else if ( currcommandis == "-krub" )
                        {
                            Vector<gentype> kernRealConstsUB(theKern.cRealConstantsUB(kernnum));
                            gentype temparg;

                            if ( kernRealConstsUB.size() <= 1 )
                            {
                                STRTHROW("Error: constant r1 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernRealConstsUB("&",1) = temparg;

                            theKern.setRealConstantsUB(kernRealConstsUB,kernnum);
                        }

                        else if ( currcommandis == "-kgub" )
                        {
                            Vector<gentype> kernRealConstsUB(theKern.cRealConstantsUB(kernnum));
                            gentype temparg;

                            if ( kernRealConstsUB.size() <= 0 )
                            {
                                STRTHROW("Error: constant r0 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernRealConstsUB("&",0) = temparg;

                            theKern.setRealConstantsUB(kernRealConstsUB,kernnum);
                        }

                        else if ( currcommandis == "-kdub" )
                        {
                            Vector<int> kernIntConstsUB(theKern.cIntConstantsUB(kernnum));
                            int temparg;

                            if ( kernIntConstsUB.size() <= 0 )
                            {
                                STRTHROW("Error: constant i0 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernIntConstsUB("&",0) = temparg;

                            theKern.setIntConstantsUB(kernIntConstsUB,kernnum);
                        }

                        else if ( currcommandis == "-kGub" )
                        {
                            Vector<int> kernIntConstsUB(theKern.cIntConstantsUB(kernnum));
                            int temparg;

                            if ( kernIntConstsUB.size() <= 0 )
                            {
                                STRTHROW("Error: constant i0 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernIntConstsUB("&",0) = temparg;

                            theKern.setIntConstantsUB(kernIntConstsUB,kernnum);
                        }









                        else if ( currcommandis == "-kf" )
                        {
                            Vector<gentype> kernRealConsts(theKern.cRealConstants(kernnum));
                            gentype temparg;

                            if ( kernRealConsts.size() <= 10 )
                            {
                                STRTHROW("Error: constant r10 not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(1),argvariables);

                            kernRealConsts("&",10) = temparg;

                            theKern.setRealConstants(kernRealConsts,kernnum);
                        }

                        else if ( currcommandis == "-kx" )
                        {
                            std::stringstream xastr(currcommand(1));
                            std::stringstream xbstr(currcommand(2));

                            Vector<int> linGradOrd;
                            Vector<Matrix<double> > linGradScal;

                            streamItIn(xastr,linGradOrd,0);
                            //streamItIn(xbstr,linGradScal,0);
                            xbstr >> linGradScal;

                            theKern.setlinGradOrd(linGradOrd);
                            theKern.setlinGradScal(linGradScal);
                        }

                        else if ( currcommandis == "-kv" )
                        {
                            Vector<gentype> kernRealConsts(theKern.cRealConstants(kernnum));
                            gentype temparg;

                            if ( kernRealConsts.size() <= safeatoi(currcommand(1),argvariables) )
                            {
                                STRTHROW("Error: constant r"+currcommand(1)+" not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(2),argvariables);

                            kernRealConsts("&",safeatoi(currcommand(1),argvariables)) = temparg;

                            theKern.setRealConstants(kernRealConsts,kernnum);
                        }

                        else if ( currcommandis == "-kV" )
                        {
                            Vector<int> kernIntConsts(theKern.cIntConstants(kernnum));
                            int temparg;

                            if ( kernIntConsts.size() <= safeatoi(currcommand(1),argvariables) )
                            {
                                STRTHROW("Error: constant i"+currcommand(1)+" not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(2),argvariables);

                            kernIntConsts("&",safeatoi(currcommand(1),argvariables)) = temparg;

                            theKern.setIntConstants(kernIntConsts,kernnum);
                        }








                        else if ( currcommandis == "-kvlb" )
                        {
                            Vector<gentype> kernRealConstsLB(theKern.cRealConstantsLB(kernnum));
                            gentype temparg;

                            if ( kernRealConstsLB.size() <= safeatoi(currcommand(1),argvariables) )
                            {
                                STRTHROW("Error: constant r"+currcommand(1)+" not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(2),argvariables);

                            kernRealConstsLB("&",safeatoi(currcommand(1),argvariables)) = temparg;

                            theKern.setRealConstantsLB(kernRealConstsLB,kernnum);
                        }

                        else if ( currcommandis == "-kVlb" )
                        {
                            Vector<int> kernIntConstsLB(theKern.cIntConstants(kernnum));
                            int temparg;

                            if ( kernIntConstsLB.size() <= safeatoi(currcommand(1),argvariables) )
                            {
                                STRTHROW("Error: constant i"+currcommand(1)+" not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(2),argvariables);

                            kernIntConstsLB("&",safeatoi(currcommand(1),argvariables)) = temparg;

                            theKern.setIntConstantsLB(kernIntConstsLB,kernnum);
                        }







                        else if ( currcommandis == "-kvub" )
                        {
                            Vector<gentype> kernRealConstsUB(theKern.cRealConstantsUB(kernnum));
                            gentype temparg;

                            if ( kernRealConstsUB.size() <= safeatoi(currcommand(1),argvariables) )
                            {
                                STRTHROW("Error: constant r"+currcommand(1)+" not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(2),argvariables);

                            kernRealConstsUB("&",safeatoi(currcommand(1),argvariables)) = temparg;

                            theKern.setRealConstantsUB(kernRealConstsUB,kernnum);
                        }

                        else if ( currcommandis == "-kVub" )
                        {
                            Vector<int> kernIntConstsUB(theKern.cIntConstants(kernnum));
                            int temparg;

                            if ( kernIntConstsUB.size() <= safeatoi(currcommand(1),argvariables) )
                            {
                                STRTHROW("Error: constant i"+currcommand(1)+" not present in kernel.");
                            }

                            safeatowhatever(temparg,currcommand(2),argvariables);

                            kernIntConstsUB("&",safeatoi(currcommand(1),argvariables)) = temparg;

                            theKern.setIntConstantsUB(kernIntConstsUB,kernnum);
                        }







    // Reset kernel if required

    if ( ( currcommandis == "-ks"  ) || ( currcommandis == "-kn"  ) || ( currcommandis == "-ku"   ) ||
         ( currcommandis == "-knn" ) || ( currcommandis == "-kuu" ) || ( currcommandis == "-krn"  ) ||
         ( currcommandis == "-kss" ) || ( currcommandis == "-kus" ) ||
         ( currcommandis == "-ku"  ) || ( currcommandis == "-ka"  ) || ( currcommandis == "-kp"   ) ||
         ( currcommandis == "-knp" ) ||
         ( currcommandis == "-kb"  ) || ( currcommandis == "-ke"  ) || ( currcommandis == "-kc"   ) ||
         ( currcommandis == "-kuc" ) || ( currcommandis == "-kS"  ) || ( currcommandis == "-kA"   ) || ( currcommandis == "-kuS"  ) ||
         ( currcommandis == "-kMA" ) || ( currcommandis == "-kMS" ) || ( currcommandis == "-kMuS" ) ||
         ( currcommandis == "-km"  ) || ( currcommandis == "-kum" ) || ( currcommandis == "-kw"   ) ||
         ( currcommandis == "-kt"  ) || ( currcommandis == "-kg"  ) || ( currcommandis == "-kr"   ) ||
         ( currcommandis == "-kd"  ) || ( currcommandis == "-kG"  ) || ( currcommandis == "-kf"   ) ||
         ( currcommandis == "-kv"  ) || ( currcommandis == "-kV"  ) || ( currcommandis == "-kx"   )    )
    {
        if ( ktype == 0 )
        {
            kernML.resetKernel(0,-1,0);
        }

        else if ( ktype == 1 )
        {
            kernML.resetUUOutputKernel(0);
        }

        else if ( ktype == 3 )
        {
            kernML.resetRFFKernel(0);
        }
    }

    if ( ( currcommandis == "-ko"  ) || ( currcommandis == "-kO"  ) || ( currcommandis == "-koz" ) ||
         ( currcommandis == "-kOz" ) || ( currcommandis == "-kan" ) || ( currcommandis == "-ktx" ) )
    {
        if ( ktype == 0 )
        {
            kernML.resetKernel(0);
        }

        else if ( ktype == 1 )
        {
             kernML.resetUUOutputKernel(0);
        }

        else if ( ktype == 3 )
        {
             kernML.resetRFFKernel(0);
        }
    }

    if ( ( currcommandis == "-kI"  ) || ( currcommandis == "-kU"  ) || ( currcommandis == "-kgg" ) )
    {
        if ( ktype == 0 )
        {
            kernML.resetKernel();
        }

        else if ( ktype == 1 )
        {
            kernML.resetUUOutputKernel();
        }

        else if ( ktype == 3 )
        {
            kernML.resetRFFKernel();
        }
    }

    return;
}









































void testTest(std::string &logfile, const ML_Mutable &svmbase, const Vector<SparseVector<gentype> > &xtest, const Vector<gentype> &ytest, int &firstsum, gentype &finalresult, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int logres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num), int startpoint, int suppressfb, int useThreads)
{
    Vector<int> cnt;
    Matrix<int> cfm;
    double res = 0.0;
    int i;
    Vector<gentype> resh;
    Vector<gentype> resg;
    Vector<gentype> gvarres;

    res = calcTest(svmbase,xtest,ytest,cnt,cfm,resh,resg,gvarres,recordxvar,startpoint,suppressfb,useThreads);

    std::string resfilenamefull = logfile+"_test_res";
    std::string tarfilenamefull = logfile+"_test_tar";
    std::string clafilenamefull = logfile+"_test_cla";
    std::string varfilenamefull = logfile+"_test_var";

    if ( logres )
    {
        writeLog(resg,resfilenamefull,getsetExtVar);
        writeLog(ytest,tarfilenamefull,getsetExtVar);
        writeLog(resh,clafilenamefull,getsetExtVar);
    }

    if ( recordxvar )
    {
        writeLog(gvarres,varfilenamefull,getsetExtVar);
    }

    if ( recordres )
    {
        argvariables("&",1)("&",5) = resh;
        argvariables("&",1)("&",6) = resg;
    }

    errstream() << "\n";

    Vector<double> accsum(7);

    // "Binary classifier" include anomaly, isClassifier does not

    if ( isBinaryClassify(svmbase) || svmbase.isClassifier() )
    {
        retVector<int> tmpva;

        measureAccuracy(accsum,resg,resh,ytest,oneintvec(resg.size(),tmpva),svmbase);
    }

    else
    {
        accsum = -1.0;

        accsum("&",0) = 1/(res+1e-6);
        accsum("&",5)         = svmbase.sparlvl();
        accsum("&",6)         = res;
    }

    errstream() << "Accuracy:  " << accsum(0) << "\n";
    errstream() << "Precision: " << accsum(1) << "\n";
    errstream() << "Recall:    " << accsum(2) << "\n";
    errstream() << "F1 Score:  " << accsum(3) << "\n";
    errstream() << "AUC:       " << accsum(4) << "\n";
    errstream() << "Sparsity:  " << accsum(5) << "\n";
    errstream() << "Error:     " << accsum(6) << "\n";

    argvariables("&",1)("&",37) = accsum(0);
    argvariables("&",1)("&",38) = accsum(1);
    argvariables("&",1)("&",39) = accsum(2);
    argvariables("&",1)("&",40) = accsum(3);
    argvariables("&",1)("&",41) = accsum(4);
    argvariables("&",1)("&",42) = accsum(5);

    argvariables("&",1)("&",2) = res;
    argvariables("&",1)("&",3) = cnt;
    argvariables("&",1)("&",4) = cfm;

    argvariables("&",1)("&",44).force_vector(cnt.size());

    for ( i = 0 ; i < cnt.size() ; ++i )
    {
        ((argvariables("&",1)("&",44)).dir_vector())("&",i) = cnt(i) ? ( 1.0 - (((double) cfm(i,i))/((double) cnt(i))) ) : 0.0;
    }

    Vector<gentype> &clr = (argvariables("&",1)("&",44)).dir_vector();

    errstream() << "Test error: "      << res << "\n";
    errstream() << "Class counts: "    << cnt << "\n";
    errstream() << "Confusion: "       << cfm << "\n";
    errstream() << "Classwise error: " << clr << "\n";

    finalresult = resfilter(argvariables);

    (void) firstsum;

    std::string sumfilenamefull = logfile+"_test_sum";
    std::string cntfilenamefull = logfile+"_test_cnt";
    std::string cfmfilenamefull = logfile+"_test_cfm";
    std::string clrfilenamefull = logfile+"_test_clr";

    writeLog(accsum,sumfilenamefull,getsetExtVar);
    writeLog(cnt   ,cntfilenamefull,getsetExtVar);
    writeLog(cfm   ,cfmfilenamefull,getsetExtVar);
    writeLog(clr   ,clrfilenamefull,getsetExtVar);

    return;
}

void testLOO(std::string &logfile, const ML_Mutable &svmbase, int &firstsum, int startpoint, gentype &finalresult, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int logres,int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num), int useThreads)
{
    Vector<int> cnt;
    Matrix<int> cfm;
    double res = 0.0;
    int i;
    Vector<gentype> resh;
    Vector<gentype> resg;
    Vector<gentype> gvarres;

    res = calcLOO(svmbase,cnt,cfm,resh,resg,gvarres,startpoint,recordxvar,useThreads);

    std::string resfilenamefull = logfile+"_LOO_res";
    std::string tarfilenamefull = logfile+"_LOO_tar";
    std::string clafilenamefull = logfile+"_LOO_cla";
    std::string varfilenamefull = logfile+"_LOO_var";

    if ( logres )
    {
        writeLog(resg,resfilenamefull,getsetExtVar);
        writeLog(svmbase.y(),tarfilenamefull,getsetExtVar);
        writeLog(resh,clafilenamefull,getsetExtVar);
    }

    if ( recordxvar )
    {
        writeLog(gvarres,varfilenamefull,getsetExtVar);
    }

    if ( recordres )
    {
        argvariables("&",1)("&",5) = resh;
        argvariables("&",1)("&",6) = resg;
    }

    errstream() << "\n";

    Vector<double> accsum(7);

    // "Binary classifier" include anomaly, isClassifier does not

    if ( isBinaryClassify(svmbase) || svmbase.isClassifier() )
    {
        measureAccuracy(accsum,resg,resh,svmbase.y(),svmbase.d(),svmbase);
    }

    else
    {
        accsum = -1.0;

        accsum("&",0) = 1/(res+1e-6);
        accsum("&",5)         = svmbase.sparlvl();
        accsum("&",6)         = res;
    }

    errstream() << "Accuracy:  " << accsum(0) << "\n";
    errstream() << "Precision: " << accsum(1) << "\n";
    errstream() << "Recall:    " << accsum(2) << "\n";
    errstream() << "F1 Score:  " << accsum(3) << "\n";
    errstream() << "AUC:       " << accsum(4) << "\n";
    errstream() << "Sparsity:  " << accsum(5) << "\n";
    errstream() << "Error:     " << accsum(6) << "\n";

    argvariables("&",1)("&",37) = accsum(0);
    argvariables("&",1)("&",38) = accsum(1);
    argvariables("&",1)("&",39) = accsum(2);
    argvariables("&",1)("&",40) = accsum(3);
    argvariables("&",1)("&",41) = accsum(4);
    argvariables("&",1)("&",42) = accsum(5);

    argvariables("&",1)("&",2) = res;
    argvariables("&",1)("&",3) = cnt;
    argvariables("&",1)("&",4) = cfm;

    argvariables("&",1)("&",44).force_vector(cnt.size());

    for ( i = 0 ; i < cnt.size() ; ++i )
    {
        ((argvariables("&",1)("&",44)).dir_vector())("&",i) = cnt(i) ? ( 1.0 - (((double) cfm(i,i))/((double) cnt(i))) ) : 0.0;
    }

    Vector<gentype> &clr = (argvariables("&",1)("&",44)).dir_vector();

    errstream() << "LOO error: "       << res << "\n";
    errstream() << "Class counts: "    << cnt << "\n";
    errstream() << "Confusion: "       << cfm << "\n";
    errstream() << "Classwise error: " << clr << "\n";

    finalresult = resfilter(argvariables);

    (void) firstsum;

    std::string sumfilenamefull = logfile+"_LOO_sum";
    std::string cntfilenamefull = logfile+"_LOO_cnt";
    std::string cfmfilenamefull = logfile+"_LOO_cfm";
    std::string clrfilenamefull = logfile+"_LOO_clr";

    writeLog(accsum,sumfilenamefull,getsetExtVar);
    writeLog(cnt   ,cntfilenamefull,getsetExtVar);
    writeLog(cfm   ,cfmfilenamefull,getsetExtVar);
    writeLog(clr   ,clrfilenamefull,getsetExtVar);

    return;
}

void testRecall(std::string &logfile, const ML_Mutable &svmbase, int &firstsum, gentype &finalresult, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int logres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num), int useThreads)
{
    Vector<int> cnt;
    Matrix<int> cfm;
    double res = 0.0;
    int i;
    Vector<gentype> resh;
    Vector<gentype> resg;
    Vector<gentype> gvarres;

    res = calcRecall(svmbase,cnt,cfm,resh,resg,gvarres,0,recordxvar,useThreads);

    std::string resfilenamefull = logfile+"_recall_res";
    std::string tarfilenamefull = logfile+"_recall_tar";
    std::string clafilenamefull = logfile+"_recall_cla";
    std::string varfilenamefull = logfile+"_recall_var";

    if ( logres )
    {
        writeLog(resg,resfilenamefull,getsetExtVar);
        writeLog(svmbase.y(),tarfilenamefull,getsetExtVar);
        writeLog(resh,clafilenamefull,getsetExtVar);
    }

    if ( recordxvar )
    {
        writeLog(gvarres,varfilenamefull,getsetExtVar);
    }

    if ( recordres )
    {
        argvariables("&",1)("&",5) = resh;
        argvariables("&",1)("&",6) = resg;
    }

    errstream() << "\n";

    Vector<double> accsum(7);

    // "Binary classifier" include anomaly, isClassifier does not

    if ( isBinaryClassify(svmbase) || svmbase.isClassifier() )
    {
        measureAccuracy(accsum,resg,resh,svmbase.y(),svmbase.d(),svmbase);
    }

    else
    {
        accsum = -1.0;

        accsum("&",0) = 1/(res+1e-6);
        accsum("&",5)         = svmbase.sparlvl();
        accsum("&",6)         = res;
    }

    errstream() << "Accuracy:  " << accsum(0) << "\n";
    errstream() << "Precision: " << accsum(1) << "\n";
    errstream() << "Recall:    " << accsum(2) << "\n";
    errstream() << "F1 Score:  " << accsum(3) << "\n";
    errstream() << "AUC:       " << accsum(4) << "\n";
    errstream() << "Sparsity:  " << accsum(5) << "\n";
    errstream() << "Error:     " << accsum(6) << "\n";

    argvariables("&",1)("&",37) = accsum(0);
    argvariables("&",1)("&",38) = accsum(1);
    argvariables("&",1)("&",39) = accsum(2);
    argvariables("&",1)("&",40) = accsum(3);
    argvariables("&",1)("&",41) = accsum(4);
    argvariables("&",1)("&",42) = accsum(5);

    argvariables("&",1)("&",2) = res;
    argvariables("&",1)("&",3) = cnt;
    argvariables("&",1)("&",4) = cfm;

    argvariables("&",1)("&",44).force_vector(cnt.size());

    for ( i = 0 ; i < cnt.size() ; ++i )
    {
        ((argvariables("&",1)("&",44)).dir_vector())("&",i) = cnt(i) ? ( 1.0 - (((double) cfm(i,i))/((double) cnt(i))) ) : 0.0;
    }

    Vector<gentype> &clr = (argvariables("&",1)("&",44)).dir_vector();

    errstream() << "Recall error: "    << res << "\n";
    errstream() << "Class counts: "    << cnt << "\n";
    errstream() << "Confusion: "       << cfm << "\n";
    errstream() << "Classwise error: " << clr << "\n";

    finalresult = resfilter(argvariables);

    (void) firstsum;

    std::string sumfilenamefull = logfile+"_recall_sum";
    std::string cntfilenamefull = logfile+"_recall_cnt";
    std::string cfmfilenamefull = logfile+"_recall_cfm";
    std::string clrfilenamefull = logfile+"_recall_clr";

    writeLog(accsum,sumfilenamefull,getsetExtVar);
    writeLog(cnt   ,cntfilenamefull,getsetExtVar);
    writeLog(cfm   ,cfmfilenamefull,getsetExtVar);
    writeLog(clr   ,clrfilenamefull,getsetExtVar);

    return;
}

void testCross(std::string &logfile, const ML_Mutable &svmbase, int &firstsum, int numreps, int startpoint, int randcross, int numfolds, gentype &finalresult, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int logres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num), int useThreads)
{
    Vector<double> repres;
    Vector<double> cnt;
    Matrix<double> cfm;
    double res = 0.0;
    int i,j;
    Vector<Vector<gentype> > resh;
    Vector<Vector<gentype> > resg;
    Vector<Vector<gentype> > gvarres;

    res = calcCross(svmbase,numfolds,randcross,repres,cnt,cfm,resh,resg,gvarres,numreps,startpoint,recordxvar,0,useThreads);

    for ( j = 0 ; j < numreps ; ++j )
    {
        std::ostringstream whichrep;

        whichrep << j;

        std::string resfilenamefull = logfile+"_nfoldcross_res"+whichrep.str();
        std::string tarfilenamefull = logfile+"_nfoldcross_tar"+whichrep.str();
        std::string clafilenamefull = logfile+"_nfoldcross_cla"+whichrep.str();
        std::string varfilenamefull = logfile+"_nfoldcross_var"+whichrep.str();

        if ( logres )
        {
            writeLog(resg(j),resfilenamefull,getsetExtVar);
            writeLog(svmbase.y(),tarfilenamefull,getsetExtVar);
            writeLog(resh(j),clafilenamefull,getsetExtVar);
        }

        if ( recordxvar )
        {
            writeLog(gvarres(j),varfilenamefull,getsetExtVar);
        }
    }

    if ( recordres )
    {
        argvariables("&",1)("&",5) = resh;
        argvariables("&",1)("&",6) = resg;
    }

    if ( randcross )
    {
        errstream() << "Random ";
    }

    if ( numreps != 1 )
    {
        errstream() << numreps << "-repeat ";
    }

    errstream() << "\n";

    Vector<double> accsum(7);

    double meanerror = 0.0;
    double meansqerror = 0.0;
    double varerror = 0.0;

    // "Binary classifier" include anomaly, isClassifier does not

    if ( isBinaryClassify(svmbase) || svmbase.isClassifier() )
    {
        Vector<double> accsumtmp(7);

        accsum = 0.0;

        for ( i = 0 ; i < resg.size() ; ++i )
        {
            measureAccuracy(accsumtmp,resg(i),resh(i),svmbase.y(),svmbase.d(),svmbase);

            accsum += accsumtmp;

            meanerror += accsumtmp(6);
            meansqerror += (accsumtmp(6)*accsumtmp(6));
        }

        accsum /= (double) resg.size();

        meanerror   /= (double) resg.size();
        meansqerror /= (double) resg.size();

        varerror = meansqerror-(meanerror*meanerror);
    }

    else
    {
        accsum = -1.0;

        accsum("&",0) = 1/(res+1e-6);
        accsum("&",5)         = svmbase.sparlvl();
        accsum("&",6)         = res;

        varerror = 0.0;
    }

    errstream() << "Accuracy:   " << accsum(0) << "\n";
    errstream() << "Precision:  " << accsum(1) << "\n";
    errstream() << "Recall:     " << accsum(2) << "\n";
    errstream() << "F1 Score:   " << accsum(3) << "\n";
    errstream() << "AUC:        " << accsum(4) << "\n";
    errstream() << "Sparsity:   " << accsum(5) << "\n";
    errstream() << "Error:      " << accsum(6) << "\n";
    errstream() << "var(Error): " << varerror  << "\n";

    argvariables("&",1)("&",37) = accsum(0);
    argvariables("&",1)("&",38) = accsum(1);
    argvariables("&",1)("&",39) = accsum(2);
    argvariables("&",1)("&",40) = accsum(3);
    argvariables("&",1)("&",41) = accsum(4);
    argvariables("&",1)("&",42) = accsum(5);
    argvariables("&",1)("&",46) = varerror;

    argvariables("&",1)("&",2) = res;
    argvariables("&",1)("&",3) = cnt;
    argvariables("&",1)("&",4) = cfm;

    argvariables("&",1)("&",44).force_vector(cnt.size());

    for ( i = 0 ; i < cnt.size() ; ++i )
    {
        ((argvariables("&",1)("&",44)).dir_vector())("&",i) = cnt(i) ? ( 1.0 - (((double) cfm(i,i))/((double) cnt(i))) ) : 0.0;
    }

    Vector<gentype> &clr = (argvariables("&",1)("&",44)).dir_vector();

    errstream() << numfolds << "-fold error: " << res << "\n";
    errstream() << "Class counts: "    << cnt << "\n";
    errstream() << "Confusion: "       << cfm << "\n";
    errstream() << "Classwise error: " << clr << "\n";

    finalresult = resfilter(argvariables);

    (void) firstsum;

    std::string sumfilenamefull = logfile+"_nfoldcross_sum";
    std::string cntfilenamefull = logfile+"_nfoldcross_cnt";
    std::string cfmfilenamefull = logfile+"_nfoldcross_cfm";
    std::string clrfilenamefull = logfile+"_nfoldcross_clr";
    std::string rprfilenamefull = logfile+"_nfoldcross_rpr";

    writeLog(accsum,sumfilenamefull,getsetExtVar);
    writeLog(cnt   ,cntfilenamefull,getsetExtVar);
    writeLog(cfm   ,cfmfilenamefull,getsetExtVar);
    writeLog(clr   ,clrfilenamefull,getsetExtVar);
    writeLog(repres,rprfilenamefull,getsetExtVar);

    return;
}

void testSparSens(std::string &logfile, const ML_Mutable &svmbase, int &firstsum, int minbad, int maxbad, double noisemean, double noisevar, int startpoint, gentype &finalresult, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int logres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num), int useThreads)
{
    Vector<double> repres;
    Vector<double> cnt;
    Matrix<double> cfm;
    double res = 0.0;
    int i,j;
    int numreps = maxbad-minbad+1;
    Vector<Vector<gentype> > resh;
    Vector<Vector<gentype> > resg;
    Vector<Vector<gentype> > gvarres;

    res = calcSparSens(svmbase,repres,cnt,cfm,resh,resg,gvarres,minbad,maxbad,noisemean,noisevar,startpoint,recordxvar,useThreads);

    for ( j = 0 ; j < numreps ; ++j )
    {
        std::ostringstream whichrep;

        whichrep << j;

        std::string resfilenamefull = logfile+"_sparsens_res"+whichrep.str();
        std::string tarfilenamefull = logfile+"_sparsens_tar"+whichrep.str();
        std::string clafilenamefull = logfile+"_sparsens_cla"+whichrep.str();
        std::string varfilenamefull = logfile+"_sparsens_var"+whichrep.str();

        if ( logres )
        {
            writeLog(resg(j),resfilenamefull,getsetExtVar);
            writeLog(svmbase.y(),tarfilenamefull,getsetExtVar);
            writeLog(resh(j),clafilenamefull,getsetExtVar);
        }

        if ( recordxvar )
        {
            writeLog(gvarres(j),varfilenamefull,getsetExtVar);
        }
    }

    if ( recordres )
    {
        argvariables("&",1)("&",5) = resh;
        argvariables("&",1)("&",6) = resg;
    }

    errstream() << "Noise-Vector Test\n";

    Vector<double> accsum(7);

    double meanerror = 0.0;
    double meansqerror = 0.0;
    double varerror = 0.0;

    // "Binary classifier" include anomaly, isClassifier does not

    if ( isBinaryClassify(svmbase) || svmbase.isClassifier() )
    {
        Vector<double> accsumtmp(7);

        accsum = 0.0;

        for ( i = 0 ; i < resg.size() ; ++i )
        {
            measureAccuracy(accsumtmp,resg(i),resh(i),svmbase.y(),svmbase.d(),svmbase);

            accsum += accsumtmp;

            meanerror += accsumtmp(6);
            meansqerror += (accsumtmp(6)*accsumtmp(6));
        }

        accsum /= (double) resg.size();

        meanerror   /= (double) resg.size();
        meansqerror /= (double) resg.size();

        varerror = meansqerror-(meanerror*meanerror);
    }

    else
    {
        accsum = -1.0;

        accsum("&",0) = 1/(res+1e-6);
        accsum("&",5)         = svmbase.sparlvl();
        accsum("&",6)         = res;

        varerror = 0.0;
    }

    errstream() << "Error over n: " << repres << "\n";
    errstream() << "Accuracy:   " << accsum(0) << "\n";
    errstream() << "Precision:  " << accsum(1) << "\n";
    errstream() << "Recall:     " << accsum(2) << "\n";
    errstream() << "F1 Score:   " << accsum(3) << "\n";
    errstream() << "AUC:        " << accsum(4) << "\n";
    errstream() << "Sparsity:   " << accsum(5) << "\n";
    errstream() << "Error:      " << accsum(6) << "\n";
    errstream() << "var(Error): " << varerror  << "\n";

    argvariables("&",1)("&",37) = accsum(0);
    argvariables("&",1)("&",38) = accsum(1);
    argvariables("&",1)("&",39) = accsum(2);
    argvariables("&",1)("&",40) = accsum(3);
    argvariables("&",1)("&",41) = accsum(4);
    argvariables("&",1)("&",42) = accsum(5);
    argvariables("&",1)("&",46) = varerror;

//    argvariables("&",1)("&",2) = res;
    argvariables("&",1)("&",2) = repres;
    argvariables("&",1)("&",3) = cnt;
    argvariables("&",1)("&",4) = cfm;

    argvariables("&",1)("&",44).force_vector(cnt.size());

    for ( i = 0 ; i < cnt.size() ; ++i )
    {
        ((argvariables("&",1)("&",44)).dir_vector())("&",i) = cnt(i) ? ( 1.0 - (((double) cfm(i,i))/((double) cnt(i))) ) : 0.0;
    }

    Vector<gentype> &clr = (argvariables("&",1)("&",44)).dir_vector();

    errstream() << "Noise Vector error: "     << repres << "\n";
    errstream() << "Noise Vector error ave: " << res << "\n";
    errstream() << "Class counts: "           << cnt << "\n";
    errstream() << "Confusion: "              << cfm << "\n";
    errstream() << "Classwise error: "        << clr << "\n";

    finalresult = resfilter(argvariables);

    (void) firstsum;

    std::string sumfilenamefull = logfile+"_sparsens_sum";
    std::string cntfilenamefull = logfile+"_sparsens_cnt";
    std::string cfmfilenamefull = logfile+"_sparsens_cfm";
    std::string clrfilenamefull = logfile+"_sparsens_clr";
    std::string rprfilenamefull = logfile+"_sparsens_rpr";

    writeLog(accsum,sumfilenamefull,getsetExtVar);
    writeLog(cnt   ,cntfilenamefull,getsetExtVar);
    writeLog(cfm   ,cfmfilenamefull,getsetExtVar);
    writeLog(clr   ,clrfilenamefull,getsetExtVar);
    writeLog(repres,rprfilenamefull,getsetExtVar);

    return;
}

void testFileVectors(int binaryRelabel, int singleDrop, std::string &logfile, const ML_Mutable &svmbase, std::string &tfilename, int reverse, int ignoreStart, int imax, int &firstsum, int coercetosingle, int coercefromsingle, const gentype &fromsingletarget, gentype &finalresult, int uselinesvector, Vector<int> &linesread, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int recordres, int logres, int recordxvar, int (*getsetExtVar)(gentype &res, const gentype &src, int num), const SparseVector<gentype> &xtemplate)
{
    int pointsadded;

    Vector<gentype> ytest;
    Vector<gentype> ytestresh;
    Vector<gentype> ytestresg;
    Vector<gentype> yvarres;
    Vector<int> outkernind;

    // File may be very large.  Do not want to keep the whole lot in memory
    // at once.  So rather than load first, then test, we do a load-and-test
    // operation that does not keep x in memory.

    pointsadded = loadFileAndTest(svmbase,xtemplate,tfilename,reverse,ignoreStart,imax,-1,coercetosingle,coercefromsingle,fromsingletarget,binaryRelabel,singleDrop,uselinesvector,linesread,ytest,ytestresh,ytestresg,yvarres,recordxvar,outkernind);

    int i;
    Vector<int> cnt;
    Matrix<int> cfm;

    double res = assessResult(svmbase,cnt,cfm,ytestresh,ytest,outkernind);

    std::string resfilenamefull = logfile+"_"+tfilename+"_res";
    std::string tarfilenamefull = logfile+"_"+tfilename+"_tar";
    std::string clafilenamefull = logfile+"_"+tfilename+"_cla";
    std::string varfilenamefull = logfile+"_"+tfilename+"_var";

    if ( logres )
    {
        writeLog(ytestresg,resfilenamefull,getsetExtVar);
        writeLog(ytest,tarfilenamefull,getsetExtVar);
        writeLog(ytestresh,clafilenamefull,getsetExtVar);
    }

    if ( recordxvar )
    {
        writeLog(yvarres,varfilenamefull,getsetExtVar);
    }

    (void) pointsadded;

    if ( recordres )
    {
        argvariables("&",1)("&",5) = ytestresh;
        argvariables("&",1)("&",6) = ytestresg;
    }

    errstream() << "\n";

    Vector<double> accsum(7);

    // "Binary classifier" include anomaly, isClassifier does not

    if ( isBinaryClassify(svmbase) || svmbase.isClassifier() )
    {
        retVector<int> tmpva;

        measureAccuracy(accsum,ytestresg,ytestresh,ytest,oneintvec(ytestresg.size(),tmpva),svmbase);
    }

    else
    {
        accsum = -1;

        accsum("&",0) = 1/(res+1e-6);
        accsum("&",5)         = svmbase.sparlvl();
        accsum("&",6)         = res;
    }

    errstream() << "Accuracy:  " << accsum(0) << "\n";
    errstream() << "Precision: " << accsum(1) << "\n";
    errstream() << "Recall:    " << accsum(2) << "\n";
    errstream() << "F1 Score:  " << accsum(3) << "\n";
    errstream() << "AUC:       " << accsum(4) << "\n";
    errstream() << "Sparsity:  " << accsum(5) << "\n";
    errstream() << "Error:     " << accsum(6) << "\n";

    argvariables("&",1)("&",37) = accsum(0);
    argvariables("&",1)("&",38) = accsum(1);
    argvariables("&",1)("&",39) = accsum(2);
    argvariables("&",1)("&",40) = accsum(3);
    argvariables("&",1)("&",41) = accsum(4);
    argvariables("&",1)("&",42) = accsum(5);

    argvariables("&",1)("&",2) = res;
    argvariables("&",1)("&",3) = cnt;
    argvariables("&",1)("&",4) = cfm;

    argvariables("&",1)("&",44).force_vector(cnt.size());

    for ( i = 0 ; i < cnt.size() ; ++i )
    {
        ((argvariables("&",1)("&",44)).dir_vector())("&",i) = cnt(i) ? ( 1.0 - (((double) cfm(i,i))/((double) cnt(i))) ) : 0.0;
    }

    Vector<gentype> &clr = (argvariables("&",1)("&",44)).dir_vector();

    errstream() << tfilename << "-test error: "    << res << "\n";
    errstream() << "Class counts: "    << cnt << "\n";
    errstream() << "Confusion: "       << cfm << "\n";
    errstream() << "Classwise error: " << clr << "\n";

    finalresult = resfilter(argvariables);

    (void) firstsum;

    std::string sumfilenamefull = logfile+"_"+tfilename+"_sum";
    std::string cntfilenamefull = logfile+"_"+tfilename+"_cnt";
    std::string cfmfilenamefull = logfile+"_"+tfilename+"_cfm";
    std::string clrfilenamefull = logfile+"_"+tfilename+"_clr";

    writeLog(accsum,sumfilenamefull,getsetExtVar);
    writeLog(cnt   ,cntfilenamefull,getsetExtVar);
    writeLog(cfm   ,cfmfilenamefull,getsetExtVar);
    writeLog(clr   ,clrfilenamefull,getsetExtVar);

    return;
}

void testnegloglike(std::string &logfile, const ML_Mutable &svmbase, int &firstsum, gentype &finalresult, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int (*getsetExtVar)(gentype &res, const gentype &src, int num), int useThreads)
{
    (void) getsetExtVar;
    (void) logfile;
    (void) useThreads;

    double res = 0.0;

    res = calcnegloglikelihood(svmbase.getMLconst());

    errstream() << "\n";

    argvariables("&",1)("&",2) = res;

    errstream() << "log likelihood: " << res << "\n";

    finalresult = resfilter(argvariables);

    (void) firstsum;

    return;
}

void testmaxinfogain(std::string &logfile, const ML_Mutable &svmbase, int &firstsum, gentype &finalresult, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int (*getsetExtVar)(gentype &res, const gentype &src, int num), int useThreads)
{
    (void) getsetExtVar;
    (void) logfile;
    (void) useThreads;

    double res = 0.0;

    res = calcmaxinfogain(svmbase.getMLconst());

    errstream() << "\n";

    argvariables("&",1)("&",2) = res;

    errstream() << "max information gain: " << res << "\n";

    finalresult = resfilter(argvariables);

    (void) firstsum;

    return;
}

void testRKHSnorm(std::string &logfile, const ML_Mutable &svmbase, int &firstsum, gentype &finalresult, const gentype &resfilter, SparseVector<SparseVector<gentype> > &argvariables, int (*getsetExtVar)(gentype &res, const gentype &src, int num), int useThreads)
{
    (void) getsetExtVar;
    (void) logfile;
    (void) useThreads;

    double res = 0.0;

    res = calcRKHSnorm(svmbase.getMLconst());

    errstream() << "\n";

    argvariables("&",1)("&",2) = res;

    errstream() << "RKHSnorm: " << res << "\n";

    finalresult = resfilter(argvariables);

    (void) firstsum;

    return;
}















// isscalar = nz if scalar
// isvector = nz if vector
// ispy     = 0 if exe
//          = 1 if python
// evalname = python/exe to be called
// sf       = scalar or function to be evaluated
// v        = vector

int pyorexeeval(int isscalar, int isvector, int ispy, const std::string &evalname, const gentype &sf, const Vector<gentype> &v, gentype &finalresult)
{
(void) isvector;
    NiceAssert( !isscalar || !isvector );

    //if ( ( ispy == 2 ) || ( ispy == 3 ) )
    //{
        remove("pyres.txt");

        std::stringstream evalstream;

        if ( isscalar )
        {
            evalstream << evalname;
            evalstream << " ";
            evalstream << sf;
            evalstream << " > pyres.txt";
        }

        else
        {
            evalstream << evalname;
            evalstream << " ";
            evalstream << v;
            evalstream << " > pyres.txt";
        }

        std::string evalstr(evalstream.str());

        if ( ispy )
        {
            // ; is treated as a "special" character and messes up python!

            for ( int i = 0 ; i < (int) evalstr.size() ; i++ )
            {
                if ( evalstr[i] == ';' )
                {
                    evalstr[i] = ':';
                }
            }

            errstream() << "Python call string: " << evalstr << "\n";

            svm_pycall(evalstr,false);
        }

        else
        {
            errstream() << "Executable call string: " << evalstr << "\n";

            svm_execall(evalstr,false);
        }

//        svm_sleep(1);

        std::ifstream resfile("pyres.txt");

        if ( resfile.is_open() )
        {
            resfile >> finalresult;
        }

        else
        {
            finalresult.makeError("No result written");
        }

        resfile.close();

        return 0;
    //}
}






















































int loadDataFromMatlab(const std::string &xmatname, const std::string &ymatname, Vector<SparseVector<gentype> > &x, Vector<gentype> &dz, char targtype, int (*getsetExtVar)(gentype &res, const gentype &src, int num))
{
    gentype Xmat;
    gentype ymat;

    Xmat.makeString(xmatname);
    ymat.makeString(ymatname);

    gentype matind;

    matind.force_matrix();

    getsetExtVar(Xmat,matind,-1);
    getsetExtVar(ymat,matind,-1);

    NiceAssert( Xmat.numRows() == ymat.numRows() );

    int N = Xmat.numRows();

    int dimx = Xmat.numCols();
    int dimy = ymat.numCols();

    dz.resize(N);
    x.resize(N);

    gentype temp;

    int i,j;

    for ( i = 0 ; i < N ; ++i )
    {
        x("&",i).zero();

        for ( j = 0 ; j < dimx ; ++j )
        {
            x("&",i)("&",j) = (Xmat.dir_matrix())(i,j);
        }

        switch ( targtype )
        {
            case 'B':
            {
                dz("&",i).force_null();

                break;
            }

            case 'Z':
            {
                NiceAssert( dimy == 1 );

                dz("&",i).force_int() = (int) (ymat.force_matrix())(i,0);

                break;
            }

            case 'V':
            {
                if ( dimy > 0 )
                {
                    dz("&",i).force_vector().resize(dimy);

                    for ( j = 0 ; j < dimy ; ++j )
                    {
                        dz("&",i)("&",j) = (ymat.force_matrix())(i,j);
                    }
                }

                break;
            }

            default:
            {
                NiceAssert( dimy == 1 );

                dz("&",i).force_double() = (ymat.force_matrix())(i,0);

                break;
            }
        }
    }

    return N;
}























void printhelp(std::ostream &output, int basic, int advanced)
{
    output << ( ( basic || advanced ) ? "SVMheavy 8.0: an SVM (and more) implementation by Alistair Shilton.           \n" : "" );
    output << ( ( basic || advanced ) ? "============                                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Copyright: all rights reserved.                                               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Usage:     svmheavyv7 {options}                                               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Each line  in the  training/testing files  contains a  single sample  with the\n" : "" );
    output << ( ( basic || advanced ) ? "following general format (where  values in {} are  optional; and != and  ! are\n" : "" );
    output << ( ( basic || advanced ) ? "functionally equivalent):                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "{>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} {sVAL} {SVAL} x                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Classification:                                                               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {>,=,!=,!,<} must not be included.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "       - y must be included.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - y=-1,0,+1,+2,... is classification (0 for test/unknown).             \n" : "" );
    output << ( ( basic || advanced ) ? "       - {tVAL} or {TVAL} sets the empirical risk scale.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - {eVAL} or {EVAL} sets the distance to surface scale.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - {sVAL} or {SVAL} sets the empirical sigma scale.                     \n" : "" );
    output << ( ( basic || advanced ) ? "       - x is the training vector.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Regression:                                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {>,=,!=,!,<} defines constraint type (optional, default is =):       \n" : "" );
    output << ( ( basic || advanced ) ? "         o >: g(x) > y                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         o =: g(x) = y                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         o <: g(x) < y                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         o ! or !=: ignore this vector (it may be referenced elsewhere).      \n" : "" );
    output << ( ( basic || advanced ) ? "       - y must be included.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - y gives the target value for this point.                             \n" : "" );
    output << ( ( basic || advanced ) ? "       - if y is anion, vector or gentype then >,< not defined.               \n" : "" );
    output << ( ( basic || advanced ) ? "       - if y is gentype then it must lie in the basis set u_i.               \n" : "" );
    output << ( ( basic || advanced ) ? "       - {tVAL} or {TVAL} sets the empirical risk scale.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - {eVAL} or {EVAL} sets the epsilon insensitivity scale.               \n" : "" );
    output << ( ( basic || advanced ) ? "       - x is the training vector.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "       - For  some SVM,  GP and  LS-SVM  models you  can use  negative epsilon\n" : "" );
    output << ( ( basic || advanced ) ? "         scales to indicate that g(x)  lies *outside* of [y-e,y+e] - that  is,\n" : "" );
    output << ( ( basic || advanced ) ? "         g(x) in (-inf,y-e] cup [y+e,inf).                                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Similarity learning:                                                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Format as for  regression, except that x takes  the form xa ~ xb (the\n" : "" );
    output << ( ( basic || advanced ) ? "         character ~ acts as a separator), so the constraint is on g({xa,xb}).\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Multi-instance learning:                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Format as for regression,  but in this case x has  the general format\n" : "" );
    output << ( ( basic || advanced ) ? "         xa ~ xb ~ ... ~ xn  (the   character  ~  acts  as  a  separator),  so\n" : "" );
    output << ( ( basic || advanced ) ? "         constraint is on g({xa,xb,...,xn}).                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Single class:                                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {>,=,!=,!,<} must not be included.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "       - y must not be included.                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {tVAL} or {TVAL} sets the empirical risk scale.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - {eVAL} or {EVAL} sets the distance to surface scale.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - x is the training vector.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Classification with Scoring:                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {>,=,!=,!,<} must not be included.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "       - y must be included.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - y = integer indicates a standard class membership constraint.        \n" : "" );
    output << ( ( basic || advanced ) ? "       - y = vector indicates a score.  Each element of the vector indicates a\n" : "" );
    output << ( ( basic || advanced ) ? "         score on a different \"axis\",  where each axis is used to derive a set\n" : "" );
    output << ( ( basic || advanced ) ? "         of rank constraints g(x_i) - g(x_j) > 1. Use null as a placeholder if\n" : "" );
    output << ( ( basic || advanced ) ? "         the x is not scored on a given axis.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - {tVAL} or {TVAL} sets the empirical risk scale.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - {eVAL} or {EVAL} sets the distance to surface scale.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - x is the training vector.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Regression with Scoring:                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {>,=,!=,!,<} may be included for scalar constraints.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - {=,!} only for scoring constraints.                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - y must be included.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - y = scalar indicates a standard constraint g(x) ? y.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - y = vector indicates a score as per classification with scoring.     \n" : "" );
    output << ( ( basic || advanced ) ? "       - {tVAL} or {TVAL} sets the empirical risk scale.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - {eVAL} or {EVAL} sets the distance to surface scale.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - x is the training vector.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Gentype regression:                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {>,=,!=,!,<} must not be included.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "       - y must be included.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - y can be real, vector, anion, string, equation, set or graph (as long\n" : "" );
    output << ( ( basic || advanced ) ? "         as the concept of (inner) product (possibly numeric) can be defined).\n" : "" );
    output << ( ( basic || advanced ) ? "       - y is projected onto the \"u\" basis (see -Aby etc below).              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {tVAL} or {TVAL} sets the empirical risk scale.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - {eVAL} or {EVAL} sets the distance to surface scale.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - x is the training vector.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Planar regression and multi-task:                                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - {>,=,!=,!,<} defines constraint type:                                \n" : "" );
    output << ( ( basic || advanced ) ? "         o >: v'.g(x) > y                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "         o =: v'.g(x) = y                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "         o <: v'.g(x) < y                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "         o ! or !=: ignore this vector (it may be referenced elsewhere).      \n" : "" );
    output << ( ( basic || advanced ) ? "       - y must be included.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - y is the scalar-valued target.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "       - {tVAL} or {TVAL} sets the empirical risk scale.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - {eVAL} or {EVAL} sets the distance to surface scale.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - x is the training vector of form x :::: 7:v. v may either be a vector\n" : "" );
    output << ( ( basic || advanced ) ? "         or an integer index to an (output) basis vector u_i. If v is a vector\n" : "" );
    output << ( ( basic || advanced ) ? "         then it must lie in the space spanned by the output basis vectors.   \n" : "" );
//    output << ( ( basic || advanced ) ? "       - when used with  planar MLs, g(x) will be  a vector and  g(x :::: 7:v)\n" : "" );
//    output << ( ( basic || advanced ) ? "         a scalar.                                                            \n" : "" );
//    output << ( ( basic || advanced ) ? "       - when  used with  non-planar MLs,  the  v  part will  be ignored  when\n" : "" );
//    output << ( ( basic || advanced ) ? "         evaluating K  unless it  is present  on all  arguments, so  g(x) will\n" : "" );
//    output << ( ( basic || advanced ) ? "         evaluate as a scalar also. In this case v = [ 0 0 ... 0 1 0 0 ... 0 ]\n" : "" );
//    output << ( ( basic || advanced ) ? "         is used to select between tasks,  making Gp block diagonal, so kernel\n" : "" );
//    output << ( ( basic || advanced ) ? "         tuning, features etc affects all  tasks but otherwise they don't tend\n" : "" );
//    output << ( ( basic || advanced ) ? "         to interfere (depending on model, obviously).                        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Multi-expert ranking:                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Like planar  regression but automatically  tunes basis u_i.  The idea\n" : "" );
    output << ( ( basic || advanced ) ? "         is  that  many   \"experts\"  give  their  opinions   (usually  ranking\n" : "" );
    output << ( ( basic || advanced ) ? "         constraints, but not necessarily)  and we want to construct a machine\n" : "" );
    output << ( ( basic || advanced ) ? "         that synthesises these (not  always compatible) sources.  Each expert\n" : "" );
    output << ( ( basic || advanced ) ? "         is assigned to a  particular basis vector u_i, and  the inner product\n" : "" );
    output << ( ( basic || advanced ) ? "         between two such vectors <u_i,u_j> reflects how similar experts i and\n" : "" );
    output << ( ( basic || advanced ) ? "         j are in  their assessments.  The  machine attempts to  automatically\n" : "" );
    output << ( ( basic || advanced ) ? "         tune these basis vectors - that is,  to learn expert similarity - and\n" : "" );
    output << ( ( basic || advanced ) ? "         thereby  combine them  (essentially transfer  learning).   Evaluating\n" : "" );
    output << ( ( basic || advanced ) ? "         g can  either define  which  \"expert\"  x is  aligned with -  that is,\n" : "" );
    output << ( ( basic || advanced ) ? "         g(x :::: 7:i) for expert i - or give all alignments - that is, g(x). \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "The format of the x vector may be  either sparse or nonsparse.  Sparse vectors\n" : "" );
    output << ( ( basic || advanced ) ? "vectors have the form  (noting that commas can be  used instead of or combined\n" : "" );
    output << ( ( basic || advanced ) ? "with whitespace):                                                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "<feature1>:<valueF1> <feature2>:<valueF2> ... <featureN>:<valueFN>            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "with all other values being assumed zero.  Sparse vectors have the form:      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "<value1> <value2> ... <valueN>                                                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "The values may, depending  on context, be real, anionic,  vector, matrix, set,\n" : "" );
    output << ( ( basic || advanced ) ? "strings (encompassed in  quotes \", or a single non-numeric  character which is\n" : "" );
    output << ( ( basic || advanced ) ? "read as a string by default) or equations  (use of symbolic features may incur\n" : "" );
    output << ( ( basic || advanced ) ? "a significant performance hit).  Notes:                                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - The product of two vectors is the inner-product of the vectors.      \n" : "" );
    output << ( ( basic || advanced ) ? "       - The product of two strings is 1 if they are identical, 0 otherwise.  \n" : "" );
    output << ( ( basic || advanced ) ? "       - The sum of two strings is the concatenation of them.                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - The product of two sets is the number of elements in common.         \n" : "" );
    output << ( ( basic || advanced ) ? "       - The sum of two sets is the union, the difference the intersection.   \n" : "" );
    output << ( ( basic || advanced ) ? "       - Simple equations like sqrt(20) will be evaluated directly.           \n" : "" );
    output << ( ( basic || advanced ) ? "       - Distributions  (eg grand(0,1)) are  processed by the kernel  and then\n" : "" );
    output << ( ( basic || advanced ) ? "         sampled as per Muandet et al, Learning from Distributions via Support\n" : "" );
    output << ( ( basic || advanced ) ? "         Measure Machines (so a support  measure machine (SMM) can be run as a\n" : "" );
    output << ( ( basic || advanced ) ? "         standard SVM with distributions in the x vectors).                   \n" : "" );
    output << ( ( basic || advanced ) ? "       - Functions  are evaluated  like distributions,  where you  provide the\n" : "" );
    output << ( ( basic || advanced ) ? "         distribution of relevant variables to the kernel.                    \n" : "" );
    output << ( ( basic || advanced ) ? "       - Infinite-dimensional  vectors  (functions  treated  as  vectors)  are\n" : "" );
    output << ( ( basic || advanced ) ? "         written [[ : f : d ]],  where f is a function of  x (eg sin(x)) and d\n" : "" );
    output << ( ( basic || advanced ) ? "         d the dimension  of x.  Domain of  x is [0,1]^d.  These  can be  used\n" : "" );
    output << ( ( basic || advanced ) ? "         like vectors: norms,  inner products  etc work  exactly as  you would\n" : "" );
    output << ( ( basic || advanced ) ? "         expect in L2 space.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - RKHS vectors  can't be  entered directly,  but they  do occur  (eg in\n" : "" );
    output << ( ( basic || advanced ) ? "         Bayesian  optimisation).  Inner products  and norms on these occur in\n" : "" );
    output << ( ( basic || advanced ) ? "         RKHS space (not L2),  but mixed inner  products with  inf-dim vectors\n" : "" );
    output << ( ( basic || advanced ) ? "         are calculated in L2.  Note  that you can't  calculate sum/difference\n" : "" );
    output << ( ( basic || advanced ) ? "         between an RKHS vector and an inf-dim vector.                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - Scalar functions are  also evaluated.  These are function of the form\n" : "" );
    output << ( ( basic || advanced ) ? "         @(i,j,n):fn, where  fn is a  function, (i,j) defines  the variable in\n" : "" );
    output << ( ( basic || advanced ) ? "         the function being treated as a scalar.  The result of the product of\n" : "" );
    output << ( ( basic || advanced ) ? "         two scalar functions is the inner  product int_0^1 conj(f(x)).g(x) dx\n" : "" );
    output << ( ( basic || advanced ) ? "         (if i != 0 and/or  j !=0 then x  is replaced  by var(i,j)),  which is\n" : "" );
    output << ( ( basic || advanced ) ? "         numerically approximated over n steps.  The product of a vector and a\n" : "" );
    output << ( ( basic || advanced ) ? "         function  in this  context is  the  same,  assuming the  vector is  a\n" : "" );
    output << ( ( basic || advanced ) ? "         sampled version of the function on [0,1].  Default values for i,j and\n" : "" );
    output << ( ( basic || advanced ) ? "         n are 0,0 and 100, respectively.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "For example the following training sets all define variants of the classic XOR\n" : "" );
    output << ( ( basic || advanced ) ? "problem:                                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Standard (y x format):                                               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -1   -1 -1                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "         1    -1 1                                                            \n" : "" );
    output << ( ( basic || advanced ) ? "         1    1  -1                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "         -1   1  1                                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Vector x (y x format):                                               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    [ -1 1 ] [ -1 1 ]                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         1     [ -1 1 ] [ 1 -1 ]                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         1     [ 1 -1 ] [ -1 1 ]                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    [ 1 -1 ] [ 1 -1 ]                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Symbolic x (y x format):                                             \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    \"a\" \"a\"                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         1     \"a\" \"b\"                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         1     \"b\" \"a\"                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    \"b\" \"b\"                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Set x with symbolic elements (y x format):                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    { \"cat\" \"horse\" }     { \"car\" }                                \n" : "" );
    output << ( ( basic || advanced ) ? "         1     { \"cat\" \"chicken\" }   { \"lemons\" \"x\"  }                        \n" : "" );
    output << ( ( basic || advanced ) ? "         1     { \"wallaby\" \"mouse\" } { \"walrus\" \"car\" }                       \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    { \"mouse\" \"wombat\" }  { \"q\" \"lemons\" }                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Distribution x (y x format):                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    grand(-1,0.1) grand(-1,0.1)                                    \n" : "" );
    output << ( ( basic || advanced ) ? "         1     grand(-1,0.1) grand(1,0.1)                                     \n" : "" );
    output << ( ( basic || advanced ) ? "         1     grand(1,0.1)  grand(-1,0.1)                                    \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    grand(1,0.1)  grand(1,0.1)                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Scalar function x (y x format):                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    @():sin(2*pi()*x) @():sin(2*pi()*x)                            \n" : "" );
    output << ( ( basic || advanced ) ? "         1     @():sin(2*pi()*x) @():cos(2*pi()*x)                            \n" : "" );
    output << ( ( basic || advanced ) ? "         1     @():cos(2*pi()*x) @():sin(2*pi()*x)                            \n" : "" );
    output << ( ( basic || advanced ) ? "         -1    @():cos(2*pi()*x) @():cos(2*pi()*x)                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "More generally x has the form (using {...} to denote optional arguments here):\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "xa0 {~ xa1 {~ xa2 ...}} {: xb0 {~ xb1 {~ xb2 ...}}} {:: e} {:::: a}           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "where xai, xbi, e  and a  have  the  same  format  as x.  These  define  rank\n" : "" );
    output << ( ( basic || advanced ) ? "constraints, gradient constraints, tuple format and augmented data as follows:\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Gradient constraints: most models  allow the inclusion of  grad constraints of\n" : "" );
    output << ( ( basic || advanced ) ? "         the form (bias may need to be fixed to 0 for some models):           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         (e'.d/dx) g(x) ? y                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         making this a  constraint on the direction  derivative of the trained\n" : "" );
    output << ( ( basic || advanced ) ? "         machine.  In the training file these have the form:                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         {>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} x :: e                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         where e has the same format as x.   Constraints on higher derivatives\n" : "" );
    output << ( ( basic || advanced ) ? "         may also be applied using  augmented data (a_6) as described shortly.\n" : "" );
    output << ( ( basic || advanced ) ? "         Note that:                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - when evaluating the model g(x :: e) = (e'.d/dx) g(x).                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Rank constraints: some  models (SVMs, GPs,  LSVs) allow the  inclusion of rank\n" : "" );
    output << ( ( basic || advanced ) ? "         constraints - basically rather then enforcing g(x) ? y, they enforce:\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         g(xa) - g(xb) ? y      (by default, -krn 0)                          \n" : "" );
    output << ( ( basic || advanced ) ? "         g(xa) + g(xb) ? y      (alternative variant, -krn 1)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         (where ? depends on model). In the training file these have the form:\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         {>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} xa : xb                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         where xa and xb have the same format as x above.  Note that:         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - this enables ordinal regression.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "       - You can combine with gradients.  For example (assume -krn 0):        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         (ea'.d/dxa) g(xa) - g(xb) ? y                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         g(xa) - (eb'.d/dxb) g(xb) ? y                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         (ea'.d/dxa) g(xa) - (eb'.d/dxb) g(xb) ? y                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         correspond, respectively, to:                                        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         {>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} xa : xb :: ea           \n" : "" );
    output << ( ( basic || advanced ) ? "         {>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} xa : xb ::: eb          \n" : "" );
    output << ( ( basic || advanced ) ? "         {>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} xa : xb :: ea ::: eb    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - when evaluating model g(xa : xb) = g(xa) - g(xb) (by default -krn 0).\n" : "" );
    output << ( ( basic || advanced ) ? "         alternatively g(xa : xb) = g(xa) + g(xb) using -krn 1.               \n" : "" );
    output << ( ( basic || advanced ) ? "       - there are two other modes  available that don't  even define a latent\n" : "" );
    output << ( ( basic || advanced ) ? "         function  g(x), so  always require  g(xa : xb) in  both training  and\n" : "" );
    output << ( ( basic || advanced ) ? "         evaluation.  These are:                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "               -krn 2:  g(xa : xb) = -g(xb : xa) (but non-transisitve)        \n" : "" );
    output << ( ( basic || advanced ) ? "               -krn 3:  g(xa : xb) = +g(xb : xa)                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - use a_13:v to instead use v.g(xa) -   g(xb) ? y                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - use a_14:w to instead use   g(xa) - w.g(xb) ? y                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - use both to instead use   v.g(xa) - w.g(xb) ? y                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Multi-Instance  format:  most  models  can  be  used  for  multi-instance  and\n" : "" );
    output << ( ( basic || advanced ) ? "         similarity (kernel function)  learning.   In this case  vectors x are\n" : "" );
    output << ( ( basic || advanced ) ? "         replaced by  sets {xa0,xa1,...},  and g(x) becomes  g({xa0,xa1,...}).\n" : "" );
    output << ( ( basic || advanced ) ? "         In this case the training file has the form:                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         {>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} xa0 ~ xa1 ~ ...         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         Note that:                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - if you have a rank constraint use xa0 ~ xa1 ~ ... : xb0 ~ xb1 ~ .... \n" : "" );
    output << ( ( basic || advanced ) ? "       - the option -kn does not work with tuple format (but -knn does).      \n" : "" );
    output << ( ( basic || advanced ) ? "       - the max number of instances is 4096 (defined in sparsevector.h).     \n" : "" );
    output << ( ( basic || advanced ) ? "       - by default, gradients are wrt xa0.  You can change this with a_9.    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Multi-Task format: multi-task learning  via ICM kernels is  possible using the\n" : "" );
    output << ( ( basic || advanced ) ? "         same format  as multi-instance  learning.  See -XT  for  information.\n" : "" );
    output << ( ( basic || advanced ) ? "         Multi-task and multi-instance can be  combined, but interpretation is\n" : "" );
    output << ( ( basic || advanced ) ? "         non-trivial: see code for details.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Augmented data: in some cases additional data may be included using the form: \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         {>,=,!=,!,<} {y} {tVAL} {TVAL} {eVAL} {EVAL} x :::: a                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         where elements in a have the following interpretation:               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_0:  replace x with  vector x_{a_0}.  If this is a vector  then x is\n" : "" );
    output << ( ( basic || advanced ) ? "               replaced  by a tuple [ x_{a_0_0} ~ x_{a_0_1} .. ]. nul for dft.\n" : "" );
    output << ( ( basic || advanced ) ? "       - a_1:  replace xb with vector x_{a_1}.  If this is a vector then xb is\n" : "" );
    output << ( ( basic || advanced ) ? "               replaced  by a tuple [ x_{a_1_0} ~ x_{a_1_1} .. ]. nul for dft.\n" : "" );
    output << ( ( basic || advanced ) ? "       - a_2:  replace e with  vector x_{a_2}.  null for default.             \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_15: replace eb with vector x_{a_12}. null for default.             \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_3:  reserved (like a_7, but refers to gentype regr basis. nul dft).\n" : "" );
    output << ( ( basic || advanced ) ? "       - a_4:  diagonal kernel over-ride.  If  present in x1,x2 then kernel is\n" : "" );
    output << ( ( basic || advanced ) ? "               replaced by delta_{x_1(a_4), x_2(a_4)} (0 if only one,nul dft).\n" : "" );
    output << ( ( basic || advanced ) ? "       - a_19: include diagonal offset a_19 to kernel.                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_20: secondary nulling.  The kernel evaluated between x's with diff.\n" : "" );
    output << ( ( basic || advanced ) ? "               a_20 values will be 0.  null for default (which is same as 0). \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_5:  reserved (null for default).                                     \n" : "" );
//    output << ( ( basic || advanced ) ? "       - a_5:  interpret a_0  vector as  a  metrical  constraint - that  is, a\n" : "" );
//    output << ( ( basic || advanced ) ? "               training constraint on the tuple (x0,x1) taking the form:      \n" : "" );
//    output << ( ( basic || advanced ) ? "               g(x0,x0) + g(x1,x1) - 2g(x0,x1) ? y                            \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_6:  order of gradient constraint  (dflt 1 if unspecified). Gradient\n" : "" );
    output << ( ( basic || advanced ) ? "               constraints become (e'.dn/dxn) g(x) ? y, where:                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "               dn/dxn = (d/dx)^{\\opower n} (the kronecker power)              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "               For example if xdim = 2 then this is:                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "        d1/dx1 = [ d/dx0 ; d/dx1 ]                                            \n" : "" );
    output << ( ( basic || advanced ) ? "        d2/dx2 = [ d2/dx0.dx0 ; d2/dx0.dx1 ; d2/dx1.dx0 ; d2/dx1.dx1 ]        \n" : "" );
    output << ( ( basic || advanced ) ? "               = [ d2/dx0^2 ; d2/dx0.dx1 ; d2/dx0.dx1 ; d2/dx1^2 ]            \n" : "" );
    output << ( ( basic || advanced ) ? "        d3/dx3 = [ d3/dx0^3 ; d3/dx0^2.dx1 ; d3/dx0^2 dx1 ; d3/dx0.dx1^2 ;    \n" : "" );
    output << ( ( basic || advanced ) ? "                   d3/dx1.dx0^2 ; d3/dx1.dx0.dx1 ; d3/dx1.dx0.dx1 ; d3/dx1^3 ]\n" : "" );
    output << ( ( basic || advanced ) ? "               = [ d3/dx0^3 ; d3/dx0^2.dx1 ; d3/dx0^2 dx1 ; d3/dx0.dx1^2 ;    \n" : "" );
    output << ( ( basic || advanced ) ? "                   d3/dx0^2.dx1 ; d3/dx0.dx1^2 ; d3/dx0.dx1^2 ; d3/dx1^3 ]    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "               You can evaluate  g(x) without  specifying e  in some models to\n" : "" );
    output << ( ( basic || advanced ) ? "               find  the gradient  dn/dxn g(x)  (and likewise  (co)variances).\n" : "" );
    output << ( ( basic || advanced ) ? "               Null for default.                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_16: just like a_6, but for second term (xb) of rank constraint.    \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_7:  vector (or index relative  to basis if integer)  part of planar\n" : "" );
    output << ( ( basic || advanced ) ? "               constraint. nullptr for default (no planar part). This offers a\n" : "" );
    output << ( ( basic || advanced ) ? "               cheap way  to do transfer  learning using eg  TRF (that is, SVM\n" : "" );
    output << ( ( basic || advanced ) ? "               RFF with feature weight tuning).  For example, if the first and\n" : "" );
    output << ( ( basic || advanced ) ? "               second tasks have:                                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "               - first task:  x ::::  7:[ 1 0 0 ... ]                         \n" : "" );
    output << ( ( basic || advanced ) ? "               - second task: x ::::  7:[ 0 1 0 ... ]                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "               and so on, this will  ensure that Gp is in block-diagonal  form\n" : "" );
    output << ( ( basic || advanced ) ? "               so alpha  is decoupled  (but not  b), but  the feature  weights\n" : "" );
    output << ( ( basic || advanced ) ? "               (kernel) are coupled.  As the kernel is the same for each block\n" : "" );
    output << ( ( basic || advanced ) ? "               this generalises  to the  more general  case of  hyperparameter\n" : "" );
    output << ( ( basic || advanced ) ? "               tuning.  Note that planar regression also decouples the bias b.\n" : "" );
    output << ( ( basic || advanced ) ? "       - a_8:  set 1 to treat distributions as samples from sets, so distances\n" : "" );
    output << ( ( basic || advanced ) ? "               are to the nearest in set, and  inner products to most similar.\n" : "" );
    output << ( ( basic || advanced ) ? "               Null for default.  These are calculated stochastically.        \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_9:  by default gradient is wrt xa for multi-instance format.  Using\n" : "" );
    output << ( ( basic || advanced ) ? "               a_9 = i changes this to gradient  with respect to xa^ (the i-th\n" : "" );
    output << ( ( basic || advanced ) ? "               instance in set).  Note this does not hold for -kss.           \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_10: just like a_9, but for second term (xb) of rank constraint.    \n" : "" );
    output << ( ( basic || advanced ) ? "               a_10 = 1 changes this to gradient wrt xa, and a_10 = 2 for xb. \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_11: set 1 to replace K be the dense integral.  So K2(x,y) with a_11\n" : "" );
    output << ( ( basic || advanced ) ? "               set for x will be replaced with int_{x_0} int_{x_1}... K2(x,y).\n" : "" );
    output << ( ( basic || advanced ) ? "               This works for kernels 500-699, 3 and 54.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_12: set 1 to replace K be the dense derivatve. So K2(x,y) with a_11\n" : "" );
    output << ( ( basic || advanced ) ? "               set for x will be replaced with d/d{x_0} d/d{x_1}... K2(x,y).  \n" : "" );
    output << ( ( basic || advanced ) ? "               This works for kernels 400-599, 3 and 55.                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_13: left-side weight for rank constraint (see above, def 1 (null)).\n" : "" );
    output << ( ( basic || advanced ) ? "       - a_14: right-side weight for rank constrain (see above, def 1 (null)).\n" : "" );
    output << ( ( basic || advanced ) ? "    - a_17,18: using xa = [ :::: 17:[ ia ib ... ] 18:[ wa wb .. ] you replaces\n" : "" );
    output << ( ( basic || advanced ) ? "               K(xa,x) with wa.K(x_ia,x) + wb.K(x_ib,x) + ....                \n" : "" );
    output << ( ( basic || advanced ) ? "       - a_{100q}: for ranking constraints in the vector-target case {>,=,...}\n" : "" );
    output << ( ( basic || advanced ) ? "               applies to  all elements of the target.  To  override for dim q\n" : "" );
    output << ( ( basic || advanced ) ? "               for a given training pair use  this, specifically a_{100q} = -1\n" : "" );
    output << ( ( basic || advanced ) ? "               means <, 0 means !,+1 means > and +2 means =.                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Pipes: - Most output sent to standard error.                                  \n" : "" );
    output << ( (          advanced ) ? "       - Help sent to standard out.                                           \n" : "" );
    output << ( (          advanced ) ? "       - All other output sent direct to files.                               \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Matlab/mex: - Standard out and standard error redirected to mexprintf.        \n" : "" );
    output << ( (          advanced ) ? "       - Logfiles written and contents mirrored by matlab variables.          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Arguments: in the descriptions below there are three types of arguments:      \n" : "" );
    output << ( (          advanced ) ? "       - Strings:  indicated by  leading $  (eg $file,  $fn).  These  must not\n" : "" );
    output << ( (          advanced ) ? "         contain whitespace.                                                  \n" : "" );
    output << ( (          advanced ) ? "       - Sets: held in curly  brackets {} (eg {0,1,2}).  The  argument must be\n" : "" );
    output << ( (          advanced ) ? "         one of the options in the list.                                      \n" : "" );
    output << ( (          advanced ) ? "       - Variables: everything  else.  These  may be integers,  reals, vectors\n" : "" );
    output << ( (          advanced ) ? "         (eg [ 1 2 3.2 ] or [ 0:0.12 1:39 5:2 ]) or anions (eg 1.0i) depending\n" : "" );
    output << ( (          advanced ) ? "         on context.                                                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Variable evaluation: a useful feature here  is that all variable arguments are\n" : "" );
    output << ( (          advanced ) ? "         evaluated.  That is,  you can enter them as  equations - for example,\n" : "" );
    output << ( (          advanced ) ? "         if you enter  2/3 this will  be evaluated  to 0.66666 -  and moreover\n" : "" );
    output << ( (          advanced ) ? "         those equations can have arguments  of the form var(i,j), where i and\n" : "" );
    output << ( (          advanced ) ? "         j are  non-negative integers (and shortcuts x,y,z,v,w,g,u = var(0,0),\n" : "" );
    output << ( (          advanced ) ? "         var(0,1),...,var(0,5), u = var(1,1), h =var(42,42) (the current ML)).\n" : "" );
    output << ( (          advanced ) ? "         A complete list is  included (-??v).                                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Global functions: these  may be used to  retrieve data  about ML  models.  Use\n" : "" );
    output << ( (          advanced ) ? "         fnA(h,i) to  retrieve value i  (eg C,  if i = 0) for the  current ML,\n" : "" );
    output << ( (          advanced ) ? "         which is stored in variable h).  Replace h with specific ML number to\n" : "" );
    output << ( (          advanced ) ? "         specify a different ML.  A complete list is included (-??v).         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Variables: a complete  list of vars is  given at the end  of this help.  Three\n" : "" );
    output << ( (          advanced ) ? "         functions  are  particularly  note-worthy  with regard  to variables,\n" : "" );
    output << ( (          advanced ) ? "         namely -fV n $f  and -fW n f,  which let  you set  var(0,n) = f (with\n" : "" );
    output << ( (          advanced ) ? "         -fV being unevaluated  and -fW evaluated), and  -echo x, which simply\n" : "" );
    output << ( (          advanced ) ? "         evaluates x and echoes it to standard out.                           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Comments: comments  can be  included in  argument streams,  which is  handy if\n" : "" );
    output << ( (          advanced ) ? "         you want to put commands in a  file.  Comments are C style - that is,\n" : "" );
    output << ( (          advanced ) ? "         /* comment goes here and gets ignored */.  Note that comments may not\n" : "" );
    output << ( (          advanced ) ? "         interupt commands, so for example  the sequence \"-c 1 /* set c */\" is\n" : "" );
    output << ( (          advanced ) ? "         good, but \"-c /* set c */\" 1 is a syntax error.                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Help options (run when encountered):                                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -?              - basic help.                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         -??             - advanced help.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "         -??k            - list of all available kernel functions.            \n" : "" );
    output << ( ( basic || advanced ) ? "         -??v            - variable assignment table.                         \n" : "" );
    output << ( ( basic || advanced ) ? "         -??g            - variable types, functions etc.                     \n" : "" );
    output << ( ( basic || advanced ) ? "         -???            - print blank lines to standard error.               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "General options (run first):                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -v   {0,1}      - verbosity level (default 1).                       \n" : "" );
    output << ( ( basic || advanced ) ? "                           0 = minimal - cerr feedback only.                  \n" : "" );
    output << ( ( basic || advanced ) ? "                           1 = normal - write details to logfile.log.         \n" : "" );
    output << ( ( basic || advanced ) ? "         -L   $file      - string used to  derive logfile  and ML description.\n" : "" );
    output << ( ( basic || advanced ) ? "                           Files will be file.log, file.svm etc.              \n" : "" );
    output << ( (          advanced ) ? "         -LL  file       - string used to  derive logfile  and ML description.\n" : "" );
    output << ( (          advanced ) ? "                           Files will be file.log, file.svm etc.              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Zmute          - mute standard error logging.                       \n" : "" );
    output << ( ( basic || advanced ) ? "         -ZMute          - mute standard out logging.                         \n" : "" );
    output << ( ( basic || advanced ) ? "         -ZMUTE          - mute standard error and standard out logging.      \n" : "" );
    output << ( ( basic || advanced ) ? "         -Zunmute        - un-mute standard error logging.                    \n" : "" );
    output << ( ( basic || advanced ) ? "         -ZunMute        - un-mute standard out logging.                      \n" : "" );
    output << ( ( basic || advanced ) ? "         -ZunMUTE        - un-mute standard error and standard out logging.   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Multi-ML options (after general options):                                     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** SVMHeavy can work on  arbitrarily many MLs at **         \n" : "" );
    output << ( (          advanced ) ? "                  ** once.  Each  ML is assigned  an index n >= 1. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** The  working  ML  (default  index 1)  defines **         \n" : "" );
    output << ( (          advanced ) ? "                  ** which of  these is  operated on by  all other **         \n" : "" );
    output << ( (          advanced ) ? "                  ** commands.  Note that copying and swapping may **         \n" : "" );
    output << ( (          advanced ) ? "                  ** be very  slow for large MLs.  The  current ML **         \n" : "" );
    output << ( (          advanced ) ? "                  ** is h.                                         **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -qR  n          - delete ML  n completely.  If  n is working  ML then\n" : "" );
    output << ( (          advanced ) ? "                           return to  clean-slate state (no  data, all default\n" : "" );
    output << ( (          advanced ) ? "                           settings).                                         \n" : "" );
    output << ( (          advanced ) ? "         -qc  n m        - overwrite ML n with copy of ML m.                  \n" : "" );
    output << ( (          advanced ) ? "         -qs  n m        - swap ML n and ML m.                                \n" : "" );
    output << ( (          advanced ) ? "         -qw  n          - set ML n  as current (working) ML.   If n  not used\n" : "" );
    output << ( (          advanced ) ? "                           previously then create clean-slate ML first.       \n" : "" );
    output << ( (          advanced ) ? "         -qpush n        - push current ML index onto stack, run -qw n.       \n" : "" );
    output << ( (          advanced ) ? "         -qpop           - push ML index n off stack, run -qw n.              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -qsave n file   - save ML n to filename file.                        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Pre-Setup options (after multi-ML options):                                   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -zl  $file      - preload the ML from file.                          \n" : "" );
    output << ( ( basic || advanced ) ? "         -z   {...}      - ML type:                                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               Support Vector Machines (SVM):                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           s - SVM: single class.                             \n" : "" );
    output << ( ( basic || advanced ) ? "                           c - SVM: binary classification (default).          \n" : "" );
    output << ( ( basic || advanced ) ? "                           m - SVM: multiclass classification.                \n" : "" );
    output << ( ( basic || advanced ) ? "                           r - SVM: scalar regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                           v - SVM: vector regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                           a - SVM: anionic regression.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                           u - SVM: cyclic regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                           g - SVM: gentype regression (any target).          \n" : "" );
    output << ( ( basic || advanced ) ? "                           p - SVM:*density estimation.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                           t - SVM: pareto frontier SVM.                      \n" : "" );
    output << ( ( basic || advanced ) ? "                           l - SVM: binary scoring (zero bias by default).    \n" : "" );
    output << ( ( basic || advanced ) ? "                           o - SVM: scalar regression with scoring.           \n" : "" );
    output << ( ( basic || advanced ) ? "                           q - SVM: vector regression with scoring.           \n" : "" );
    output << ( ( basic || advanced ) ? "                           i - SVM: planar regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                           h - SVM: multi-expert ranking.                     \n" : "" );
    output << ( ( basic || advanced ) ? "                           j - SVM: multi-expert binary classification.       \n" : "" );
    output << ( ( basic || advanced ) ? "                           b - SVM: similarity learning.                      \n" : "" );
    output << ( ( basic || advanced ) ? "                           d - SVM: basic SVM for kernel inheritance (-x).    \n" : "" );
    output << ( ( basic || advanced ) ? "                           R - SVM: scalar regression using random FF.#       \n" : "" );
    output << ( ( basic || advanced ) ? "                           B - SVM: binary classifier using random FF.#       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               *Uses  1-norm cost,  kernel can be  non-Mercer.\n" : "" );
    output << ( ( basic || advanced ) ? "                                Recommend setting r0 = N, norm with -Sna.     \n" : "" );
    output << ( ( basic || advanced ) ? "                               #Kernel choice is limited to 3,4,13,19.        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               Least-squares SVMs (LSV):                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                         lsc - LSV: binary classification.                    \n" : "" );
    output << ( ( basic || advanced ) ? "                         lsr - LSV: scalar regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                         lsv - LSV: vector regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                         lsa - LSV: anionic regression.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                         lsg - LSV: gentype regression.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                         lsi - LSV: planar regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                         lso - LSV: scalar regression with scoring.           \n" : "" );
    output << ( ( basic || advanced ) ? "                         lsq - LSV: vector regression with scoring.           \n" : "" );
    output << ( ( basic || advanced ) ? "                         lsh - LSV: multi-expert ranking.                     \n" : "" );
    output << ( ( basic || advanced ) ? "                         lsR - LSV: scalar regression random FF.#             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               Gaussian processes (GPR):                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                         gpr - GPR: gaussian process scalar regression.       \n" : "" );
    output << ( ( basic || advanced ) ? "                         gpv - GPR: gaussian process vector regression.****   \n" : "" );
    output << ( ( basic || advanced ) ? "                         gpa - GPR: gaussian process anionic regression.      \n" : "" );
    output << ( ( basic || advanced ) ? "                         gpg - GPR: gaussian process gentype regression.      \n" : "" );
    output << ( ( basic || advanced ) ? "                         gpc - GPR: gaussian process binary classification.*  \n" : "" );
    output << ( ( basic || advanced ) ? "                         gpR - GPR: gaussian process scalar regression RFF.#  \n" : "" );
    output << ( ( basic || advanced ) ? "                         gpC - GPR: gaussian process binary classify RFF.#*   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               *Uses expectation propagation (EP).            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                               Type-II Multi-Layer Kernel Machines (MLM):     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                         mlr - MLM: Type-II MLK machine scalar regression.    \n" : "" );
    output << ( (          advanced ) ? "                         mlc - MLM: Type-II MLK machine scalar regression.    \n" : "" );
    output << ( (          advanced ) ? "                         mlv - MLM: Type-II MLK machine scalar regression.    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                         NB: MLM  is  extremely  experimental.   Binary  might\n" : "" );
    output << ( (          advanced ) ? "                             work (probably not), but none of the others do.  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               K-nearest-neighbour networks (KNN):            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                         knc - KNN: binary classification.                    \n" : "" );
    output << ( ( basic || advanced ) ? "                         knm - KNN: multiclass classification.                \n" : "" );
    output << ( ( basic || advanced ) ? "                         knr - KNN: scalar regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                         knv - KNN: vector regression.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                         kna - KNN: anionic regression.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                         kng - KNN: gentype regression.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                         knp - KNN: density estimation.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               Improvement measures (IMPs):                   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                          ei - IMP: expected (hypervolume) improvement.       \n" : "" );
    output << ( ( basic || advanced ) ? "                         svm - IMP: 1-norm 1-class modded SVM mono-surrogate. \n" : "" );
    output << ( ( basic || advanced ) ? "                         rls - IMP: Random linear scalarisation.              \n" : "" );
    output << ( ( basic || advanced ) ? "                         rns - IMP: Random draw from  a GP transformed into an\n" : "" );
    output << ( ( basic || advanced ) ? "                                    increasing function on [0,1]^d.           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               Learning blocks and glue (BLK):                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                         nop - BLK: NOP machine.                              \n" : "" );
    output << ( ( basic || advanced ) ? "                         mer - BLK: Mercer kernel inheritance block.          \n" : "" );
    output << ( ( basic || advanced ) ? "                         con - BLK: consensus machine.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                         fna - BLK: user function machine (elementwise).*     \n" : "" );
    output << ( ( basic || advanced ) ? "                         fnb - BLK: user function machine (vectorwise).*      \n" : "" );
    output << ( ( basic || advanced ) ? "                         mxa - BLK: mex function machine (elementwise).       \n" : "" );
    output << ( ( basic || advanced ) ? "                         mxb - BLK: mex function machine (vectorwise).        \n" : "" );
    output << ( ( basic || advanced ) ? "                          io - BLK: user I/O machine.                         \n" : "" );
    output << ( ( basic || advanced ) ? "                         sys - BLK: system call machine.                      \n" : "" );
    output << ( ( basic || advanced ) ? "                         avr - BLK: scalar averaging machine.                 \n" : "" );
    output << ( ( basic || advanced ) ? "                         avv - BLK: vector averaging machine.                 \n" : "" );
    output << ( ( basic || advanced ) ? "                         ava - BLK: anionic averaging machine.                \n" : "" );
    output << ( ( basic || advanced ) ? "                         fcb - BLK: function callback (do not use).           \n" : "" );
    output << ( ( basic || advanced ) ? "                         ber - BLK: Bernstein basis polynomial.               \n" : "" );
    output << ( ( basic || advanced ) ? "                         bat - BLK: Battery model.**                          \n" : "" );
    output << ( ( basic || advanced ) ? "                         ker - BLK: kernel specialisation.***                 \n" : "" );
    output << ( ( basic || advanced ) ? "                         mba - BLK: multi-block sum.                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                    (g(x), kernel transfer ave over multi MLs)\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                               *This function can be a  distribution, in which\n" : "" );
    output << ( ( basic || advanced ) ? "                                case g(x) is a sample from  this distribution.\n" : "" );
    output << ( ( basic || advanced ) ? "                                You can \"freeze\"  this (ie. take a  sample and\n" : "" );
    output << ( ( basic || advanced ) ? "                                return it consistently afterwards) by sampling\n" : "" );
    output << ( ( basic || advanced ) ? "                                using -St.                                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                              **Battery model as per Cer1.  Evaluation is:    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                g([3 i(x) v s])= time to reach target  voltage\n" : "" );
    output << ( ( basic || advanced ) ? "                                                with charging current i(t). If\n" : "" );
    output << ( ( basic || advanced ) ? "                                                target not reached then result\n" : "" );
    output << ( ( basic || advanced ) ? "                                                is max time + s.Verr.         \n" : "" );
    output << ( ( basic || advanced ) ? "                                g([2 v(x) v s])= time to reach target  voltage\n" : "" );
    output << ( ( basic || advanced ) ? "                                                with charging voltage v(t).   \n" : "" );
    output << ( ( basic || advanced ) ? "                                g([1 p(x) v s])= time to reach target  voltage\n" : "" );
    output << ( ( basic || advanced ) ? "                                                with charging power p(t).     \n" : "" );
    output << ( ( basic || advanced ) ? "                                g([0 i(x) v s])= time to reach target  voltage\n" : "" );
    output << ( ( basic || advanced ) ? "                                                with discharging current i(t).\n" : "" );
    output << ( ( basic || advanced ) ? "                                g([-1 t i v s]) = given  vectors    time  (t),\n" : "" );
    output << ( ( basic || advanced ) ? "                                                current (i),  voltage (v), for\n" : "" );
    output << ( ( basic || advanced ) ? "                                                current  charging, return  how\n" : "" );
    output << ( ( basic || advanced ) ? "                                                close the simulation is to the\n" : "" );
    output << ( ( basic || advanced ) ? "                                                given data (L2 voltage error).\n" : "" );
    output << ( ( basic || advanced ) ? "                                g([ -2 dfile m N s ]) = lie g([ -1 ... ]), but\n" : "" );
    output << ( ( basic || advanced ) ? "                                                data is in a file dfile.  m is\n" : "" );
    output << ( ( basic || advanced ) ? "                                                the startpoint in  the file, N\n" : "" );
    output << ( ( basic || advanced ) ? "                                                is the  max number  of obs  to\n" : "" );
    output << ( ( basic || advanced ) ? "                                                compare (-1 for all), s is the\n" : "" );
    output << ( ( basic || advanced ) ? "                                                scalarisation.  Result is:    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                s*earlystop + ave_error       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                where ave_error is the average\n" : "" );
    output << ( ( basic || advanced ) ? "                                                voltage  error  and  earlystop\n" : "" );
    output << ( ( basic || advanced ) ? "                                                is the number  of observations\n" : "" );
    output << ( ( basic || advanced ) ? "                                                skipped  at  the  end  due  to\n" : "" );
    output << ( ( basic || advanced ) ? "                                                model failure.                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                             ***This defines inheritable a (m-)kernels:       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                K(y)   = sum_qi  lambda_iq K(x_i,z_0)         \n" : "" );
    output << ( ( basic || advanced ) ? "                                K(y,z) = sum_qij lambda_iq K(x_i,x_j,y,z)     \n" : "" );
    output << ( ( basic || advanced ) ? "                                ...                                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                where lambda  can be set (eg by  Bayesian opt)\n" : "" );
    output << ( ( basic || advanced ) ? "                                using -SAA.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                            ****For some reason with vectorial GPs you need to\n" : "" );
    output << ( ( basic || advanced ) ? "                                do full setup *before* adding data - e.g.:    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                 ./svmheavyv7.exe -z gpv -m m -Ad 2 -c 1 -kt 2\n" : "" );
    output << ( ( basic || advanced ) ? "                                                      -kd 2 -Zx -AA xorand.txt\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                (note the -Zx before adding data here, and the\n" : "" );
    output << ( ( basic || advanced ) ? "                                -Ad to  set the  target space  dimension right\n" : "" );
    output << ( ( basic || advanced ) ? "                                after defining type).                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           Using this  function at  any point  will completely\n" : "" );
    output << ( ( basic || advanced ) ? "                           remove any existing ML.                            \n" : "" );
    output << ( (          advanced ) ? "         -zd  {...}      - like  -z,  but keeps  data.   Note that  this is  a\n" : "" );
    output << ( (          advanced ) ? "                           potentially lossy  function, and  may give an error\n" : "" );
    output << ( (          advanced ) ? "                           for  incompatible types  (eg going  from regression\n" : "" );
    output << ( (          advanced ) ? "                           to classification is not possible).  Moreover there\n" : "" );
    output << ( (          advanced ) ? "                           is a degree  of guesswork  involved, so  don't rely\n" : "" );
    output << ( (          advanced ) ? "                           too much on this option.                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -zv  {once,red} - vectorial/anionic SVM type.  Modes are:            \n" : "" );
    output << ( ( basic || advanced ) ? "                           once   - at-once regression.                       \n" : "" );
    output << ( ( basic || advanced ) ? "                           red    - reduction to binary regression (default). \n" : "" );
    output << ( ( basic || advanced ) ? "         -zc  {1vsA,1vs1,- multiclass classifier type.  Modes are:            \n" : "" );
    output << ( ( basic || advanced ) ? "              DAG,MOC,     1vsA   - 1 versus all (reduction to binary).       \n" : "" );
    output << ( ( basic || advanced ) ? "              maxwin,      1vs1   - 1 versus 1 (reduction to binary).         \n" : "" );
    output << ( ( basic || advanced ) ? "              recdiv}      DAG    - directed acyclic graph (reduct to binary).\n" : "" );
    output << ( ( basic || advanced ) ? "                           MOC    - minimum output coding (reduct to binary). \n" : "" );
    output << ( ( basic || advanced ) ? "                           maxwin - max-wins SVM (at once).                   \n" : "" );
    output << ( ( basic || advanced ) ? "                           recdiv - recursive division SVM (at once, default).\n" : "" );
    output << ( ( basic || advanced ) ? "         -zo  {sch,tax}  - one-class SVM method:                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           sch    - Scholkopf 1999 type (default).            \n" : "" );
    output << ( ( basic || advanced ) ? "                           tax    - Tax  and   Duin,   \"Support  Vector   Data\n" : "" );
    output << ( ( basic || advanced ) ? "                                    Description\" (SVDD), Machine Learning, 54,\n" : "" );
    output << ( ( basic || advanced ) ? "                                    2004.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Setup options (after pre-setup options):                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -N   n          - hint of expected training  set size (this will help\n" : "" );
    output << ( ( basic || advanced ) ? "                           minimise memory usage and duplication overhead). -1\n" : "" );
    output << ( ( basic || advanced ) ? "                           is translated to INT_MAX-1 (effectively inf).      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fo  n $file    - open file for  future processing  using placeholder\n" : "" );
    output << ( (          advanced ) ? "                           n.  The  file may  then  be  used for  training and\n" : "" );
    output << ( (          advanced ) ? "                           testing.  Variable var(0,n) will contain the number\n" : "" );
    output << ( (          advanced ) ? "                           of vectors remaining in  the file.  If n is already\n" : "" );
    output << ( (          advanced ) ? "                           in use then the  current file will be  closed and a\n" : "" );
    output << ( (          advanced ) ? "                           new file opened.                                   \n" : "" );
    output << ( (          advanced ) ? "         -foe n $file    - target-at-end version of -fo.                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fret n m       - tag variable  var(n,m) to be retained  after return\n" : "" );
    output << ( (          advanced ) ? "                           from for example -MF or -g, -gb etc.               \n" : "" );
    output << ( (          advanced ) ? "         -fV  n $fn      - set argument var(0,n) = $fn (not evaluated).       \n" : "" );
    output << ( (          advanced ) ? "         -fW  n v        - set argument var(0,n) = v (evaluated).             \n" : "" );
    output << ( (          advanced ) ? "         -fWW n v        - set argument var(0,n) = v   (evaluated   but    not\n" : "" );
    output << ( (          advanced ) ? "                           finalised, so  for example  random numbers  are not\n" : "" );
    output << ( (          advanced ) ? "                           drawn from and globals are not evaluated.          \n" : "" );
    output << ( (          advanced ) ? "         -fru n          - set argument var(0,n) = uniform random U(0,1).     \n" : "" );
    output << ( (          advanced ) ? "         -frn n          - set argument var(0,n) = gaussian random N(0,1).    \n" : "" );
    output << ( (          advanced ) ? "         -fri n          - set argument var(0,n) = random integer.            \n" : "" );
    output << ( (          advanced ) ? "         -fM  n args     - set argument var(130,n) =  string  of  args.   args\n" : "" );
    output << ( (          advanced ) ? "                           should be enclosed in {}.  These are used as macros\n" : "" );
    output << ( (          advanced ) ? "                           later.                                             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fu  n i x      - single-objective  test function  evaluation.  Given\n" : "" );
    output << ( (          advanced ) ? "                           decision  vector  x evaluates  test  function  i to\n" : "" );
    output << ( (          advanced ) ? "                           produce output that is stored in variable var(0,n).\n" : "" );
    output << ( (          advanced ) ? "                           These functions can also be accessed in expressions\n" : "" );
    output << ( (          advanced ) ? "                           using one of:                                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - fnB(-1,i,x)                                      \n" : "" );
    output << ( (          advanced ) ? "                           - fnC(-2,i,x,A).                                   \n" : "" );
    output << ( (          advanced ) ? "                           - testfn(i,x).                                     \n" : "" );
    output << ( (          advanced ) ? "                           - testfnA(i,x,A).                                  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           available functions are (d=dim(x), c/f Wikipedia): \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "+-------------------------------+---+------------------------+---------------+\n" : "" );
    output << ( (          advanced ) ? "|  i: Function name             | d | Range                  | Minimum       |\n" : "" );
    output << ( (          advanced ) ? "+-------------------------------+---+------------------------+---------------+\n" : "" );
    output << ( (          advanced ) ? "|  1: Rastrigin function        | d | -5.12   <= x_i <= 5.12 | f(0)     = 0  |\n" : "" );
    output << ( (          advanced ) ? "|  2: Ackley's function         | d | -5      <= x_i <= 5    | f(0)     = 0  |\n" : "" );
    output << ( (          advanced ) ? "|  3: Sphere function           | d | -inf    <= x_i <= inf  | f(0)     = 0  |\n" : "" );
    output << ( (          advanced ) ? "|  4: Rosenbrock function       | d | -inf    <= x_i <= inf  | f(1)     = 0  |\n" : "" );
    output << ( (          advanced ) ? "|  5: Beale's function          | 2 | -4.5    <= x,y <= 4.5  | f(3,0.5) = 0  |\n" : "" );
    output << ( (          advanced ) ? "|  6: Goldstein-Price function  | 2 | -2      <= x,y <= 2    | f(0,-1)  = 3  |\n" : "" );
    output << ( (          advanced ) ? "|  7: Booth's function          | 2 | -10     <= x,y <= 10   | f(1,3)   = 0  |\n" : "" );
    output << ( (          advanced ) ? "|  8: Bukin function N.6        | 2 | -15,-3  <= x,y <= -5,3 | f(-10,1) = 0  |\n" : "" );
    output << ( (          advanced ) ? "|  9: Matyas function           | 2 | -10     <= x,y <= 10   | f(0,0)   = 0  |\n" : "" );
    output << ( (          advanced ) ? "| 10: Levi function N.13        | 2 | -10     <= x,y <= 10   | f(1,1)   = 0  |\n" : "" );
    output << ( (          advanced ) ? "| 11: Himmelblau's function     | 2 | -5      <= x,y <= 5    | f(s,t)   = 0  |\n" : "" );
    output << ( (          advanced ) ? "| 12: Three-hump camel function | 2 | -5      <= x,y <= 5    | f(0,0)   = 0  |\n" : "" );
    output << ( (          advanced ) ? "| 13: Easom function            | 2 | -100    <= x,y <= 100  | f(pi,pi) = -1 |\n" : "" );
    output << ( (          advanced ) ? "| 14: Cross-in-tray function    | 2 | -10     <= x,y <= 10   | f(a,a)   = b  |\n" : "" );
    output << ( (          advanced ) ? "| 15: Eggholder function        | 2 | -512    <= x,y <= 512  | f(c,d)   = e  |\n" : "" );
    output << ( (          advanced ) ? "| 16: Holder table function     | 2 | -10     <= x,y <= 10   | f(f,f)   = g  |\n" : "" );
    output << ( (          advanced ) ? "| 17: McCormick function        | 2 | -1.5,-3 <= x,y <= 4,4  | f(h,j)   = k  |\n" : "" );
    output << ( (          advanced ) ? "| 18: Schaffer function N. 2    | 2 | -100    <= x,y <= 100  | f(0,0)   = 0  |\n" : "" );
    output << ( (          advanced ) ? "| 19: Schaffer function N. 4    | 2 | -100    <= x,y <= 100  | f(0,l)   = m  |\n" : "" );
    output << ( (          advanced ) ? "| 20: Styblinski-Tang function  | d | -5      <= x_i <= 5    | q <= f(p) <= r|\n" : "" );
    output << ( (          advanced ) ? "| 21: Stability test function 1 | 1 |  0      <= x   <= 1    | f(0.2)   = 1.3|\n" : "" );
    output << ( (          advanced ) ? "|     (also has unstable max at |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "|     f(0.5) = 1.65 (2nd order) |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "|     and f(1) = 1.5 (1st))     |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "| 22: Stability test function 2 | 1 |  0      <= x   <= 1    | f(1)     = 4  |\n" : "" );
    output << ( (          advanced ) ? "|     Sum  of   two  gaussians, |   |                        | f(0.5)   = 1  |\n" : "" );
    output << ( (          advanced ) ? "|     gamma  =   1/(5.sqrt(2)), |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "|     centres  at  1  (alpha 4) |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "|     and 0.5  (alpha  1).  Use |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "|     A = 0.2, B = 0.05, pmax=2 |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "| 23: Sum   of  RBFs   function | d |  0      <= x   <= 1    | depends       |\n" : "" );
    output << ( (          advanced ) ? "|     sum_i a_{i,0} exp(-(||x-a_{i,2:...}||_2^2)/(2*a_{i,1}*a_{i,1}))        |\n" : "" );
    output << ( (          advanced ) ? "| 24: Drop-wave function        | 2 | -5.12   <= x,y <= 5.12 | f(0,0)   = -1 |\n" : "" );
    output << ( (          advanced ) ? "| 25: Gramancy and Lee function | 1 | -0.5    <= x   <= 2.5  | f(0.548)=-0.95|\n" : "" );
    output << ( (          advanced ) ? "| 26: Langermann function       | 2 |  0      <= x,y <= 10   | unknown       |\n" : "" );
    output << ( (          advanced ) ? "| 27: Griewank function         | d | -600    <= x_i <= 600  | f(0)     = 0  |\n" : "" );
    output << ( (          advanced ) ? "| 28: Levy function             | d | -10     <= x_i <= 10   | f(1)     = 0  |\n" : "" );
    output << ( (          advanced ) ? "| 29: Shwefel function          | d | -500    <= x_i <= 500  | f(420.9687)=0 |\n" : "" );
    output << ( (          advanced ) ? "| 30: Shubert function          | 2 | -10     <= x,y <= 10   | f(?)=186.7309 |\n" : "" );
    output << ( (          advanced ) ? "| 31: Bohachevsky function 1    | 2 | -100    <= x,y <= 100  | f(0)=0        |\n" : "" );
    output << ( (          advanced ) ? "| 32: Bohachevsky function 2    | 2 | -100    <= x,y <= 100  | f(0)=0        |\n" : "" );
    output << ( (          advanced ) ? "| 33: Bohachevsky function 3    | 2 | -100    <= x,y <= 100  | f(0)=0        |\n" : "" );
    output << ( (          advanced ) ? "| 34: Perm 0,D,1                | d | -2      <= x,y <= 2 |f(1,1/2,...,1/d)=0|\n" : "" );
    output << ( (          advanced ) ? "|                               |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "| 10xx: function xx, normalised | d | -1      <= x   <= 1    | depends       |\n" : "" );
    output << ( (          advanced ) ? "|     (nominally) so  -1<=x<=1, |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "|     0<=f(x)<=1                |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "| 20xx: function xx, normalised | d |  0      <= x   <= 1    | depends       |\n" : "" );
    output << ( (          advanced ) ? "|     (nominally)  so  0<=x<=1, |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "|     0<=f(x)<=1                |   |                        |               |\n" : "" );
    output << ( (          advanced ) ? "+-------------------------------+---+------------------------+---------------+\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Stability test function:                           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           f(x) = exp(-20*(x-0.2)*(x-0.2))                    \n" : "" );
    output << ( (          advanced ) ? "                                + exp(-20*sqrt(0.00001+((x-0.5)*(x-0.5))))    \n" : "" );
    output << ( (          advanced ) ? "                                + exp(2*(x-0.8))                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Constants: a = +-1.34941                           \n" : "" );
    output << ( (          advanced ) ? "                                      b = -2.06261                            \n" : "" );
    output << ( (          advanced ) ? "                                      c = 512                                 \n" : "" );
    output << ( (          advanced ) ? "                                      d = 404.2319                            \n" : "" );
    output << ( (          advanced ) ? "                                      e = -959.6407                           \n" : "" );
    output << ( (          advanced ) ? "                                      f = +-8.05502                           \n" : "" );
    output << ( (          advanced ) ? "                                      g = -19.2085                            \n" : "" );
    output << ( (          advanced ) ? "                                      h = -0.54719                            \n" : "" );
    output << ( (          advanced ) ? "                                      j = -1.54719                            \n" : "" );
    output << ( (          advanced ) ? "                                      k = -1.9133                             \n" : "" );
    output << ( (          advanced ) ? "                                      l = 1.25313                             \n" : "" );
    output << ( (          advanced ) ? "                                      m = 0.292579                            \n" : "" );
    output << ( (          advanced ) ? "                                      p = -2.903534                           \n" : "" );
    output << ( (          advanced ) ? "                                      q = -39.16617n                          \n" : "" );
    output << ( (          advanced ) ? "                                      r = -39.16616n                          \n" : "" );
    output << ( (          advanced ) ? "                                      (s,t) = (3,2), (-2.805,3.131),          \n" : "" );
    output << ( (          advanced ) ? "                                          (-3.779,-3.283), (3.584,-1.848)     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fuu n i x a    - like -fu with additional user parameter matrix a.  \n" : "" );
    output << ( (          advanced ) ? "         -ft  n i M x    - multi-objective  test  function  evaluation.  Given\n" : "" );
    output << ( (          advanced ) ? "                           decision  vector x  evaluates  test  function i  to\n" : "" );
    output << ( (          advanced ) ? "                           produce  an  M-dimensional  output  vector  that is\n" : "" );
    output << ( (          advanced ) ? "                           stored  in  variable  var(0,n).  These can  also be\n" : "" );
    output << ( (          advanced ) ? "                           accessed via:                                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - fnC(-3,i,x,M)                                    \n" : "" );
    output << ( (          advanced ) ? "                           - partestfn(i,x,M)                                 \n" : "" );
    output << ( (          advanced ) ? "                           - partestfnA(i,x,M,A)                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Available test functions  are (c/f SCH1, DTLZ, FON1\n" : "" );
    output << ( (          advanced ) ? "                           and Wikipedia):                                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "+-------------+---+-----+-----------------------------------------+----------+\n" : "" );
    output << ( (          advanced ) ? "|  i: Fn name | n | M   | Function                                | Range    |\n" : "" );
    output << ( (          advanced ) ? "+-------------+---+-----+-----------------------------------------+----------+\n" : "" );
    output << ( (          advanced ) ? "|  1: DTLZ1   | n | <=n |  [ x0...xM-2.(1+g(z))/2          ; ]    | 0<=x<=1  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ x0...xM-3.(1-xM-2).(1+g(z))/2 ; ]    |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | -[   ...                         ; ]    |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ x0...(1-x1).(1+g(z))/2        ; ]    |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1-x0).(1+g(z))/2               ]    |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | g(z) = 100.( #(z) + sum_i ( (zi-0.5)^2  |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |               - cos(20*pi*(zi-0.5)) ) ) |          |\n" : "" );
    output << ( (          advanced ) ? "|  2: DTLZ2   | n | <=n |  [ (1+g(z)).c(x0)....c(xM-2).c(xM-1); ] | 0<=x<=1  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1+g(z)).c(x0)....c(xM-2).s(xM-1); ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | -[   ...                              ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1+g(z)).c(x0).s(x1)             ; ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1+g(z)).s(x0)                     ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | g(z) = sum_i (zi-0.5)^2                 |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | s(x) = sin( x.pi/2 )                    |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | c(x) = cos( x.pi/2 )                    |          |\n" : "" );
    output << ( (          advanced ) ? "|  3: DTLZ3   | n | <=n |  [ (1+g(z)).c(x0)....c(xM-2).c(xM-1); ] | 0<=x<=1  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1+g(z)).c(x0)....c(xM-2).s(xM-1); ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | -[   ...                              ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1+g(z)).c(x0).s(x1)             ; ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1+g(z)).s(x0)                     ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | g(z) = 100.( #(z) + sum_i ( (zi-0.5)^2  |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |               - cos(20*pi*(zi-0.5)) ) ) |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | s(x) = sin( x.pi/2 )                    |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | c(x) = cos( x.pi/2 )                    |          |\n" : "" );
    output << ( (          advanced ) ? "|  4: DTLZ4   | n | <=n |  [ (1+g(z)).c(x0)....c(xM-2).c(xM-1); ] | 0<=x1<=1 |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1+g(z)).c(x0)....c(xM-2).s(xM-1); ] | -5<=xi<=5|\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | -[   ...                              ] | 2<=i<=n  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1+g(z)).c(x0).s(x1)             ; ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1+g(z)).s(x0)                     ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | g(z) = sum_i (zi-0.5)^2                 |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | s(x) = sin( (x^alpha).pi/2 )            |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | c(x) = cos( (x^alpha).pi/2 )            |          |\n" : "" );
    output << ( (          advanced ) ? "|  5: DTLZ5   | n | <=n |  [ (1+g(z)).c(x0)....c(xM-2).c(xM-1); ] | 0<=x<=1  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1+g(z)).c(x0)....c(xM-2).s(xM-1); ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | -[   ...                              ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1+g(z)).c(x0).s(x1)             ; ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1+g(z)).s(x0)                     ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | g(z) = sum_i (zi-0.5)^2                 |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | s(x) = sin( theta.pi/2 )                |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | c(x) = cos( theta.pi/2 )                |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | theta = (pi/(4.(1+g(z)))).(1+2.g(z).xi) |          |\n" : "" );
    output << ( (          advanced ) ? "|  6: DTLZ6   | n | <=n |  [ (1+g(z)).c(x0)....c(xM-2).c(xM-1); ] | 0<=x<=1  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1+g(z)).c(x0)....c(xM-2).s(xM-1); ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | -[   ...                              ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1+g(z)).c(x0).s(x1)             ; ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1+g(z)).s(x0)                     ] |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | g(z) = sum_i zi^0.1                     |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | s(x) = sin( theta.pi/2 )                |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | c(x) = cos( theta.pi/2 )                |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | theta = (pi/(4.(1+g(z)))).(1+2.g(z).xi) |          |\n" : "" );
    output << ( (          advanced ) ? "|  7: DTLZ7   | n | <=n |  [ x0                             ; ]   | 0<=x<=1  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ x1                             ; ]   |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | -[   ...                            ]   |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ xM-2                           ; ]   |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (1+g(z)).h(f1,f2,...,fM-2,g(z))  ]   |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | g(z) = 1 + (9/#(z)) sum_i zi            |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | h = M-sum_i((fi/(1+g)).(1+sin(3pi.fi))) |          |\n" : "" );
    output << ( (          advanced ) ? "|  8: DTLZ8   | n | <n  | -[ sum_ib^is xi ], i = 0,1,...,M-1      | 0<=x<=1  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |        ib = floor(i*n/M)-1              |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |        is = floor((i+1)*n/M)-1          |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | **constraints are not implemented yet.  |          |\n" : "" );
    output << ( (          advanced ) ? "|  9: DTLZ9   | n | <n  | -[ sum_ib^is xi^0.1 ], i = 0,1,...,M-1  | 0<=x<=1  |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |        ib = floor(i*n/M)-1              |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |        is = floor((i+1)*n/M)-1          |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | **constraints are not implemented yet.  |          |\n" : "" );
    output << ( (          advanced ) ? "| 10: FON1    | n | 2   | -[ 1-exp(-|| x - 1/sqrt(n) ||^2) ; ]    | -4<=x<=4 |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ 1-exp(-|| x + 1/sqrt(n) ||^2)   ]    |          |\n" : "" );
    output << ( (          advanced ) ? "| 11: SCH1    | 1 | 2   | -[ x^2     ; ]                          | free     |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (x-2)^2   ]                          |          |\n" : "" );
    output << ( (          advanced ) ? "| 12: SCH2    | 1 | 2   |  [ { -x     if     x <= 1 } ]           | -5<=x<=10|\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ { x-2    if 1 < x <= 3 } ]           |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     | -[ { 4-x    if 3 < x <= 4 } ]           |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ { x-4    if 4 < x      } ]           |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [                          ]           |          |\n" : "" );
    output << ( (          advanced ) ? "|             |   |     |  [ (x-5)^2                  ]           |          |\n" : "" );
    output << ( (          advanced ) ? "+-------------+---+-----------------------------------------------+----------+\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "  10xx: function xx, normalised (nominally) so -1<=x<=1 and -1<=f(x)<=0       \n" : "" );
    output << ( (          advanced ) ? "  20xx: function xx, normalised (nominally) so  0<=x<=1 and -1<=f(x)<=0       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           SCH1: Schaffer, J. D.:  Some Experiments in Machine\n" : "" );
    output << ( (          advanced ) ? "                                 Learning   using  Vector   Evaluated  Genetic\n" : "" );
    output << ( (          advanced ) ? "                                 Algorithms - PhD Thesis, 1984.               \n" : "" );
    output << ( (          advanced ) ? "                           DTLZ: Deb,   Kalyanmoy  and   Thiele,  Lothar   and\n" : "" );
    output << ( (          advanced ) ? "                                 Laumanns,   Marco    and   Zitzler,   Eckart:\n" : "" );
    output << ( (          advanced ) ? "                                 \"Scalable  Test  Problems   for  Evolutionary\n" : "" );
    output << ( (          advanced ) ? "                                 Multiobjective Optimization\".                \n" : "" );
    output << ( (          advanced ) ? "                           FON1: Fonseca,  C.  M.  and   Fleming,  P.  J.:  An\n" : "" );
    output << ( (          advanced ) ? "                                 Overview of Evolutionary Algorithms in Multi-\n" : "" );
    output << ( (          advanced ) ? "                                 Objective     Optimisation.      Evolutionary\n" : "" );
    output << ( (          advanced ) ? "                                 Multiobjective   Optimisation,    Theoretical\n" : "" );
    output << ( (          advanced ) ? "                                 Advances and Applications, pg. 105-145, 2005.\n" : "" );
    output << ( (          advanced ) ? "                                 (as re-interpretted in DTLZ for n-dim).      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fat alpha      - sets alpha value used by DTLZ4 in -ft evaluation.  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** There are two  types of variable:  global and **         \n" : "" );
    output << ( (          advanced ) ? "                  ** local.  Local  variables are  specific to and **         \n" : "" );
    output << ( (          advanced ) ? "                  ** available everywhere  within a single thread. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Global  variables  are  accessible  from  all **         \n" : "" );
    output << ( (          advanced ) ? "                  ** current running threads,  allowing for inter- **         \n" : "" );
    output << ( (          advanced ) ? "                  ** thread  communications.    Use  local  unless **         \n" : "" );
    output << ( (          advanced ) ? "                  ** strictly necessary.                           **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fVg  n $fn     - like -fV  but reads from global variables.         \n" : "" );
    output << ( (          advanced ) ? "         -fWg  n v       - like -fW  but reads from global variables.         \n" : "" );
    output << ( (          advanced ) ? "         -fVG  n $fn     - like -fV  but writes to global variables.          \n" : "" );
    output << ( (          advanced ) ? "         -fWG  n v       - like -fW  but writes to global variables.          \n" : "" );
    output << ( (          advanced ) ? "         -fuuG n i x a   - like -fuu but writes to global variables.          \n" : "" );
    output << ( (          advanced ) ? "         -fuG  n i x     - like -fu  but writes to global variables.          \n" : "" );
    output << ( (          advanced ) ? "         -ftG  n i M x   - like -ft  but writes to global variables.          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -br  d          - binary   class  label:  by   default,   for  binary\n" : "" );
    output << ( (          advanced ) ? "                           classification,  class labels are  -1 and +1.  This\n" : "" );
    output << ( (          advanced ) ? "                           option lets  you automatically  re-label vectors as\n" : "" );
    output << ( (          advanced ) ? "                           they are  loaded from  a file,  so class  d becomes\n" : "" );
    output << ( (          advanced ) ? "                           class +1 and  all other classes  are relabelled -1.\n" : "" );
    output << ( (          advanced ) ? "                           (default 0, no relabelling).                       \n" : "" );
    output << ( (          advanced ) ? "         -bd  d          - class skipping: for classification, if this is non-\n" : "" );
    output << ( (          advanced ) ? "                           zero then points from  this class coming from files\n" : "" );
    output << ( (          advanced ) ? "                           will be silently ignored. This is handy if you want\n" : "" );
    output << ( (          advanced ) ? "                           to ignore a  particular class in a  multiclass case\n" : "" );
    output << ( (          advanced ) ? "                           (default 0, no skipping).                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -XT  [ x ]      - set x template  (sparse).  If there are elements in\n" : "" );
    output << ( (          advanced ) ? "                           this template  that are not  in a  testing/training\n" : "" );
    output << ( (          advanced ) ? "                           vector  then they will  be added.  This  is helpful\n" : "" );
    output << ( (          advanced ) ? "                           for multitask  learning.  Use -XT [ ~ i ],  where i\n" : "" );
    output << ( (          advanced ) ? "                           is the  task number, and  use  -kS -ks 2  -ki 0 ...\n" : "" );
    output << ( (          advanced ) ? "                           -ki 1 -kt 48 -kr r1, where ...  is the usual kernel\n" : "" );
    output << ( (          advanced ) ? "                           setup and r1 is  the similarity (scalar  or matrix)\n" : "" );
    output << ( (          advanced ) ? "                           between tasks.                                     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- MEX (Matlab) only options                     --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fWm n $fn v    - set argument var(0,n) = fn(v), where fn is a MATLAB\n" : "" );
    output << ( (          advanced ) ? "                           function.  Use v = null if no args required, set if\n" : "" );
    output << ( (          advanced ) ? "                           more than one arg required.                        \n" : "" );
    output << ( (          advanced ) ? "         -fWM n i v      - set argument var(0,n) = fni(v) (v evaluated), where\n" : "" );
    output << ( (          advanced ) ? "                           fni is a MATLAB function  handle specified when you\n" : "" );
    output << ( (          advanced ) ? "                           called svmmatlab.   If fni if a  variable and not a\n" : "" );
    output << ( (          advanced ) ? "                           handle then result is value of variable.           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -ac  {svc,svr}  - classification method:                             \n" : "" );
    output << ( ( basic || advanced ) ? "                           svc: normal SVM classifier (default).              \n" : "" );
    output << ( ( basic || advanced ) ? "                           svr: classify via regression.                      \n" : "" );
    output << ( ( basic || advanced ) ? "         -B   {f,v,p,n}  - bias type:                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                           f - fixed bias (bias value defaults to 0).         \n" : "" );
    output << ( ( basic || advanced ) ? "                           v - variable bias (default).                       \n" : "" );
    output << ( ( basic || advanced ) ? "                           p - positive bias.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                           n - negative bias.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -R   {l,q,o,g,G}- empirical risk type.                               \n" : "" );
    output << ( ( basic || advanced ) ? "                           l - linear (default).                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           q - quadratic.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                           o - linear cost, but with  1-norm regularisation on\n" : "" ); 
    output << ( ( basic || advanced ) ? "                               alpha  (not feature  space:  use -m  for that).\n" : "" ); 
    output << ( ( basic || advanced ) ? "                               (to  enforce 1-norm  regularisation  with hard-\n" : "" ); 
    output << ( ( basic || advanced ) ? "                               margin use this in combination with -c 1e20).  \n" : "" ); 
    output << ( ( basic || advanced ) ? "                           g - generalised linear cost (iterative fuzzy).     \n" : "" );
    output << ( ( basic || advanced ) ? "                           G - generalised quadratic cost (iterative fuzzy).  \n" : "" );
    output << ( ( basic || advanced ) ? "                           Note  that  quadratic  quadratic empirical  ignores\n" : "" );
    output << ( ( basic || advanced ) ? "                           values set by -c+, -c-, -c=, -cc and -jc. Note also\n" : "" );
    output << ( ( basic || advanced ) ? "                           that epsilon insensitivity  is used for both linear\n" : "" );
    output << ( ( basic || advanced ) ? "                           and quadratic cases, so  to construct an LS-SVR you\n" : "" );
    output << ( ( basic || advanced ) ? "                           need to set -R q -w 0.                             \n" : "" );
    output << ( ( basic || advanced ) ? "         -T   {f,s}      - tube type:                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                           f - fixed tube (default).                          \n" : "" );
    output << ( ( basic || advanced ) ? "                           s - tube shrinking on.                             \n" : "" );
    output << ( ( basic || advanced ) ? "         -TT  {0,1}      - svm_kconst alpha usage:                            \n" : "" );
    output << ( ( basic || advanced ) ? "                           0 - alphas define kernel through standard method.  \n" : "" );
    output << ( ( basic || advanced ) ? "                           1 - alpha^2 also written as kernel weights.        \n" : "" );
    output << ( ( basic || advanced ) ? "                           Default is 0, so alpha defines a kernel through eg.\n" : "" );
    output << ( ( basic || advanced ) ? "                           kernel type 801 inheritance,  but if you set this 1\n" : "" );
    output << ( ( basic || advanced ) ? "                           alpha defines  weights in a kernel  sum with kernel\n" : "" );
    output << ( ( basic || advanced ) ? "                           kernel type 800 inheritance. This is made easy with\n" : "" );
    output << ( ( basic || advanced ) ? "                           the -x commands later.                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- LSV specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -bv             - LSV variable bias (default).                       \n" : "" );
    output << ( ( basic || advanced ) ? "         -bz             - LSV zero bias.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "         -bn  n          - LSV posterior variance terms:                      \n" : "" );
    output << ( ( basic || advanced ) ? "                          -1 - usual (exact) posterior variance calc.         \n" : "" );
    output << ( ( basic || advanced ) ? "                          0  - return prior variance (kernel) instead.        \n" : "" );
    output << ( ( basic || advanced ) ? "                          >0 - approximate posterior  variance using this many\n" : "" );
    output << ( ( basic || advanced ) ? "                               training vectors closest  (in feature space) to\n" : "" );
    output << ( ( basic || advanced ) ? "                               the point being tested.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- GPR specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -bgv            - GPR variable bias.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -bgz            - GPR zero bias (default).                           \n" : "" );
    output << ( ( basic || advanced ) ? "         -bgn n          - GPR posterior variance terms:                      \n" : "" );
    output << ( ( basic || advanced ) ? "                          -1 - usual (exact) posterior variance calc.         \n" : "" );
    output << ( ( basic || advanced ) ? "                          0  - return prior variance (kernel) instead.        \n" : "" );
    output << ( ( basic || advanced ) ? "                          >0 - approximate posterior  variance using this many\n" : "" );
    output << ( ( basic || advanced ) ? "                               training vectors closest  (in feature space) to\n" : "" );
    output << ( ( basic || advanced ) ? "                               the point being tested.                        \n" : "" );
    output << ( ( basic || advanced ) ? "         -bgep           - GPR inequalities trained using expect prop (deflt).\n" : "" );
    output << ( ( basic || advanced ) ? "         -bgnc           - GPR inequalities treated as per SVM.               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- MLM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -mlR n {l,q}    - MLM regularisation type at layer n:                \n" : "" );
    output << ( ( basic || advanced ) ? "                           l - 1-norm.                                        \n" : "" );
    output << ( ( basic || advanced ) ? "                           q - 2-norm.                                        \n" : "" );
    output << ( ( basic || advanced ) ? "         -mls n          - Set number of layers (not including output, dft 0).\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- BLK specific options                          --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -mc  n          - (Mercer kernel inheritance block): sets the size of\n" : "" );
    output << ( (          advanced ) ? "                           the kernel cache (if any) stored inside this block.\n" : "" );
    output << ( (          advanced ) ? "                           Unlike  other  caches  this one  is  simply  an n*n\n" : "" );
    output << ( (          advanced ) ? "                           matrix, and needs to be manually flushed for kernel\n" : "" );
    output << ( (          advanced ) ? "                           changes etc.  Set  -1 for no cache.  This  does not\n" : "" );
    output << ( (          advanced ) ? "                           store 4-kernel  or m-kernel (m>2)  evaluations.  To\n" : "" );
    output << ( (          advanced ) ? "                           clear the cache reuse -mc n flag.                  \n" : "" );
    output << ( (          advanced ) ? "         -mcn {0,1}      - (Mercer  kernel inheritance  block):  normalisation\n" : "" );
    output << ( (          advanced ) ? "                           on (1) or off (0, default).                        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -mbI i n        - (Multi-block average): insert ML n as element i.   \n" : "" );
    output << ( (          advanced ) ? "         -mbA i          - (Multi-block average): remove ML element i.        \n" : "" );
    output << ( (          advanced ) ? "         -mba i n        - (Multi-block average): set ML n as element i.      \n" : "" );
    output << ( (          advanced ) ? "         -mbw i w        - (Multi-block average): set ML n weight as w.       \n" : "" );
    output << ( (          advanced ) ? "         -mbm m          - (Multi-block average): set block mode:             \n" : "" );
    output << ( (          advanced ) ? "                           0: normal weights sum of blocks                    \n" : "" );
    output << ( (          advanced ) ? "                           1: stack  of priors.  The first ML acts  as a prior\n" : "" );
    output << ( (          advanced ) ? "                              mean for  the second,  the second for  the third\n" : "" );
    output << ( (          advanced ) ? "                              and so on.  Only the final ML decides contributs\n" : "" );
    output << ( (          advanced ) ? "                              to the posterior variance.                      \n" : "" );
    output << ( (          advanced ) ? "                           2: stack of  priors, BUT kernel  tuning only  tunes\n" : "" );
    output << ( (          advanced ) ? "                              the final (last) ML on the stack.               \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -msn n          - set order of Bernstein basis b_{i,n}(x).           \n" : "" );
    output << ( (          advanced ) ? "         -msw i          - set index of Bernstein basis b_{i,n}(x).           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -bat p          - Set battery parameters.  p is a vector:            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                          (defaults)                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                         [ C_0star ] Battery capacity at Istar (261.9 Ah)     \n" : "" );
    output << ( (          advanced ) ? "                         [ K_c     ] Battery parameter (1.18)                 \n" : "" );
    output << ( (          advanced ) ? "                         [ theta_f ] Electrolyte freezing temp (-40 degree C) \n" : "" );
    output << ( (          advanced ) ? "                         [ epsilon ] Battery parameter (1.29)                 \n" : "" );
    output << ( (          advanced ) ? "                         [ delta   ] Battery parameter (1.40)                 \n" : "" );
    output << ( (          advanced ) ? "                         [ Istar   ] Nominal battery current (49 A)           \n" : "" );
    output << ( (          advanced ) ? "                         [         ]                                          \n" : "" );
    output << ( (          advanced ) ? "                         [ E_m0    ] Battery voltage at full charge (2.135 V) \n" : "" );
    output << ( (          advanced ) ? "                         [ K_E     ] Battery parameter (0.58e-3 V/degree C    \n" : "" );
    output << ( (          advanced ) ? "                         [ tau_1   ] Battery parameter (5000 sec)             \n" : "" );
    output << ( (          advanced ) ? "                         [ R_00    ] Battery parameter (2e-3 ohm)             \n" : "" );
    output << ( (          advanced ) ? "                         [ R_10    ] Battery parameter (0.7e-3 ohm)           \n" : "" );
    output << ( (          advanced ) ? "                         [ R_20    ] Battery parameter (15e-3 ohm)            \n" : "" );
    output << ( (          advanced ) ? "                         [ A_0     ] Battery parameter (-0.3)                 \n" : "" );
    output << ( (          advanced ) ? "                         [ A_21    ] Battery parameter (-8.0)                 \n" : "" );
    output << ( (          advanced ) ? "                         [ A_22    ] Battery parameter (-8.45)                \n" : "" );
    output << ( (          advanced ) ? "                         [         ]                                          \n" : "" );
    output << ( (          advanced ) ? "                         [ E_p     ] Battery parameter (1.95 V)               \n" : "" );
    output << ( (          advanced ) ? "                         [ V_p0    ] Battery parameter (0.1 V)                \n" : "" );
    output << ( (          advanced ) ? "                         [ A_p     ] Battery parameter (2.0)                  \n" : "" );
    output << ( (          advanced ) ? "                         [ G_p0    ] Battery parameter (2e-12 sec)            \n" : "" );
    output << ( (          advanced ) ? "                         [         ]                                          \n" : "" );
    output << ( (          advanced ) ? "                         [ C_theta ] Battery parameter (15 Wh/degree C)       \n" : "" );
    output << ( (          advanced ) ? "                         [ R_theta ] Battery parameter (0.2 degree C/W)       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           use null to leave parameter as-is.                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -bam p          - Set time period for battery model (sec, dflt 3600).\n" : "" );
    output << ( (          advanced ) ? "         -bac p          - Set max (dis)charge current (amps, default 30).    \n" : "" );
    output << ( (          advanced ) ? "         -bad d          - Set time-step for battery model (sec, dflt 0.05).  \n" : "" );
    output << ( (          advanced ) ? "         -bav v          - Set start voltage for battery model (volts, 2.135).\n" : "" );
    output << ( (          advanced ) ? "         -baT t          - Set ambient/start temperture for battery (deg, 20).\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Optimization options (after setup options):                                   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -oo             - do not optimise the ML this time.                  \n" : "" );
    output << ( ( basic || advanced ) ? "         -oO             - optimise the ML this time (default).               \n" : "" );
    output << ( ( basic || advanced ) ? "                           NB: -oo and  -oO are persistent, so if  you use -oo\n" : "" );
    output << ( ( basic || advanced ) ? "                               to save time you need  to use -oO later so that\n" : "" );
    output << ( ( basic || advanced ) ? "                               optimisation occurs.                           \n" : "" );
    output << ( ( basic || advanced ) ? "         -olr r          - sets learning rate (def 0.3).                      \n" : "" );
    output << ( ( basic || advanced ) ? "         -olrb r         - sets second learning rate (def 0.3, eg used for the\n" : "" );
    output << ( ( basic || advanced ) ? "                           inner loop in SVM_Scalar_rff).                     \n" : "" );
    output << ( ( basic || advanced ) ? "         -olrc r         - sets third learning rate (def 0.3).                \n" : "" );
    output << ( ( basic || advanced ) ? "         -olrd r         - sets fourth learning rate (def 0.3).               \n" : "" );
    output << ( ( basic || advanced ) ? "         -oe  e          - accuracy e>=0 required of  g(x) in soln (dflt .01).\n" : "" );
    output << ( ( basic || advanced ) ? "                           (use -oe A to set accuracy = max(0.01*eps,100*zt)).\n" : "" );
    output << ( ( basic || advanced ) ? "         -oea e          - set (only) first optimality tolerance.             \n" : "" );
    output << ( ( basic || advanced ) ? "         -oeb e          - set second optimality tolerance.                   \n" : "" );
    output << ( ( basic || advanced ) ? "         -oz  z          - set zero tolerance.                                \n" : "" );
    output << ( ( basic || advanced ) ? "         -ot  m          - terminate optimization after  m iterations, even if\n" : "" );
    output << ( ( basic || advanced ) ? "                           solution not found (default LONG_MAX).             \n" : "" );
    output << ( ( basic || advanced ) ? "         -oy  t          - set  training  timeout  time  (t  in  seconds).  If\n" : "" );
    output << ( ( basic || advanced ) ? "                           training is not  completed after this  time then it\n" : "" );
    output << ( ( basic || advanced ) ? "                           will be stopped  prematurely.  t is a float.  Times\n" : "" );
    output << ( ( basic || advanced ) ? "                           less than 1 second will be interpretted unlimited. \n" : "" );
    output << ( ( basic || advanced ) ? "         -oY  t          - set  training  end time  (t in  seconds).  This  is\n" : "" );
    output << ( ( basic || advanced ) ? "                           absolute  time relative to  tnow().  Default  is -1\n" : "" );
    output << ( ( basic || advanced ) ? "                           (-1 means don't stop at absolute time).            \n" : "" );
    output << ( ( basic || advanced ) ? "         -oM  n          - size of  kernel cache  in  MB.  For unlimited  (aka\n" : "" );
    output << ( ( basic || advanced ) ? "                           1000000 rows) select -1 (default).                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -ofy            - turn on Cholesky factorisation fudging (that is, if\n" : "" );
    output << ( ( basic || advanced ) ? "                           Cholesky becomes  near singular  then  add a  small\n" : "" );
    output << ( ( basic || advanced ) ? "                           diagonal offset.  This is  arguably a bad idea, but\n" : "" );
    output << ( ( basic || advanced ) ? "                           sometimes it is necessary to make it work.         \n" : "" );
    output << ( ( basic || advanced ) ? "         -ofn            - turn off Cholesky factorisation fudging (default). \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -om  {a,s,d,g}  - optimisation method used.  Methods are:            \n" : "" );
    output << ( (          advanced ) ? "                           a - active set optimisation (default).             \n" : "" );
    output << ( (          advanced ) ? "                           s - SMO   optimisation   (only  valid   for  binary\n" : "" );
    output << ( (          advanced ) ? "                               classification, regression or single-class with\n" : "" );
    output << ( (          advanced ) ? "                               no tube shrinking).                            \n" : "" );
    output << ( (          advanced ) ? "                           d - D2C  optimisation   (only   valid   for  binary\n" : "" );
    output << ( (          advanced ) ? "                               classification, regression or single-class with\n" : "" );
    output << ( (          advanced ) ? "                               no tube shrinking).                            \n" : "" );
    output << ( (          advanced ) ? "                           g - gradient descent (using -ofa, -of... settings).\n" : "" );
    output << ( (          advanced ) ? "         -ofa {0,1,2,3}  - method for 4-norm optimisation:                    \n" : "" );
    output << ( (          advanced ) ? "                           0: simple gradient steps.                          \n" : "" );
    output << ( (          advanced ) ? "                           1: line-search gradient steps (default).           \n" : "" );
    output << ( (          advanced ) ? "                           2: simple Newton steps.                            \n" : "" );
    output << ( (          advanced ) ? "                           3: line-search Newton steps.                       \n" : "" );
    output << ( (          advanced ) ? "                           (2 and 3 are not recommended unless using -R q).   \n" : "" );
    output << ( (          advanced ) ? "         -ofe e          - tolerance e>=0 for 4-norm optim (default 0.005).   \n" : "" );
    output << ( (          advanced ) ? "         -ofm m          - Momentum factor for 4-norm optim (default 0.05).   \n" : "" );
    output << ( (          advanced ) ? "         -ofr t          - Learning rate for 4-norm optim (default 0.3).      \n" : "" );
    output << ( (          advanced ) ? "         -ofs s          - lr scaleback factor for 4-norm optim (default 0.8).\n" : "" );
    output << ( (          advanced ) ? "         -oft m          - max iterations for 4-norm optim (default 100).     \n" : "" );
    output << ( (          advanced ) ? "         -ofM n          - max 4-kernel cache for 4-norm optim (default 1000).\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- MLM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -omr r          - Learning rate for MLM optim (default 0.3).         \n" : "" );
    output << ( ( basic || advanced ) ? "         -ome d          - Stop when change in average error less than (0.02).\n" : "" );
    output << ( ( basic || advanced ) ? "         -oms s          - Initialisation sparsity (default 1).               \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Preload options (after training options):                                     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -pR             - reset the ML to initial state (keeps training dat).\n" : "" );
    output << ( (          advanced ) ? "         -pRR            - restart the ML (clean-slate, defaults, no data).   \n" : "" );
    output << ( (          advanced ) ? "         -pr  i          - remove training vector i.                          \n" : "" );
    output << ( (          advanced ) ? "         -pro n          - remove the first n training vectors.               \n" : "" );
    output << ( (          advanced ) ? "         -psz i z        - set target z for training vector i.                \n" : "" );
    output << ( (          advanced ) ? "         -pcw i C        - set C>0 weight for training vector i.              \n" : "" );
    output << ( (          advanced ) ? "         -pcs s          - scale all preloaded C weights (t/TVALs) by s>0.    \n" : "" );
    output << ( (          advanced ) ? "         -pww i eps      - set eps>=0 weight for training vector i.           \n" : "" );
    output << ( (          advanced ) ? "         -pws s          - scale all preloaded eps weight (e/EVALs) by s>=0.  \n" : "" );
    output << ( (          advanced ) ? "         -ps  s          - scale whole  ML by s>=0.  The exact meaning of this\n" : "" );
    output << ( (          advanced ) ? "                           depends on the ML type as follows:                 \n" : "" );
    output << ( (          advanced ) ? "         -pS             - scale to ensure that abs2(alpha) = 1.              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           SVM: alpha and b are scaled by s.                  \n" : "" );
    output << ( (          advanced ) ? "                           LSV: like SVM.                                     \n" : "" );
    output << ( (          advanced ) ? "                           GPR: y and K (kernel weight) are scaled by s.      \n" : "" );
    output << ( (          advanced ) ? "                           BLK consensus: scale all parts.                    \n" : "" );
    output << ( (          advanced ) ? "                           Others: may or may not be implemented, see code.   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fic            - fill caches. This can be handy if you have a Mercer\n" : "" );
    output << ( (          advanced ) ? "                           kernel  inheritance block  (kernel cache)  that you\n" : "" );
    output << ( (          advanced ) ? "                           want to fill for a specific setting before modding.\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -pk  $file      - load kernel matrix from file (bypass kernel).  File\n" : "" );
    output << ( (          advanced ) ? "                           format is  MATLAB style matrix, []s  must included.\n" : "" );
    output << ( (          advanced ) ? "                           This may be handy for  multi-instance learning with\n" : "" );
    output << ( (          advanced ) ? "                           many (1000s) of instances  in some examples, as for\n" : "" );
    output << ( (          advanced ) ? "                           reasons unknown this can crash during kernel eval. \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -prz            - remove non-support vectors.                        \n" : "" );
    output << ( (          advanced ) ? "         -prm n          - like  -prz, but  continue removing  support vectors\n" : "" );
    output << ( (          advanced ) ? "                           until the number of vectors reaches at max n.      \n" : "" );
    output << ( (          advanced ) ? "         -psd i d        - set class d for training vector i.                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- GP specific options                           --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -pdw i C        - set C>0 weight for training vector i.              \n" : "" );
    output << ( (          advanced ) ? "         -pds s          - scale all preloaded C weights (t/TVALs) by s>0.    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Load options (adding training vectors, after preload options):                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -AA  $file      - add vectors from fname.                            \n" : "" );
    output << ( ( basic || advanced ) ? "         -AN  i j k $file -add vectors from file, ignoring i vectors at start,\n" : "" );
    output << ( ( basic || advanced ) ? "                           adding at  most j vectors  (-1 if all),  and adding\n" : "" );
    output << ( ( basic || advanced ) ? "                           vectors starting from index k (-1 to add to end).  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -AU  d [x]      - add training vector x,  target d.  For single-class\n" : "" );
    output << ( (          advanced ) ? "                           ML, target d is ignored  but must still be present.\n" : "" );
    output << ( (          advanced ) ? "                           x must be an (optionally sparse) enclosed vector.  \n" : "" );
    output << ( (          advanced ) ? "                           eg -AU -1 [ 0:1 1:-1 ]                             \n" : "" );
    output << ( (          advanced ) ? "                           or, in non-sparse notation, -AU -1 [ 1 -1 ]        \n" : "" );
    output << ( (          advanced ) ? "         -AY  d x        - like -AU, but x is any vector.                     \n" : "" );
    output << ( (          advanced ) ? "         -AZ  d x m      - like -AU, but x is any vector and ML m.            \n" : "" );
    output << ( (          advanced ) ? "         -AV  [d] [[x]]  - add multiple training vectors, eg                  \n" : "" );
    output << ( (          advanced ) ? "                     -AV [ -1 -1 1 1 ] [ [ -1 -1 ] [ 1 1 ] [ 1 -1 ] [ -1 1 ] ]\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -AGl l          - Set lower bound (scalar or vector) for -AG, -AGc). \n" : "" );
    output << ( (          advanced ) ? "         -AGu u          - Set upper bound (scalar or vector) for -AG, -AGc). \n" : "" );
    output << ( (          advanced ) ? "         -Ag  N d f v    - generate and add training data.  N pairs generated,\n" : "" );
    output << ( (          advanced ) ? "                           vectors have dim d, function is f with noise var v,\n" : "" );
    output << ( (          advanced ) ? "                           features N(0,1).                                   \n" : "" );
    output << ( (          advanced ) ? "         -AG  N d f v    - generate and add training data.  N pairs generated,\n" : "" );
    output << ( (          advanced ) ? "                           vectors have dim d, function is f with noise var v,\n" : "" );
    output << ( (          advanced ) ? "                           features U(l,u).                                   \n" : "" );
    output << ( (          advanced ) ? "         -Agc N d f v c  - like -Ag, but only adds data when c(x) == 1.       \n" : "" );
    output << ( (          advanced ) ? "         -AGc N d f v c  - like -AG, but only adds data when c(x) == 1.       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -AVv d x        - add multiple training vectors with  target vector d\n" : "" );
    output << ( (          advanced ) ? "                           and input  vectors x, eg  -AVv var(81,0)  var(80,0)\n" : "" );
    output << ( (          advanced ) ? "                           after -tx will train a model on LOO error.         \n" : "" );
    output << ( (          advanced ) ? "         -AVV d x s      - add multiple training vectors with target vector d,\n" : "" );
    output << ( (          advanced ) ? "                           input vectors x  and sigma weights s.  For  example\n" : "" );
    output << ( (          advanced ) ? "                           you might use var(83,0) in the above example.      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** The following  suffixes modify  the behaviour **         \n" : "" );
    output << ( (          advanced ) ? "                  ** of -AA and -AN (among others):                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** e: For target-at-end format files, the suffix **         \n" : "" );
    output << ( (          advanced ) ? "                  **    can be used.  So for example:              **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -AA  trainfile   (target-at-start format)  **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -AAe trainfile   (target-at-end format)    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** i: add training vectors  from open files, use **         \n" : "" );
    output << ( (          advanced ) ? "                  **    the i suffix, replace  the filename with n **         \n" : "" );
    output << ( (          advanced ) ? "                  **    (the file number).                         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** r: To add training  vectors randomly  from an **         \n" : "" );
    output << ( (          advanced ) ? "                  **    open file, use the r suffix instead of i.  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** I/R: To add training  vectors from open files **         \n" : "" );
    output << ( (          advanced ) ? "                  **    but leave them so  they can be reused, use **         \n" : "" );
    output << ( (          advanced ) ? "                  **    the I/R suffixes (just like i/r suffixes). **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** l: labels/targets  may not always  be present **         \n" : "" );
    output << ( (          advanced ) ? "                  **    training files, the l sufix can be used to **         \n" : "" );
    output << ( (          advanced ) ? "                  **    apply a  label to  all vectors  in an  un- **         \n" : "" );
    output << ( (          advanced ) ? "                  **    labelled   training  file   (ie.   a  file **         \n" : "" );
    output << ( (          advanced ) ? "                  **    containing only x).  General format is:    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    {usual flag}l {usual options} y            **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    where y is the required class/target.  eg. **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -AA trainfile   becomes -AAl trainfile y   **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -ANe i j k fle  becomes -ANel i j k fle y  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** u: If the training file has labels that arent **         \n" : "" );
    output << ( (          advanced ) ? "                  **    needed (for example,  you want to  train a **         \n" : "" );
    output << ( (          advanced ) ? "                  **    1-class ML using a labelled training file) **         \n" : "" );
    output << ( (          advanced ) ? "                  **    then the u suffix  can be used  to strip / **         \n" : "" );
    output << ( (          advanced ) ? "                  **    ignore the labels.  The general format is: **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    {usual flag}u {usual options}              **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    For example:                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -AN i j k fle   becomes -ANu i j k fle     **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -AAe trainfile  becomes -AAeu trainfile    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** NB: the l  suffix  is  incompatible  with the **         \n" : "" );
    output << ( (          advanced ) ? "                  **     1-class  ML,  and  the u  suffix is  only **         \n" : "" );
    output << ( (          advanced ) ? "                  **     compatible with the 1-class ML.           **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** The complete list of suffixed options are:    **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -AAe,-AAi,-AAl,-AAu,-AAel,-AAeu,-AAil,-AAiu,  **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -ANe,-ANi,-ANr, -ANl,-ANu,-ANel, -ANeu,-ANil, **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -ANiu, -ANrl, -ANru and others.               **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Ad  n          - set target space dimension (vector regression).    \n" : "" );
    output << ( (          advanced ) ? "         -AD  n          - set target order (anionic regression: 0,1,2,...).  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Rudimentary  transductive learning  supported **         \n" : "" );
    output << ( (          advanced ) ? "                  ** by the following  functions.  Given a dataset **         \n" : "" );
    output << ( (          advanced ) ? "                  ** (labels ignored) points  are classified using **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the ML and added to the  training set if g(x) **         \n" : "" );
    output << ( (          advanced ) ? "                  ** exceeds a given the threshold (set by -ATb).  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** If anomaly detection is  switched on, and the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** trigger level (-ATn) is positive then if more **         \n" : "" );
    output << ( (          advanced ) ? "                  ** than this trigger level of anomalies are det- **         \n" : "" );
    output << ( (          advanced ) ? "                  ** ected  then a  new class  is created  and the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** detected  anomalies  satisfying  the distance **         \n" : "" );
    output << ( (          advanced ) ? "                  ** requirement placed in this class.             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** File format is  the same as for  -AA etc, but **         \n" : "" );
    output << ( (          advanced ) ? "                  ** note  that labels  will always be  ignored if **         \n" : "" );
    output << ( (          advanced ) ? "                  ** present.                                      **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -ATA $file      - transductively add vectors from file (see -AA).    \n" : "" );
    output << ( (          advanced ) ? "         -ATN i j k $file -transductively add vectors from file (see -AN).    \n" : "" );
    output << ( (          advanced ) ? "         -ATb d          - distance from classification  boundary required for\n" : "" );
    output << ( (          advanced ) ? "                           point  to be  transductively  added  to that  class\n" : "" );
    output << ( (          advanced ) ? "                           (default: 1).                                      \n" : "" );
    output << ( (          advanced ) ? "         -ATa d          - trigger  distance (distance  point must  lie inside\n" : "" );
    output << ( (          advanced ) ? "                           anonaly class to be counted, default 1).           \n" : "" );
    output << ( (          advanced ) ? "         -ATn N          - set trigger level  for anomalies  (num of anomalies\n" : "" );
    output << ( (          advanced ) ? "                           anomalies required  before they  can be grouped and\n" : "" );
    output << ( (          advanced ) ? "                           incorporated as a new class (default 0, disabled). \n" : "" );
    output << ( (          advanced ) ? "         -ATx d          - class  label  to be  used if  anomaly  class  added\n" : "" );
    output << ( (          advanced ) ? "                           (default 0, disabled).                             \n" : "" );
    output << ( (          advanced ) ? "         -ATy c          - method control for transduction:                   \n" : "" );
    output << ( (          advanced ) ? "                           0: add no training vectors.                        \n" : "" );
    output << ( (          advanced ) ? "                           1: add only non-anomalies satisfying distance -ATb.\n" : "" );
    output << ( (          advanced ) ? "                           2: add only anomalies  satisfying distance -ATa (if\n" : "" );
    output << ( (          advanced ) ? "                              new  class  creation   is  enabled  and  trigger\n" : "" );
    output << ( (          advanced ) ? "                              condition met, otherwise don't add).            \n" : "" );
    output << ( (          advanced ) ? "                           3: combination of methods 1 and 2 (default).       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Aq  n m v      - add n random features N(m,v) to all x.             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- gentype basis definition options (\"u\" above)  --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Aby            - set basis equal to training targets.               \n" : "" );
    output << ( ( basic || advanced ) ? "         -Abu            - set basis equal to user defined values.            \n" : "" );
    output << ( ( basic || advanced ) ? "         -AeA $file      - add basis elements from fname.                     \n" : "" );
    output << ( ( basic || advanced ) ? "         -AeU f          - add basis element f.                               \n" : "" );
    output << ( ( basic || advanced ) ? "         -AeR n d        - set basis of n elements, each a random, 1-norm unit\n" : "" );
    output << ( ( basic || advanced ) ? "                           vector of dimension d.                             \n" : "" );
    output << ( ( basic || advanced ) ? "         -Ar  i          - remove basis element i.                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- planar basis definition options (\"v\" above)   --         \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- (also multi-expert ranking)                   --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -ABy            - set basis equal to training targets.               \n" : "" );
    output << ( ( basic || advanced ) ? "         -ABu            - set basis equal to user defined values.            \n" : "" );
    output << ( ( basic || advanced ) ? "         -AEA $file      - add basis elements from fname.                     \n" : "" );
    output << ( ( basic || advanced ) ? "         -AEU f          - add basis element f.                               \n" : "" );
    output << ( ( basic || advanced ) ? "         -AER n d        - set basis of n elements, each a random, 1-norm unit\n" : "" );
    output << ( ( basic || advanced ) ? "                           vector of dimension d.                             \n" : "" );
    output << ( ( basic || advanced ) ? "         -AR  i          - remove basis element i.                            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- MEX (Matlab) only options                     --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -AW  $yvar $xvar -get training data from  matlab variables.  xvar and\n" : "" );
    output << ( (          advanced ) ? "                           yvar  have n  rows,  each of  which  is a  training\n" : "" );
    output << ( (          advanced ) ? "                           vector.  yvar is not used  in the single-class case\n" : "" );
    output << ( (          advanced ) ? "                           but must still be present.                         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                   [ y1 ]         [ x1' ]   [ x11 x12 .. x1d ]\n" : "" );
    output << ( (          advanced ) ? "                           yvar =  [ y2 ], xvar = [ x2' ] = [ x2d x2d .. x2d ]\n" : "" );
    output << ( (          advanced ) ? "                                   [ .. ]         [ ... ]   [                ]\n" : "" );
    output << ( (          advanced ) ? "                                   [ yn ]         [ xn' ]   [ xnd xnd .. xnd ]\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Ac  d          - add class to SVM  (multiclass only)  without adding\n" : "" );
    output << ( ( basic || advanced ) ? "                           any vectors from that class.                       \n" : "" );
    output << ( (          advanced ) ? "         -Acz d          - like -Ac d, but sets epsilon = 0 for this component\n" : "" );
    output << ( (          advanced ) ? "                           if recursive division or max wins multiclass used. \n" : "" );
    output << ( (          advanced ) ? "         -Aca d nu       - add anomaly detector to multiclass SVM with label d\n" : "" );
    output << ( (          advanced ) ? "                           and anomaly detection parameter nu.                \n" : "" );
    output << ( (          advanced ) ? "         -Acd            - remove anomaly detector.                           \n" : "" );
    output << ( (          advanced ) ? "         -As  n          - set single-class  SVM non-anomaly  label (+1 or -1,\n" : "" );
    output << ( (          advanced ) ? "                           +1 by defaulg).                                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Post-load options (after adding training vectors) options:                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Snx            - remove any existing data normalisation.            \n" : "" );
    output << ( ( basic || advanced ) ? "         -Sna            - normalise data features to zero mean, unit var.    \n" : "" );
    output << ( ( basic || advanced ) ? "         -Snb            - normalise data features to zero median, unit var.  \n" : "" );
    output << ( ( basic || advanced ) ? "         -Snc            - normalise data features to range 0,1.              \n" : "" );
    output << ( ( basic || advanced ) ? "         -SNa            - like -Sna, but no shifting applied.                \n" : "" );
    output << ( ( basic || advanced ) ? "         -SNb            - like -Snb, but no shifting applied.                \n" : "" );
    output << ( ( basic || advanced ) ? "         -SNc            - like -Snc, but no shifting applied.                \n" : "" );
    output << ( ( basic || advanced ) ? "         -SnA            - like -Sna, but applies min scale to all elements.  \n" : "" );
    output << ( ( basic || advanced ) ? "         -SnB            - like -Snb, but applies min scale to all elements.  \n" : "" );
    output << ( ( basic || advanced ) ? "         -SnC            - like -Snc, but applies min scale to all elements.  \n" : "" );
    output << ( ( basic || advanced ) ? "         -SNA            - like -Sna, but no shift and min scale.             \n" : "" );
    output << ( ( basic || advanced ) ? "         -SNB            - like -Snb, but no shift and min scale.             \n" : "" );
    output << ( ( basic || advanced ) ? "         -SNC            - like -Snc, but no shift and min scale.             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Sra f          - random alpha/weight initialisation with sparsity f.\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  ** NOTES: - normalisation done by shift+scale in **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          the kernel, so  the same changes are **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          automatically  applied to all future **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          training/testing data.               **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **        - calculated on per-feature basis.     **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **        - does not  apply to  categorical, set **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          or graph valued features.            **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **        - vector-valued  features  are  normed **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          to  zero  mean, unit  covar  matrix. **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          Hence  normalisation can  be used to **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          implement  the  Mahalanobis norm  if **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          data  is given as  a single  vector- **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          valued feature - ie. in the training **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **          file use [ x ] rather than x.        **         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- Sampling options                              --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Stl l          - Set lower bound vector for sampling (deft [ 0 ]).  \n" : "" );
    output << ( ( basic || advanced ) ? "         -Stu u          - Set upper bound vector for sampling (deft [ 1 ]).  \n" : "" );
    output << ( ( basic || advanced ) ? "         -StN N          - Set number of  gridpoints for sampling (deft  100).\n" : "" );
    output << ( ( basic || advanced ) ? "                           Use negative value for  random (non-grid) sampling.\n" : "" );
    output << ( ( basic || advanced ) ? "                           Use 0 for  auto (-10jd^2, where j is  the number of\n" : "" );
    output << ( ( basic || advanced ) ? "                           samples already in model and d is the dimension, as\n" : "" );
    output << ( ( basic || advanced ) ? "                           per Kandasami's TS-BO paper) random samples.       \n" : "" );
    output << ( ( basic || advanced ) ? "         -Sts s          - Set  split  for  sampling  (deft 0).  If  > 1  then\n" : "" );
    output << ( ( basic || advanced ) ? "                           sampling actually gives an s-kernel g([x1 ~ x2 ~...\n" : "" );
    output << ( ( basic || advanced ) ? "                           xs]), where [ x1 ; x2 ; ... ; xs ] \\in [l,u].      \n" : "" );
    output << ( ( basic || advanced ) ? "         -Stx s            Scheme for sampling x points.  Options are:        \n" : "" );
    output << ( ( basic || advanced ) ? "                           0 - \"True\" pseudo-random (default).                \n" : "" );
    output << ( ( basic || advanced ) ? "                           1 - pre-defined   sequence   of   random   samples,\n" : "" );
    output << ( ( basic || advanced ) ? "                               generated sequentially.                        \n" : "" );
    output << ( ( basic || advanced ) ? "                           2 - pre-defined  sequence of  random samples,  same\n" : "" );
    output << ( ( basic || advanced ) ? "                               everytime.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                           3 - grid of Nsamp^dim samples                      \n" : "" );
    output << ( ( basic || advanced ) ? "         -Stt t          - Set type for sampling (deft 0).  Types are:        \n" : "" );
    output << ( ( basic || advanced ) ? "                           0 - unbounded draw.                                \n" : "" );
    output << ( ( basic || advanced ) ? "                           1 - positive (definite/symm) draw by clip max(0,y).\n" : "" );
    output << ( ( basic || advanced ) ? "                           2 - positive (definite/symm) draw by flip |y|.     \n" : "" );
    output << ( ( basic || advanced ) ? "                           3 - negative (definite/symm) draw by clip min(0,y).\n" : "" );
    output << ( ( basic || advanced ) ? "                           4 - negative (definite/symm) draw by flip -|-y|.   \n" : "" );
    output << ( ( basic || advanced ) ? "                           5 - unbounded (symmetric) draw.                    \n" : "" );
    output << ( ( basic || advanced ) ? "                           1x - As above, but force alpha >= 0 after sample.  \n" : "" );
    output << ( ( basic || advanced ) ? "                           2x - As above, but force alpha <= 0 after sample.  \n" : "" );
    output << ( ( basic || advanced ) ? "                           1xx - As above, but square the function afterwards.\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Existing observations treated as observations\n" : "" );
    output << ( ( basic || advanced ) ? "                                 on the function before squaring.             \n" : "" );
    output << ( ( basic || advanced ) ? "                           2xx - Like 1xx,  but existing observations  treated\n" : "" );
    output << ( ( basic || advanced ) ? "                                 as observations on the squared function.     \n" : "" );
    output << ( ( basic || advanced ) ? "                           3xx - Like  2xx, but  return the  function *before*\n" : "" );
    output << ( ( basic || advanced ) ? "                                 squaring occurs.                             \n" : "" );
    output << ( ( basic || advanced ) ? "         -Stc s          - Set variance scale for draw from GP(m,s.cov).      \n" : "" );
    output << ( ( basic || advanced ) ? "         -Stq s          - Set slack for draw (expands  lower bounds on x from\n" : "" );
    output << ( ( basic || advanced ) ? "                           xmin to -s.xmax.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "         -St             - Take sample from whatever prior distribution the ML\n" : "" );
    output << ( ( basic || advanced ) ? "                           represents using a grid  of N samples on x ~ [l,u].\n" : "" );
    output << ( ( basic || advanced ) ? "                           For  example to  sample from  the posterior  of the\n" : "" );
    output << ( ( basic || advanced ) ? "                           current GP and put in in ML i you might use:       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           -Zx -qc i h -qw i -St [ 0 0 ] [ 1 1 ] 200 1        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           NB: when inequality observations or observations on\n" : "" );
    output << ( ( basic || advanced ) ? "                           g(x)^2  are present then  g(x) is drawn  with these\n" : "" );
    output << ( ( basic || advanced ) ? "                           removed and  h(x) constructed  such that  g(x)+h(x)\n" : "" );
    output << ( ( basic || advanced ) ? "                           satisfies all (trivial and nontrivial) constraints.\n" : "" );
    output << ( ( basic || advanced ) ? "         -Spt            - Set up for -St, but don't actually take sample.    \n" : "" );
    output << ( ( basic || advanced ) ? "         -Sjt            - JIT variant of -St.  When the  model is evaluated a\n" : "" );
    output << ( ( basic || advanced ) ? "                           sample is taken at that  point (unless the variance\n" : "" );
    output << ( ( basic || advanced ) ? "                           at that point is already low). This is functionally\n" : "" );
    output << ( ( basic || advanced ) ? "                           equivalent to a fine grid but faster.              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Sjt+           - Like -Sjt, but forces positive samples.            \n" : "" );
    output << ( ( basic || advanced ) ? "         -Sjt-           - Like -Sjt, but forces negative samples.            \n" : "" );
    output << ( ( basic || advanced ) ? "         -Snt            - Reverse of -St, if that makes sense in context.    \n" : "" );
    output << ( ( basic || advanced ) ? "         -Svc sigscale   - Sigma scale for JIT sampling.                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Sx  f          - Target transform: y -> f(y).                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- BLK (ker) specific options                    --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -SA  $file      - load lambda matrix from $file (size N*m).          \n" : "" );
    output << ( ( basic || advanced ) ? "         -SAi i q a      - set lambda[i,q].                                   \n" : "" );
    output << ( ( basic || advanced ) ? "         -SAA [ a ]      - set lambda matrix (size N*m).                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- BLK (mba) specific options                    --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Sdi i          - for mba only, disable vector i for top model only. \n" : "" );
    output << ( ( basic || advanced ) ? "         -SdI i j        - for mba only, like -Sdi [ i,i+1,...,j].            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Sa  $file      - load alpha from $file.                             \n" : "" );
    output << ( ( basic || advanced ) ? "         -Sai i a        - set alpha[i].                                      \n" : "" );
    output << ( ( basic || advanced ) ? "         -Saa [ a ]      - set alpha.                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Sb  $file      - load bias from $file.                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           (NB: must be in raw format for recursive division) \n" : "" );
    output << ( ( basic || advanced ) ? "         -Sbb b          - set bias                                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Learning options (after post-load modifications):                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -c   CN         - Set  standard ML  tradeoff parameter  C/N = CN >= 0\n" : "" );
    output << ( ( basic || advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( ( basic || advanced ) ? "         -d   lambda     - Set N.lambda, which is N/C (default 1)             \n" : "" );
    output << ( ( basic || advanced ) ? "         -ccs lambda     - (alias for -d).                                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  ** NB: in this code, C/N  is the upper bound for **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **     |alpha| (unless  using quadratic cost, of **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **     course, but in this  case similar changes **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **     apply), whereas  elsewhere (SVMlight, for **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **     example)  this upper bound is  denoted C. **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **     C may mean different  things in different **         \n" : "" );
    output << ( ( basic || advanced ) ? "                  **     ML blocks.                                **         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -c+  s          - classification: C scale factor s>0 for class d = +1\n" : "" );
    output << ( ( basic || advanced ) ? "                           regression:  C   scale  factor  for   lower  bounds\n" : "" );
    output << ( ( basic || advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( ( basic || advanced ) ? "         -c-  s          - classification: C scale factor s>0 for class d = -1\n" : "" );
    output << ( ( basic || advanced ) ? "                           regression:  C   scale  factor  for   upper  bounds\n" : "" );
    output << ( ( basic || advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( (          advanced ) ? "         -c=  s          - regression:  C  scale  factor  s>0  for  equalities\n" : "" );
    output << ( (          advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( (          advanced ) ? "         -cd  d s        - classification: C  scale  factor s>0  for  class d.\n" : "" );
    output << ( (          advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( (          advanced ) ? "         -cs  s          - Scale C/N by s>0.                                  \n" : "" );
    output << ( (          advanced ) ? "         -c+s s          - Scale c+ by s>0.                                   \n" : "" );
    output << ( (          advanced ) ? "         -c-s s          - Scale c- by s>0.                                   \n" : "" );
    output << ( (          advanced ) ? "         -c=s s          - Scale c= by s>0.                                   \n" : "" );
    output << ( (          advanced ) ? "         -cds d s        - Scale cd by s>0.                                   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -cw  i w        - set C weight w>0 for training vector i.            \n" : "" );
    output << ( (          advanced ) ? "         -ww  i w        - set eps weight w>=0 for training vector i.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -mvb beta       - multi-ranking spread regularisation term (deflt 1).\n" : "" );
    output << ( (          advanced ) ? "         -mvi m          - maximum iterations m>=0 for multi-ranking outerloop\n" : "" );
    output << ( (          advanced ) ? "                           training (0 for unlimited - default).              \n" : "" );
    output << ( (          advanced ) ? "         -mvlr r         - multi-ranking learning rate r>0 (default 0.3).     \n" : "" );
    output << ( (          advanced ) ? "         -mvzt t         - multi-ranking zero tolerance t>=0 (default 0.01).  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -m   {r,m}      - hessian element type:                              \n" : "" );
    output << ( (          advanced ) ? "                           r - real-valued (default).                         \n" : "" );
    output << ( (          advanced ) ? "                           m - matrix/anion  valued  (vector/anion  regression\n" : "" );
    output << ( (          advanced ) ? "                               with  non-real  kernel  and  non-real  training\n" : "" );
    output << ( (          advanced ) ? "                               vectors).                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -mu  {0,1,2}    - Prior mean type:                                   \n" : "" );
    output << ( ( basic || advanced ) ? "                           0 - prior mean 0 (default).                        \n" : "" );
    output << ( ( basic || advanced ) ? "                           1 - prior mean mu(x) specified directly.           \n" : "" );
    output << ( ( basic || advanced ) ? "                           2 - prior mean g(x) inherited from ML.             \n" : "" );
    output << ( ( basic || advanced ) ? "         -mugt mu        - Prior mean function of var(0,i), i=0,1,... (-mu 1).\n" : "" );
    output << ( ( basic || advanced ) ? "         -muml ml        - Prior mean ML number (-mu 2).                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -w   eps        - epsilon: sets  width of epsilon  tube/insensitivity\n" : "" );
    output << ( ( basic || advanced ) ? "                           eps>=0 for  regression  or  boundary  distance  for\n" : "" );
    output << ( ( basic || advanced ) ? "                           classification or  single class  (default 0.001 for\n" : "" );
    output << ( ( basic || advanced ) ? "                           regression, 1 for  classification/single class, 0.1\n" : "" );
    output << ( ( basic || advanced ) ? "                           for cyclic regression.                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           ASIDE: setting eps = -|z| you  can have constraints\n" : "" );
    output << ( ( basic || advanced ) ? "                                  of the form g(x) < y-|z| or g(x) > y+|z|.   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -w+  s          - classification: epsilon scale s>=0 for class d = +1\n" : "" );
    output << ( ( basic || advanced ) ? "                           regression:  epsilon scale factor  for lower bounds\n" : "" );
    output << ( ( basic || advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( ( basic || advanced ) ? "         -w-  s          - classification: epsilon scale s>=0 for class d = -1\n" : "" );
    output << ( ( basic || advanced ) ? "                           regression:  epsilon scale factor  for upper bounds\n" : "" );
    output << ( ( basic || advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( (          advanced ) ? "         -w=  s          - regression:  epsilon  scale   s>=0  for  equalities\n" : "" );
    output << ( (          advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( (          advanced ) ? "         -wd  d s        - classification: epsilon  scale  s>=0  for  class d.\n" : "" );
    output << ( (          advanced ) ? "                           (default 1).                                       \n" : "" );
    output << ( (          advanced ) ? "         -ws  s          - Scale eps by s>=0.                                 \n" : "" );
    output << ( (          advanced ) ? "         -w+s s          - Scale eps+ by s>=0.                                \n" : "" );
    output << ( (          advanced ) ? "         -w-s s          - Scale eps- by s>=0.                                \n" : "" );
    output << ( (          advanced ) ? "         -w=s s          - Scale eps= by s>=0.                                \n" : "" );
    output << ( (          advanced ) ? "         -wds d s        - Scale epsd by s>=0.                                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -j   s          - equivalent to -c- 1 -c+ s.                         \n" : "" );
    output << ( (          advanced ) ? "         -jc  s          - equivalent to -c- 1 -c+ s.                         \n" : "" );
    output << ( (          advanced ) ? "         -jw  s          - equivalent to -w- 1 -w+ s.                         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Tl  s          - linear tube shrinking value s>=0 (if used, dflt 0).\n" : "" );
    output << ( (          advanced ) ? "         -Tq  s          - quadratic tube shrinking vl s>=0 (if used, dflt 0).\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Bf  b          - fixed bias  used for  fixed bias case  (default 0).\n" : "" );
    output << ( (          advanced ) ? "                           For the vectorial case must be in raw format.      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Mn             - set no monotonicity constraints (default).         \n" : "" );
    output << ( (          advanced ) ? "         -Mi             - set monotonicity constraint increasing.            \n" : "" );
    output << ( (          advanced ) ? "         -Md             - set monotonicity constraint decreasing.            \n" : "" );
    output << ( (          advanced ) ? "                           (these constraints  are sufficient,  not necessary,\n" : "" );
    output << ( (          advanced ) ? "                           and  only  apply for  a  few  kernels  with  finite\n" : "" );
    output << ( (          advanced ) ? "                           dimensional feature maps.  They further assume that\n" : "" );
    output << ( (          advanced ) ? "                           all training vectors are elementwise non-negative).\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Nl  f          - linear bias forcing term f, 1-class case (dflt 0). \n" : "" );
    output << ( (          advanced ) ? "         -Nq  f          - quadratic bias forcing f>=0, 1-class case (dft 0). \n" : "" );
    output << ( (          advanced ) ? "         -Nld d f        - linear bias forcing term f, multi case (dflt 0).   \n" : "" );
    output << ( (          advanced ) ? "         -Nqd d f        - quadratic bias forcing f>=0, multi case (deft 0).  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -dd  d          - sets d in classify-with-reject  (binary and reduce-\n" : "" );
    output << ( (          advanced ) ? "                           to-binary multiclas only). Recommend combining with\n" : "" );
    output << ( (          advanced ) ? "                           -w 1 (defaults) or this  might not work  as you may\n" : "" );
    output << ( (          advanced ) ? "                           expect. For reduce-to-binary multiclass rejects are\n" : "" );
    output << ( (          advanced ) ? "                           labelled as anomalies if anomaly detection is used,\n" : "" );
    output << ( (          advanced ) ? "                           overriding the standard anomaly detector.  See:    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           o Bartlett et al \"Classification with Reject Option\n" : "" );
    output << ( (          advanced ) ? "                             using Hinge Loss\", JMLR 2008.                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -nm  m          - norm  used   when  calculating  margin   (any  even\n" : "" );
    output << ( (          advanced ) ? "                           positive  integer  is  allowed, but  training  time\n" : "" );
    output << ( (          advanced ) ? "                           increases drastically for m > 4.  In practice m = 4\n" : "" );
    output << ( (          advanced ) ? "                           is feasible for  moderate datasets,  m > 4 only for\n" : "" );
    output << ( (          advanced ) ? "                           small (toy) datasets).                             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Notes: - suggest  using -om d  for speed  here (D2C\n" : "" );
    output << ( (          advanced ) ? "                           (m>=4)   won't actually get  used, but it stops the\n" : "" );
    output << ( (          advanced ) ? "                                    optimiser   attempting   to   maintain   a\n" : "" );
    output << ( (          advanced ) ? "                                    Cholesky factorisation of the Hessian).   \n" : "" );
    output << ( (          advanced ) ? "                                  - use of  -knn may  be required to  keep the\n" : "" );
    output << ( (          advanced ) ? "                                    Hessian from exploding.                   \n" : "" );
    output << ( (          advanced ) ? "                                  - large C values should be avoided.         \n" : "" );
    output << ( (          advanced ) ? "                                  - most kernels are fine up to about N = 250,\n" : "" );
    output << ( (          advanced ) ? "                                    but  suggest  using  -kan 2  -mtb  with  a\n" : "" );
    output << ( (          advanced ) ? "                                    distance based kernel form m > 4 to enable\n" : "" );
    output << ( (          advanced ) ? "                                    inner-product cacheing (or K4 pre-cacheing\n" : "" );
    output << ( (          advanced ) ? "                                    may become prohibitive).                  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Fi  m          - max iterations m>=0 for iterative fuzzy training (0\n" : "" );
    output << ( (          advanced ) ? "                           for unlimited - default).                          \n" : "" );
    output << ( (          advanced ) ? "         -Flr r          - iterative fuzzy learning rate r>0 (default 0.3).   \n" : "" );
    output << ( (          advanced ) ? "         -Fzt t          - fuzzy zero tolerance t>=0 (default 0.01).          \n" : "" );
    output << ( (          advanced ) ? "         -Fc  $fn        - generalised cost fn of var(0,0) (default tanh(x)). \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -th  theta      - set theta (psd regularisation) for similarity.     \n" : "" );
    output << ( (          advanced ) ? "         -thn {0,1}      - set normalised (1, deft) or unnormal similarity.   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SVM Random Feature Specific Options           --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           Random fourier features follows Rahini and Retch if\n" : "" );
    output << ( ( basic || advanced ) ? "                           feature tuning is turned off.  Otherwise the random\n" : "" );
    output << ( ( basic || advanced ) ? "                           features are selected from a reference distribution\n" : "" );
    output << ( ( basic || advanced ) ? "                           (gaussian) with length-scale as specified, and then\n" : "" );
    output << ( ( basic || advanced ) ? "                           the scaling  function is chosen with  an additional\n" : "" );
    output << ( ( basic || advanced ) ? "                           regularisation term 1/2D  ||u-1||^2, and the kernel\n" : "" );
    output << ( ( basic || advanced ) ? "                           defines the RKHS from which the density function is\n" : "" );
    output << ( ( basic || advanced ) ? "                           selected.                                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -nN  m          - number of random fourier features used.            \n" : "" );
    output << ( ( basic || advanced ) ? "         -trf {0,1,-1,2} - turn random fourier feature tuning on/off (dflt 2):\n" : "" );
    output << ( ( basic || advanced ) ? "                           0:  no tuning.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                           1:  tuning with positive density (+ve def kernel). \n" : "" );
    output << ( ( basic || advanced ) ? "                           -1: tuning with negative density (-ve def kernel). \n" : "" );
    output << ( ( basic || advanced ) ? "                           2:  tuning with general density (Krein kernel).    \n" : "" );
    output << ( ( basic || advanced ) ? "         -tmv mv         - set minimum v for regularisation.                  \n" : "" );
    output << ( ( basic || advanced ) ? "         -trk k          - set k parameter for PEGASOS  optimisation (dflt -1,\n" : "" );
    output << ( ( basic || advanced ) ? "                           which is use all).                                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -dc  D          - Set regul 1/Lambda for feature weights (deft 1).   \n" : "" );
    output << ( ( basic || advanced ) ? "         -ec  E          - Set regul 1/gamma for feature weights (deft 1).    \n" : "" );
    output << ( ( basic || advanced ) ? "         -fc  F          - Set F in F/2 ( v'.1 - G )^2 term (deft 0).         \n" : "" );
    output << ( ( basic || advanced ) ? "         -Gc  G          - Set G in F/2 ( v'.1 - G )^2 term (deft 0).         \n" : "" );
    output << ( ( basic || advanced ) ? "         -dcs sD         - Set regul Lambda for feature weights (deft 1).     \n" : "" );
    output << ( ( basic || advanced ) ? "         -ecs sE         - Set regul gamma for feature weights (deft 1).      \n" : "" );
    output << ( ( basic || advanced ) ? "         -fcs sF         - Set 1/F in F/2 ( v'.1 - G )^2 term (deft inf).     \n" : "" );
    output << ( ( basic || advanced ) ? "         -Gcs sG         - Set 1/G in F/2 ( v'.1 - G )^2 term (deft inf).     \n" : "" );
    output << ( ( basic || advanced ) ? "         -rfs {0,1}      - Define type of random fourier features:            \n" : "" );
    output << ( ( basic || advanced ) ? "                           0 - features have form 1/sqrt(m) [ cos sin ].      \n" : "" );
    output << ( ( basic || advanced ) ? "                           1 - features have form 1/sqrt(m) cos (default).    \n" : "" );
    output << ( ( basic || advanced ) ? "                           The first option is technically correct, the second\n" : "" );
    output << ( ( basic || advanced ) ? "                           is most commonly used.  In  fact, cos(w'x+b) with b\n" : "" );
    output << ( ( basic || advanced ) ? "                           uniformly distributed on  [0,2pi] would be correct,\n" : "" );
    output << ( ( basic || advanced ) ? "                           but apparently this  performs worse than cos alone,\n" : "" );
    output << ( ( basic || advanced ) ? "                           so we simplify to the cos(w'x) option.             \n" : "" );
    output << ( ( basic || advanced ) ? "         -dia {0,1,2,...}- Controls the inner loop for RFF tuning:            \n" : "" );
    output << ( ( basic || advanced ) ? "                           0 - matrix inv to find w on inner loop.            \n" : "" );
    output << ( ( basic || advanced ) ? "                           1 - ADAM to find w on inner loop.                  \n" : "" );
    output << ( ( basic || advanced ) ? "                           2 - ADAM to find w on inner loop  with hotstart.   \n" : "" );
    output << ( ( basic || advanced ) ? "                           3 - stochastic ADAM.                               \n" : "" );
    output << ( ( basic || advanced ) ? "                           4 - gradient descent.                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           5 - stochastic gradient descent.                   \n" : "" );
    output << ( ( basic || advanced ) ? "                           6 - accel. matrix inv. (or active setish thing).   \n" : "" );
    output << ( ( basic || advanced ) ? "                           7 - use PEGASOS where possible (default).          \n" : "" );
    output << ( ( basic || advanced ) ? "                           8 - train dual SVM with inferred kernel.           \n" : "" );
    output << ( ( basic || advanced ) ? "                           Of these, 0 is recommended for quadratic regression\n" : "" );
    output << ( ( basic || advanced ) ? "                           and 7 for everything else.                         \n" : "" );
    output << ( ( basic || advanced ) ? "         -dog {0,1,2,...}- Controls the outer loop for RFF tuning:            \n" : "" );
    output << ( ( basic || advanced ) ? "                           0,2 - optimise on v.^-1 using modified gradient.   \n" : "" );
    output << ( ( basic || advanced ) ? "                           1,3 - optimise on v using exact gradient.          \n" : "" );
    output << ( ( basic || advanced ) ? "                           Methods 1  and 3  differ in  how they  factorise H.\n" : "" );
    output << ( ( basic || advanced ) ? "                           Method 1 uses direct Cholesky factorisation, method\n" : "" );
    output << ( ( basic || advanced ) ? "                           3 borrows the  method from SVM_Scalar  (default 1).\n" : "" );
    output << ( ( basic || advanced ) ? "                           Method 3 is incompatible with -dia 7.              \n" : "" );
    output << ( ( basic || advanced ) ? "                           4,6 - like 0,2 but do single-level optimisation.   \n" : "" );
    output << ( ( basic || advanced ) ? "                           5,7 - like 1,3 but do single-level optimisation.   \n" : "" );
    output << ( ( basic || advanced ) ? "         -mtl {0,m}      - You can do  multi-task learning  for m  tasks using\n" : "" );
    output << ( ( basic || advanced ) ? "                           -mtl m.  The x  format for each x is [ x :::: 7:[ 0\n" : "" );
    output << ( ( basic || advanced ) ? "                           ... 0 1 0 ... 0 ] ], where the 1 indcates the task.\n" : "" );
    output << ( ( basic || advanced ) ? "                           The feature weights  are tuned to  optimise the sum\n" : "" );
    output << ( ( basic || advanced ) ? "                           of all tasks.                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- KNN specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -k   k          - sets the number of neighours k in the KNN.         \n" : "" );
    output << ( ( basic || advanced ) ? "         -K   i          - sets the KNN weight function.  The weight is:      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                             kappa(D(x))/kappabar                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           where D is set by the  kernel controls (this is the\n" : "" );
    output << ( ( basic || advanced ) ? "                           distance  metric),  normalised  to range  0->1, and\n" : "" );
    output << ( ( basic || advanced ) ? "                           kappa is set by this argument.  Options are:       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           0: kappa(d) = 1/2                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                           1: kappa(d) = 1-d                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                           2: kappa(d) = 3/4 (1-d^2)                          \n" : "" );
    output << ( ( basic || advanced ) ? "                           3: kappa(d) = 15/16 (1-d^2)^2                      \n" : "" );
    output << ( ( basic || advanced ) ? "                           4: kappa(d) = 35/32 (1-d^2)^3                      \n" : "" );
    output << ( ( basic || advanced ) ? "                           5: kappa(d) = pi/4 cos(pi/2 d)                     \n" : "" );
    output << ( ( basic || advanced ) ? "                           6: kappa(d) = 1/sqrt(2.pi) exp(d^2/2)              \n" : "" );
    output << ( ( basic || advanced ) ? "                           7: kappa(d) = 1/d                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- GP specific options                           --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -d   sigma      - sets the measurement noise for the GP.             \n" : "" );
    output << ( (          advanced ) ? "         -dw  i w        - set sigma weight w>0 for training vector i.        \n" : "" );
    output << ( (          advanced ) ? "         -ds  s          - Scale sigma by s>0.                                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- IMP specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -iz  zref       - sets zref factor used by EHI.                      \n" : "" );
    output << ( ( basic || advanced ) ? "         -ie  i          - sets EHI calculation method.  Options are:         \n" : "" );
    output << ( ( basic || advanced ) ? "                           0: fully optimised recursive method (default).     \n" : "" );
    output << ( ( basic || advanced ) ? "                           1: partially optimised recursive method.           \n" : "" );
    output << ( ( basic || advanced ) ? "                           2: un-optimised recursive method.                  \n" : "" );
    output << ( ( basic || advanced ) ? "                           3: Hupkens IRS method.                             \n" : "" );
    output << ( ( basic || advanced ) ? "                           4: Couckuyt method with binary cells.              \n" : "" );
    output << ( ( basic || advanced ) ? "         -is  i          - sets scalarisation method.  Options are:           \n" : "" );
    output << ( ( basic || advanced ) ? "                           0: random linear scalarisation (default).          \n" : "" );
    output << ( ( basic || advanced ) ? "                           1: random Chebyshev scalarisation.                 \n" : "" );
    output << ( ( basic || advanced ) ? "                           2: augmented Chebyshev scalarisation.              \n" : "" );
    output << ( ( basic || advanced ) ? "                           3: modified Chebyshev scalarisation.               \n" : "" );
    output << ( ( basic || advanced ) ? "         -ia  alpha        set alpha factor for augmented/modified Chebyshev. \n" : "" );
    output << ( ( basic || advanced ) ? "         -in  n            set number of samples in TS grid for rns.          \n" : "" );
    output << ( ( basic || advanced ) ? "         -il  n            set sampling slack in TS  grid for rns.  If this is\n" : "" );
    output << ( ( basic || advanced ) ? "                           non-zero  then samples  are taken  from [-n,1]^xdim\n" : "" );
    output << ( ( basic || advanced ) ? "                           rather than  [0,1]^xdim, so that the  imp increases\n" : "" );
    output << ( ( basic || advanced ) ? "                           along the edges rather than being fixed (0) there. \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- BLK specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -blx $fn        - set operating function for block (applied to output\n" : "" );
    output << ( ( basic || advanced ) ? "                           except  for  user  function  machine,  where  it is\n" : "" );
    output << ( ( basic || advanced ) ? "                           applied to all inputs).                            \n" : "" );
    output << ( ( basic || advanced ) ? "         -bly $fn        - set MEX callback  function for block  (used to calc\n" : "" );
    output << ( ( basic || advanced ) ? "                           g(x) for types mxa and mxb.                        \n" : "" );
    output << ( ( basic || advanced ) ? "         -blz i          - set integer argument for MEX callback function (for\n" : "" );
    output << ( ( basic || advanced ) ? "                           MEX  this is  >=0 for  commandline argument,  -1 to\n" : "" );
    output << ( ( basic || advanced ) ? "                           load  named external  variable,  -3  to run  matlab\n" : "" );
    output << ( ( basic || advanced ) ? "                           function.   External   variables  can  be  function\n" : "" );
    output << ( ( basic || advanced ) ? "                           handles.  Default is -3).                          \n" : "" );
    output << ( ( basic || advanced ) ? "         -bls fn         - set SYSTEM  call used  to calculate g(xx)  for sys.\n" : "" );
    output << ( ( basic || advanced ) ? "                           On call, x,y,z,... are  sustituted with values from\n" : "" );
    output << ( ( basic || advanced ) ? "                           from xx.  For example:                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           -bls \\[ \\\".\\/echoit.exe\\\" x \\\"\\>\\ temp.txt\\\" \\]    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           (note escape  characters - these may  or may not be\n" : "" );
    output << ( ( basic || advanced ) ? "                           required  depending on  the operating  environment)\n" : "" );
    output << ( ( basic || advanced ) ? "                           will  cause calls  to g(xx)  to be  evaluated using\n" : "" );
    output << ( ( basic || advanced ) ? "                           the system call:                                   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           ./echoit.exe x > temp.txt                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           where x is replaced with xx(0).  In this example we\n" : "" );
    output << ( ( basic || advanced ) ? "                           use the shortcut where a vector is converted to the\n" : "" );
    output << ( ( basic || advanced ) ? "                           elements concatenated with spaces between them.    \n" : "" );
    output << ( ( basic || advanced ) ? "         -bfx $fn        - save x data to file $fn on sys g(x) if defined.    \n" : "" );
    output << ( ( basic || advanced ) ? "         -bfy $fn        - save y data to file $fn on sys g(x) if defined.    \n" : "" );
    output << ( ( basic || advanced ) ? "         -bfxy $fn       - save x,y data to file $fn on sys g(x) if defined.  \n" : "" );
    output << ( ( basic || advanced ) ? "         -bfyx $fn       - save y,x data to file $fn on sys g(x) if defined.  \n" : "" );
    output << ( ( basic || advanced ) ? "         -bfr $fn        - get result from file $fn on sys g(x) if defined.   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- MLM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -mlc i c        - Set C (regularisation) value for layer i (deflt 1).\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Kernel selection options (after learning options):                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** By default a  single kernel is  chosen.  More **         \n" : "" );
    output << ( (          advanced ) ? "                  ** complex kernel  dictionaries are  possible by **         \n" : "" );
    output << ( (          advanced ) ? "                  ** setting kernel dictionary  size > 1.  In this **         \n" : "" );
    output << ( (          advanced ) ? "                  ** case K(x,y) = w0.K0(x,y)  + w1.K1(x,y) + ..., **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where w0,w1,... are weights and K0,K1,... are **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the kernel fucntions.                         **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -ki  i          - define which kernel elm i  is being set (default 0,\n" : "" );
    output << ( (          advanced ) ? "                           or 1 if this is an MLM non-input layer).           \n" : "" );
    output << ( (          advanced ) ? "         -ks  n          - set kernel dictionary size n (default 1).          \n" : "" );
    output << ( ( basic || advanced ) ? "         -kn             - set kernel normalised (this element).              \n" : "" );
    output << ( ( basic || advanced ) ? "         -ku             - set kernel unnormalised (this element, default).   \n" : "" );
    output << ( ( basic || advanced ) ? "         -kss            - set kernel symm set (overall, default).            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                K([x1~x2],[x3~x4]) = sqrt(K(x1,x3).K(x1,x4).K(x2,x3).K(x2.x4))\n" : "" );
    output << ( ( basic || advanced ) ? "                K(x1     ,[x3~x4]) = sqrt(K(x1,x3).K(x1,x4).K(x1,x3).K(x1.x4))\n" : "" );
    output << ( ( basic || advanced ) ? "                K([x1~x2],x3     ) = sqrt(K(x1,x3).K(x1,x3).K(x2,x3).K(x2.x3))\n" : "" );
    output << ( ( basic || advanced ) ? "                K(x1     ,x3     ) = sqrt(K(x1,x3).K(x1,x3).K(x1,x3).K(x1.x3))\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           currently only works for 2-kernels.                \n" : "" );
    output << ( ( basic || advanced ) ? "         -kus            - unset kernel symm set.                             \n" : "" );
    output << ( ( basic || advanced ) ? "         -knn            - set kernel normalised (overall).                   \n" : "" );
    output << ( ( basic || advanced ) ? "         -kuu            - set kernel unnormalised (overall, default).        \n" : "" );
    output << ( ( basic || advanced ) ? "         -kp             - set kernel as axis-product type.                   \n" : "" );
    output << ( ( basic || advanced ) ? "         -knp            - set kernel as not-axis-product type (overall,dflt).\n" : "" );
    output << ( ( basic || advanced ) ? "         -krn v          - set kernel rank constraint kernel construct type:  \n" : "" );
    output << ( ( basic || advanced ) ? "                  0: phi(x,x') = phi(x)-phi(x')  (default)                    \n" : "" );
    output << ( ( basic || advanced ) ? "                  1: phi(x,x') = phi(x)+phi(x')                               \n" : "" );
    output << ( ( basic || advanced ) ? "                  2: phi(x,x') = phi(x) otimes phi(x') - phi(x') otimes phi(x)\n" : "" );
    output << ( ( basic || advanced ) ? "                  3: phi(x,x') = phi(x) otimes phi(x') + phi(x') otimes phi(x)\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -kg  g          - set x scale, non-ARD style ( x:= x/g ).            \n" : "" );
    output << ( ( basic || advanced ) ? "         -kgg g          - set x scale, ARD style ( x_i := x_i/g_i for all i).\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -km             - modify so K(x,y) -> K(x,x).K(y,y).                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -kum            - undo -km (default).                                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -kt  t          - type of kernel function:                           \n" : "" );
    output << ( ( basic || advanced ) ? "                           Kernels 0-99 are intended for ML use (deft 2).     \n" : "" );
    output << ( ( basic || advanced ) ? "                           Kernels 100-299 are intended for NN use (deft 201).\n" : "" );
    output << ( ( basic || advanced ) ? "                           Kernels 300-399 are intended for kNN use (dft 300).\n" : "" );
    output << ( ( basic || advanced ) ? "                           Defaults: 2 for most MLs.                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                     200 for density estimation.              \n" : "" );
    output << ( ( basic || advanced ) ? "                                     300 for K-nearest neighbours.            \n" : "" );
    output << ( ( basic || advanced ) ? "                                     200 for neural-networks.                 \n" : "" );
    output << ( ( basic || advanced ) ? "                           For full list see -??k.                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "    0   = Constant kernel: K(x,y) = r1                                        \n" : "" );
    output << ( ( basic || advanced ) ? "    1   = Linear kernel: K(x,y) = <x,y>/(r0.r0)                               \n" : "" );
    output << ( ( basic || advanced ) ? "    2   = Polynomial kernel: K(x,y) = ( r1 + <x,y>/(r0.r0) )^i0               \n" : "" );
    output << ( ( basic || advanced ) ? "    3   = Gaussian kernel: K(x,y) = exp(-||x-y||^2/(2*r0*r0)-r1)              \n" : "" );
    output << ( ( basic || advanced ) ? "    4   = Laplacian kernel: K(x,y) = exp(-||x-y||/r0-r1)                      \n" : "" );
    output << ( ( basic || advanced ) ? "    5   = Polynoise kernel: K(x,y) = exp(-||x-y||^r1/(r1*r0^r1)-r2)           \n" : "" );
    output << ( ( basic || advanced ) ? "    7   = Sigmoid kernel (CPD): K(x,y) = tanh( <x,y>/(r0.r0) + r1 )           \n" : "" );
    output << ( ( basic || advanced ) ? "    8   = Rational quadratic kernel: K(x,y) =( 1+||x-y||^2/(2*r0*r0*r1))^(-r1)\n" : "" );
    output << ( ( basic || advanced ) ? "    9   = Multiquadric kernel (NM): K(x,y) = sqrt(||x-y||^2/(r0.r0)+r1^2)     \n" : "" );
    output << ( ( basic || advanced ) ? "    10  = Inverse multiquadric kernel: K(x,y) = 1/sqrt(||x-y||^2/(r0*r0)+r1^2)\n" : "" );
    output << ( ( basic || advanced ) ? "    11  = Circular kernel (MR2): K(x,y) = 2/pi * ( arccos(-||x-y||/r0)        \n" : "" );
    output << ( ( basic || advanced ) ? "                                         - ||x-y||*sqrt(1-||x-y||^2/r0^2)/r0 )\n" : "" );
    output << ( ( basic || advanced ) ? "    12  = Spherical kernel (MR3): K(x,y) = 1 - 1.5*||x-y||/r0                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                         + 0.5*||x-y||^3/r0^3                 \n" : "" );
    output << ( ( basic || advanced ) ? "    13  = Wave kernel: K(x,y) = (r0/||x-y||).sin(||x-y||/r0)                  \n" : "" );
    output << ( ( basic || advanced ) ? "    14  = Power kernel: K(x,y) = -(||x-y||/r0)^r1                             \n" : "" );
    output << ( ( basic || advanced ) ? "    15  = Log kernel (CPD): K(x,y) = -log((||x-y||/r0)^r1 + 1)                \n" : "" );
    output << ( ( basic || advanced ) ? "    19  = Cauchy kernel: K(x,y) = 1/(1+((||x-y||^2/(r0.r0))))                 \n" : "" );
    output << ( ( basic || advanced ) ? "    23  = Generalised T-Student kernel: K(x,y) = 1/(1+||x-y||^r0)             \n" : "" );
    output << ( ( basic || advanced ) ? "    24  = Vovk's real polynomial: K(x,y)= (1-((<x,y>/r0^2)^i0))/(1-<x,y>/r0^2)\n" : "" );
    output << ( ( basic || advanced ) ? "    25  = Weak fourier kernel: K(x,y) = pi.cosh(pi-(||x-y||/r0))              \n" : "" );
    output << ( ( basic || advanced ) ? "    26  = Thin spline (1): K(x,y) = ((||x-y||/r0)^(r1+0.5))                   \n" : "" );
    output << ( ( basic || advanced ) ? "    27  = Thin spline (2): K(x,y) = ((||x-y||/r0)^r1)*ln(sqrt(||x-y||/r0))    \n" : "" );
    output << ( ( basic || advanced ) ? "    32  = Diagonal offset kernel: r1 if diagonal Hessian, 0 otherwise         \n" : "" );
    output << ( ( basic || advanced ) ? "    33  = Uniform kernel: K(x,y) = 1/2r0 if |||x-y||| < r0, 0 otherwise       \n" : "" );
    output << ( ( basic || advanced ) ? "    34  = Triang kernel: K(x,y) = 1/r0 (1-|||x-y|||/r0) if |||x-y||| < r0, 0  \n" : "" );
    output << ( ( basic || advanced ) ? "    35  = d-Matern kernel:                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "    37  = d+1/2-Matern kernel:                                                \n" : "" );
    output << ( ( basic || advanced ) ? "    38  = 1/2-Matern kernel: K(x,y) = exp(-||x-y||/r0)                        \n" : "" );
    output << ( ( basic || advanced ) ? "    39  = 3/2-Matern kernel: K(x,y) = (1 + sqrt(3)*||x-y||/r0).               \n" : "" );
    output << ( ( basic || advanced ) ? "                                      exp(-sqrt(3)*||x-y||/r0)                \n" : "" );
    output << ( ( basic || advanced ) ? "    40  = 5/2-Matern kernel: K = (1 + sqrt(5)*||x-y||/r0 + 5*||x-y||^2/r0^2). \n" : "" );
    output << ( ( basic || advanced ) ? "                                      exp(-sqrt(5)*||x-y||/r0)                \n" : "" );
    output << ( ( basic || advanced ) ? "    41  = RBF rescale kernel: K(x,y) = z^(1/(2*r0*r0)) = exp(log(z)/(2*r0*r0))\n" : "" );
    output << ( ( basic || advanced ) ? "    42  = Inverse gudermannian kernel: K(x,y) = igd(<x,y>/(r0.r0))            \n" : "" );
    output << ( ( basic || advanced ) ? "    43  = Log ratio kernel: K(x,y) = log((1+<x,y>/(r0.r0))/(1-<x,y>/(r0.r0))) \n" : "" );
    output << ( ( basic || advanced ) ? "    44  = Exponential kernel: K(x,y) = exp(<x,y>/(r0.r0)-r1)                  \n" : "" );
    output << ( ( basic || advanced ) ? "    45  = Hyperbolic sine kernel: K(x,y) = sinh(<x,y>/(r0.r0))                \n" : "" );
    output << ( ( basic || advanced ) ? "    46  = Hyperbolic cosine kernel: K(x,y) = cosh(<x,y>/(r0.r0))              \n" : "" );
    output << ( ( basic || advanced ) ? "    47  = Sinc kernel: K(x,y) = sinc(||x-y||/r0).cos(2*pi*||x-y||/(r0.r1))    \n" : "" );
    output << ( ( basic || advanced ) ? "    48  = LUT kernel: K(x,y) = r1((int) x, (int) y) if r1 is a matrix         \n" : "" );
    output << ( ( basic || advanced ) ? "                             = r1 if (int) x != (int) y and r1 not a matrix   \n" : "" );
    output << ( ( basic || advanced ) ? "                             = 1  if (int) x == (int) y and r1 not a matrix   \n" : "" );
    output << ( ( basic || advanced ) ? "               (this can be used eg to implement multitask learning with ICM).\n" : "" );
    output << ( ( basic || advanced ) ? "    52  = Radius kernel (BOCK, inner prod part): K(x,y) = ||x||.||y||/(r0.r0) \n" : "" );
    output << ( ( basic || advanced ) ? "    53  = Radius kernel 2 (BOCK, inner prod part): K(x,y)=(1-(1-||x||^r1)^r2).\n" : "" );
    output << ( ( basic || advanced ) ? "                                              ...  (1-(1-||y||^r1)^r2)/(r0.r0)\n" : "" );
    output << ( ( basic || advanced ) ? "          (use a normalised linear kernel for the angular BOCK inner prod).   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "    100 = Linear 0/1 neuron:    K(z) = z/(r0.r0)                              \n" : "" );
    output << ( ( basic || advanced ) ? "    101 = Logistic 0/1 neuron:  K(z) = 1/(1+exp(-r0.z))                       \n" : "" );
    output << ( ( basic || advanced ) ? "    102 = Gen. logistic 0/1:    K(z) = 1/(1+r1.exp(-r2.(z-r3)/(r0.r0)))^(1/r2)\n" : "" );
    output << ( ( basic || advanced ) ? "    103 = Heavyside 0/1 neuron: K(z) = 1 if real(z) > 0, 0 otherwise          \n" : "" );
    output << ( ( basic || advanced ) ? "    104 = ReLU 0/1 neuron:      K(z) = z/(r0.r0) if real(z) > 0, 0 otherwise  \n" : "" );
    output << ( ( basic || advanced ) ? "    105 = Softplus 0/1 neuron:  K(z) = ln(r1+exp(z/(r0.r0)))                  \n" : "" );
    output << ( ( basic || advanced ) ? "    106 = Leaky ReLU 0/1 neuron:K(z) = z/(r0.r0) if real(z) > 0               \n" : "" );
    output << ( ( basic || advanced ) ? "                                     = (r1*z)/(r0.r0) if real(z) <= 0         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "    200 = Linear -1/+1 neuron:    K(z) = z/(r0.r0)-1                          \n" : "" );
    output << ( ( basic || advanced ) ? "    201 = Logistic -1/+1 neuron:  K(z) = 2/(1+exp(-z/(r0.r0))) -1             \n" : "" );
    output << ( ( basic || advanced ) ? "    202 = Gen. logistic -1/+1: K(z) = 2/(1+r1.exp(-r2.(z-r3)/(r0^2)))^(1/r2)-1\n" : "" );
    output << ( ( basic || advanced ) ? "    203 = Heavyside -1/+1 neuron: K(z) = 1   if real(z) > 0, -1 otherwise     \n" : "" );
    output << ( ( basic || advanced ) ? "    204 = ReLU -1/+1 neuron:      K(z) = z/(r0.r0)-1 if real(z) > 0, -1 other \n" : "" );
    output << ( ( basic || advanced ) ? "    205 = Softplus -1/+1 neuron:  K(z) = 2.ln(r1+exp(z/(r0.r0))) -1           \n" : "" );
    output << ( ( basic || advanced ) ? "    204 = Leaky ReLU -1/+1 neuron:K(z) = z/(r0.r0)-1 if real(z) > 0           \n" : "" );
    output << ( ( basic || advanced ) ? "                                       = (r1*z)/(r0.r0)-1 if real(z) <= 0     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "    300 = Euclidean distance: K(x,y) = -1/2 ||x-y||_2^2/(r0.r0)               \n" : "" );
    output << ( ( basic || advanced ) ? "    301 = 1-norm distance:    K(x,y) = -1/2 ||x-y||_1^2/(r0.r0)               \n" : "" );
    output << ( ( basic || advanced ) ? "    302 = inf-norm distance:  K(x,y) = -1/2 ||x-y||_inf^2/(r0.r0)             \n" : "" );
    output << ( ( basic || advanced ) ? "    303 = 0-norm distance:    K(x,y) = -1/2 ||x-y||_0^2/(r0.r0)               \n" : "" );
    output << ( ( basic || advanced ) ? "    304 = p-norm distance:    K(x,y) = -1/2 ||x-y||_r1^2/(r0.r0)              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "    8xx = kernel transfer (see below)                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "        Notes: - ' indicates conjugate transpose                              \n" : "" );
    output << ( ( basic || advanced ) ? "               - ||x||^2 = conj(x)'x        (not the norm if (hyper-)complex).\n" : "" );
    output << ( ( basic || advanced ) ? "               - ||x-y||^2 = (x-y').(x-y')  (not the norm if (hyper-)complex).\n" : "" );
    output << ( ( basic || advanced ) ? "                           = ||x||^2 + ||y||^2 - 2<x,y>                       \n" : "" );
    output << ( ( basic || advanced ) ? "               - <x,y> = ( x'y + conj(x)'conj(y) )/2.                         \n" : "" );
    output << ( ( basic || advanced ) ? "               - for neural kernels, z=<x,y>.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -kg  x          - kernel param r0  = x   (default 1).                \n" : "" );
    output << ( ( basic || advanced ) ? "         -kr  x          - kernel param r1  = x   (default 0 or 1).           \n" : "" );
    output << ( ( basic || advanced ) ? "         -kf  $fn        - kernel param r10 = $fn (dft (var(0,1)+var(0,2))/2).\n" : "" );
    output << ( ( basic || advanced ) ? "         -kv  i x        - kernel param ri  = x   (default 0).                \n" : "" );
    output << ( ( basic || advanced ) ? "         -kd  x          - kernel param i0  = x   (default 2).                \n" : "" );
    output << ( ( basic || advanced ) ? "         -kG  x          - kernel param i0  = x   (default 1).                \n" : "" );
    output << ( ( basic || advanced ) ? "         -kV  i x        - kernel param ii  = x   (default 0).                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -kglb  x        - kernel param r0 nominal lower bound (kernel tune). \n" : "" );
    output << ( (          advanced ) ? "         -krlb  x        - kernel param r1 nominal lower bound (kernel tune). \n" : "" );
    output << ( (          advanced ) ? "         -kvlb  i x      - kernel param ri nominal lower bound (kernel tune). \n" : "" );
    output << ( (          advanced ) ? "         -kdlb  x        - kernel param i0 nominal lower bound (kernel tune). \n" : "" );
    output << ( (          advanced ) ? "         -kGlb  x        - kernel param i0 nominal lower bound (kernel tune). \n" : "" );
    output << ( (          advanced ) ? "         -kVlb  i x      - kernel param ii nominal lower bound (kernel tune). \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -kgub  x        - kernel param r0 nominal upper bound (kernel tune). \n" : "" );
    output << ( (          advanced ) ? "         -krub  x        - kernel param r1 nominal upper bound (kernel tune). \n" : "" );
    output << ( (          advanced ) ? "         -kvub  i x      - kernel param ri nominal upper bound (kernel tune). \n" : "" );
    output << ( (          advanced ) ? "         -kdub  x        - kernel param i0 nominal upper bound (kernel tune). \n" : "" );
    output << ( (          advanced ) ? "         -kGub  x        - kernel param i0 nominal upper bound (kernel tune). \n" : "" );
    output << ( (          advanced ) ? "         -kVub  i x      - kernel param ii nominal upper bound (kernel tune). \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -kI  v          - set kernels  indexing using  given index  vector v,\n" : "" );
    output << ( (          advanced ) ? "                           where  the  argument  is a  vector of  non-negative\n" : "" );
    output << ( (          advanced ) ? "                           integers   (eg [ 0 4 5 ])   in   increasing   order\n" : "" );
    output << ( (          advanced ) ? "                           corresponding to the indexes used.                 \n" : "" );
    output << ( (          advanced ) ? "         -kU             - set kernel unindexed.                              \n" : "" );
    output << ( (          advanced ) ? "         -kw  w          - set weight w>=0 of kernel function (default 1). The\n" : "" );
    output << ( (          advanced ) ? "                           weight can be anything (not just double), which may\n" : "" );
    output << ( (          advanced ) ? "                           be useful eg for matrix-valued kernels.            \n" : "" );
    output << ( (          advanced ) ? "         -kwlb w         - nominal lower bound on -kw for kernel tuning.      \n" : "" );
    output << ( (          advanced ) ? "         -kwub w         - nominal upper bound on -kw for kernel tuning.      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -kx  [ n0 ... ] [ A0 ... ] - set kernel transform, so:               \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                    K(x,y) = sum_i Ai (d^ni/dx^ni) (d^ni/dy^ni)^T K(x,y) Ai^T \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           This allows  us to  implement the  method described\n" : "" );
    output << ( (          advanced ) ? "                           in Jidling  et al,  \"Linearly  constrained Gaussian\n" : "" );
    output << ( (          advanced ) ? "                           processes\" (though this  applies to more general ML\n" : "" );
    output << ( (          advanced ) ? "                           blocks,  provided care is taken  with bias  terms).\n" : "" );
    output << ( (          advanced ) ? "                           Note this isn't currently  compatible with gradient\n" : "" );
    output << ( (          advanced ) ? "                           constraints, and may interact with rank constraints\n" : "" );
    output << ( (          advanced ) ? "                           and  multitask  learning  in unexpected  ways.  For\n" : "" );
    output << ( (          advanced ) ? "                           example, (9) in Jidling is coded as:               \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              -kx [ 1 ] [ [ 0 -1 ; 1 0 ] ]                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           for  2-dimensional training  data, and  constraints\n" : "" );
    output << ( (          advanced ) ? "                           us to divergence-free  trained machines.  For 3-dim\n" : "" );
    output << ( (          advanced ) ? "                           divergence-free data (Jid1 supplementary e.g. 1):  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              -kx [ 1 ] [ [ 0 1 -1 ; -1 0 1 ; 1 -1 0 ] ]      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           For a curl-free vector field (Jid1 supp e.g. 2):   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              -kx [ 1 ] [ [ 1 0 0 ; 0 1 0 ; 0 0 1 ] ]         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -ka  n          - number of samples used  when computing distribution\n" : "" );
    output << ( (          advanced ) ? "                           similarity (Muandet et al SMM).                    \n" : "" );
    output << ( (          advanced ) ? "         -kb  [ i j .. ] - indices of var(0,..) variables sampled.            \n" : "" );
    output << ( (          advanced ) ? "         -ke  [ fi fj ..]- distribution useds for var(0,..) ].                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Kernel  chaining  allows the  construction of **         \n" : "" );
    output << ( (          advanced ) ? "                  ** rudimentary deep kernels.  If a kernel elm is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** chained then rather than adding the output of **         \n" : "" );
    output << ( (          advanced ) ? "                  ** this element to the result it is instead used **         \n" : "" );
    output << ( (          advanced ) ? "                  ** to calculate  the next kernel  element in the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** chain.   So if the  dictionary size  is 3, k0 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** is chained but k1 and k2 are not then:        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** K(x,y) = k1(m0(x),m0(y)) + k2(x,y)            **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where m0 is  the feature map  associated with **         \n" : "" );
    output << ( (          advanced ) ? "                  ** k0.  This only works  for kernels that can be **         \n" : "" );
    output << ( (          advanced ) ? "                  ** written as:                                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** kn(x,y) = kn(||x||^2,||y||^2,<x,y>)           **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** as for example in the above example:          **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** K(x,y) = k1(k0(x,x),k0(y,y),k0(x,y))+k2(x,y)  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** (it's  a little  more  involved for  division **         \n" : "" );
    output << ( (          advanced ) ? "                  ** algebraic kernels, but that's essentially it) **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** NB: - chaining   is  only   implemented   for **         \n" : "" );
    output << ( (          advanced ) ? "                  **       standard 2-norm kernels.                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Example: -ks 3 -ki 0 -kc -kt 2 -kd 2 -ki 1    **         \n" : "" );
    output << ( (          advanced ) ? "                  **          -kt 7 -ki 2 -kt 1                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **   gives: k(x,y) = tanh(1+(1+<x,y>)^2) + <x,y> **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -kc             - set kernel chained.                                \n" : "" );
    output << ( (          advanced ) ? "         -kuc            - set kernel unchained.                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Kernel splitting allows  different kernels to **         \n" : "" );
    output << ( (          advanced ) ? "                  ** applied to  different parts  of the  vectors. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** For example if splitting  is set of element 1 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** of the kernel with dictionary size 2 then:    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** K(x0 ~ x1,x2 ~ x3) = k1(x0,x2).k2(x1,x3) (-kS)**         \n" : "" );
    output << ( (          advanced ) ? "                  ** K(x0 ~ x1,x2 ~ x3) = k1(x0,x2)+k2(x1,x3) (-kA)**         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Note  that  this  is instead  of  the  multi- **         \n" : "" );
    output << ( (          advanced ) ? "                  ** instance  interpretation  -  you  cannot  use **         \n" : "" );
    output << ( (          advanced ) ? "                  ** both at the same time.  m-kernel splitting is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** also supported but is somewhat complicated.   **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -kS             - set kernel split.                                  \n" : "" );
    output << ( (          advanced ) ? "         -kA             - additive kernel split.                             \n" : "" );
    output << ( (          advanced ) ? "         -kuS            - set kernel unsplit (default).                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Kernel  multiply points allow  for support of **         \n" : "" );
    output << ( (          advanced ) ? "                  ** product kernels of the form:                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** K(x,y) = k0(x,y).k1(x,y)....   (-kMS)         **         \n" : "" );
    output << ( (          advanced ) ? "                  ** K(x,y) = k0(x,y)+k1(x,y)....   (-kMA)         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where k0 is specified  by elements 0 to first **         \n" : "" );
    output << ( (          advanced ) ? "                  ** element  with -kMS  set, k1  is specified  by **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the  element  immediately  after this  to the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** next with -kMS set,  and so on (final kn goes **         \n" : "" );
    output << ( (          advanced ) ? "                  ** to end of elements).                          **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** NB: on  evaluation, kernel is  first split at **         \n" : "" );
    output << ( (          advanced ) ? "                  **     at  multiply points,  then the  fragments **         \n" : "" );
    output << ( (          advanced ) ? "                  **     are split at split  points, then chaining **         \n" : "" );
    output << ( (          advanced ) ? "                  **     occurs.                                   **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -kMS            - set kernel multiply point.                         \n" : "" );
    output << ( (          advanced ) ? "         -kMA            - set kernel addition point.                         \n" : "" );
    output << ( (          advanced ) ? "         -kMuS           - set kernel non-multiply point.                     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** These options  allow kernel  parameters to be **         \n" : "" );
    output << ( (          advanced ) ? "                  ** taken from elements  of the training vectors. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** That is, for example:                         **         \n" : "" );
    output << ( (          advanced ) ? "                  **    ri = conj(xj).yj                           **         \n" : "" );
    output << ( (          advanced ) ? "                  **   (ri = yj for neural networks)               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** This allows for example the RBF width to be a **         \n" : "" );
    output << ( (          advanced ) ? "                  ** function of position  in input space tuned on **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the density of points.   The kernel weight is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** parameter r-1 (-ko -1 ...).                   **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -ko  i j        - replace parameter  ri with input  product xj.yj (or\n" : "" );
    output << ( (          advanced ) ? "                           just yj for neural networks).                      \n" : "" );
    output << ( (          advanced ) ? "         -kO  i j        - replace  parameter ii  with input  (int) xj.yj  (or\n" : "" );
    output << ( (          advanced ) ? "                           just (int) yj for neural networks).                \n" : "" );
    output << ( (          advanced ) ? "         -koz            - delete all defined ri parameter replacements.      \n" : "" );
    output << ( (          advanced ) ? "         -kOz            - delete all defined ii parameter replacements.      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** When computing m-kernels we replace ||x-y||^2 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** with one of:                                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 0: ||x||_m^m + ||y||_m^m +..- m.<<x,y,...>>_m **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1: ||x||_p^2 + ||y||_p^2 +..- p.<<x,y,...>>_m **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2: ||x||_p^2 + ||y||_p^2 +..-                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **                    (1/m).(sum_{ij} <xi,xj>))  **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 5: ||x_0-x_1||_2^2 + ||x_2-x_3||_2^2 + ...    **         \n" : "" );
    output << ( (          advanced ) ? "                  **      (this is effectively a hyperkernel type) **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Alternatively  you  can   use  a  moment-like **         \n" : "" );
    output << ( (          advanced ) ? "                  ** kernel:                                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** K(x0,x1,...) = D sum_{s} K(||x{s}||_p^2)      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where: x{s} = sum_i s_i x_i                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **        s = [ +-1 +-1 ... ] has dim m          **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** The following variants are available:         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 103: D = 1/2^{m-1}                            **         \n" : "" );
    output << ( (          advanced ) ? "                  **      s : |i:si=+1| + |i:si=-1| in 4Z_+        **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 104: D = 1/m!                                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **      s : |i:si=+1| = |i:si=-1|                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 203: like 103,  but expansion  only occurs on **         \n" : "" );
    output << ( (          advanced ) ? "                  **      first kernel in chain.                   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 204: like 104,  but expansion  only occurs on **         \n" : "" );
    output << ( (          advanced ) ? "                  **      first kernel in chain.                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 300: true moment-kernel expansion.            **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -kan i          - Set difference definition (default 1).             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Assumptions: these can speed up optimisation, **         \n" : "" );
    output << ( (          advanced ) ? "                  ** but make  sure they're  valid and  disable if **         \n" : "" );
    output << ( (          advanced ) ? "                  ** they are not.                                 **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -mtb            - always use  inner-product cache for speed.  This is\n" : "" );
    output << ( (          advanced ) ? "                           faster  if you're  doing kernel  tuning,  but  uses\n" : "" );
    output << ( (          advanced ) ? "                           almost double  the memory and offers  no speedup if\n" : "" );
    output << ( (          advanced ) ? "                           kernel is not tuned.                               \n" : "" );
    output << ( (          advanced ) ? "         -bmx            - save memory by not using inner-prd cache (default).\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
//    output << ( (          advanced ) ? "         -kcy            - enable re-calc  of <x,y>,  ||x-y|| (where possible)\n" : "" );
//    output << ( (          advanced ) ? "                           when adjusting  kernel parameters.  Note  that this\n" : "" );
//    output << ( (          advanced ) ? "                           does not  guarantee  re-calc  will  happen  for all\n" : "" );
//    output << ( (          advanced ) ? "                           kernels (eg <x,y>+b  or  ||x-y||  cannot  always be\n" : "" );
//    output << ( (          advanced ) ? "                           calculated from K(x,y), this won't help when moving\n" : "" );
//    output << ( (          advanced ) ? "                           from an  inner product to  distance-based kernel or\n" : "" );
//    output << ( (          advanced ) ? "                           vice-versa, and  is incompatible  with shift/scale)\n" : "" );
//    output << ( (          advanced ) ? "                           and indexing).  Also  the speed-up  gained is often\n" : "" );
//    output << ( (          advanced ) ? "                           small  unless dealing  with kernel  transfer with a\n" : "" );
//    output << ( (          advanced ) ? "                           non-trivial base kernel inheritance.  Also be aware\n" : "" );
//    output << ( (          advanced ) ? "                           that this will double the memory used by the kernel\n" : "" );
//    output << ( (          advanced ) ? "                           cache when the kernel is changed.                  \n" : "" );
//    output << ( (          advanced ) ? "         -kcn            - disable <x,y> and ||x-y|| re-calc (default).       \n" : "" );
//    output << ( (          advanced ) ? "                                                                              \n" : "" );
//    output << ( (          advanced ) ? "                  ** NOTE: rather than use -kcy,  consider using a **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** mer block (-z  mer) with an  appropriate size **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** cache.  For  example,  assuming ML 0  is type **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** mer with a  linear kernel (-kt 1)  with cache **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** size n (-mc n)  then the following  will make **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** use of the  cacheing of  <x,y> inside  an RBF **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** kernel:                                       **         \n" : "" );
//    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** -ks 2 -ki 0 -kc -kt 800 -ktx 0 -ki 1 -kt 3 -kg 1 **      \n" : "" );
//    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** So the kernel is k1(k0(x,y)),  where k1 is an **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** RBF (set by -ki 1  -kt 3 -kg 1) and k0 refers **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** to ML  0 (set by  -ki 0  -kt 800 -ktx 0,  -kc **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** meaning chained), where k0 caches results.    **         \n" : "" );
//    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- Kernel transfer                               --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Kernel  transfer  is  a   method  for  taking **         \n" : "" );
    output << ( (          advanced ) ? "                  ** features learnt in  training an ML and coding **         \n" : "" );
    output << ( (          advanced ) ? "                  ** it into a kernel that  can be used elsewhere. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** For  example for  an SVM  with  kernel  K the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** transferred kernel Kx is defined to be:       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Kx(y,z) = sum_ij alpha_i alpha_j K(y,z,xi,xj) **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where K here is  interpretted as an m-kernel. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** You  can  do this  for  multiple  levels  and **         \n" : "" );
    output << ( (          advanced ) ? "                  ** treat  them like  any other  kernel.  To  use **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the  kernel  so constructed  from an  ML  set **         \n" : "" );
    output << ( (          advanced ) ? "                  ** kernel 8xx (-kt 8xx)  and the ML number using **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -ktx  i (i  is the  ML number  providing Kx). **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Available kernels are (Kx is kernel of ML i): **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 800: Trivial: K(x,y) = Kx(x,y)                **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 801: m-norm:                                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **      K(x,y) = sum_ij ai aj Kx(x,y,xi,xj)      **         \n" : "" );
    output << ( (          advanced ) ? "                  **      where for SVM ai = alpha_i for xi        **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 802: Moment:                                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **      K(x,y) = sum_ij ai aj Kx(x,xi) Kx(y,xj)  **         \n" : "" );
    output << ( (          advanced ) ? "                  **      where for SVM a_i = alpha_i for x_i      **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 803: reserved.                                **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 804: K-learn:                                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **      K(x,y) = sum_i ai Kx(xi,(x,y))           **         \n" : "" );
    output << ( (          advanced ) ? "                  **      where for SVM a_i = alpha_i for x_i      **         \n" : "" );
    output << ( (          advanced ) ? "                  **      Typically xi = (xai,xbi)                 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 805: K2-learn:                                **         \n" : "" );
    output << ( (          advanced ) ? "                  **      K(x,y) = (sum_i ai Kx(xi,(x,y)))^2       **         \n" : "" );
    output << ( (          advanced ) ? "                  **      where for SVM a_i = alpha_i for x_i      **         \n" : "" );
    output << ( (          advanced ) ? "                  **      Typically xi = (xai,xbi)                 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 806: Traditional multilayer network:          **         \n" : "" );
    output << ( (          advanced ) ? "                  **      K(x,y) = Kx(f(x),f(y))                   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 807: Harmonic hyperkernel:                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **      K(x,y) = sum_ij ai.aj/(1-K(x,y)K(xi,xj)) **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 808: Random Fourier Feature kernel:           **         \n" : "" );
    output << ( (          advanced ) ? "                  **      K(x,y) = sum_q v_q.phi_q(x).phi_q(y)     **         \n" : "" );
    output << ( (          advanced ) ? "                  **      where phi_q(x) is the qth random fourier **         \n" : "" );
    output << ( (          advanced ) ? "                  **      feature   (either  [  cos(wi'.x)  ]   or **         \n" : "" );
    output << ( (          advanced ) ? "                  **      [ cos(wi'.x) sin(wi'.x) ])  and  v_q  is **         \n" : "" );
    output << ( (          advanced ) ? "                  **      the weight, which  could be 1 for RFF or **         \n" : "" );
    output << ( (          advanced ) ? "                  **      a tuned weight for tuned RFF.            **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 809: Posterior: K(x,y) = cov(x,y)             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 81x: like 80x, but assumes  a common dataset, **         \n" : "" );
    output << ( (          advanced ) ? "                  **      so indices are passed through and caches **         \n" : "" );
    output << ( (          advanced ) ? "                  **      used.                                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Example: suppose ML 0  is a trained  SVM with **         \n" : "" );
    output << ( (          advanced ) ? "                  **      kernel 3 (RBF), and we are setting up ML **         \n" : "" );
    output << ( (          advanced ) ? "                  **      1s kernel using the command:             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **      -ks 2 -ki 0 -kc -kt 801 -ktx 0 -ki 1     **         \n" : "" );
    output << ( (          advanced ) ? "                  **      -kt 2 -kd 2                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **      gives you the kernel function:           **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **      K(x,y) = (K0(x,y)+1)^2                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **      where:                                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **      K0(x,y) = sum_ij ai aj Kr(x,y,ui,uj)     **         \n" : "" );
    output << ( (          advanced ) ? "                  **      Kr(x,y,u,v) = exp(4<x,y,u,v>-x4-y4-uU-vV)**         \n" : "" );
    output << ( (          advanced ) ? "                  **      ai = alpha_i for ML 0                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **      ui = x_i for ML 0                        **         \n" : "" );
    output << ( (          advanced ) ? "                  **      uU = ||u||_4^4                           **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **      That is,  m(x) (the  feature  map of the **         \n" : "" );
    output << ( (          advanced ) ? "                  **      the rbf  kernel Kr(x,y))  is elementwise **         \n" : "" );
    output << ( (          advanced ) ? "                  **      weighted  to  give   w.*m(x)  using  the **         \n" : "" );
    output << ( (          advanced ) ? "                  **      weights  found by  the  SVM  ML 0,  then **         \n" : "" );
    output << ( (          advanced ) ? "                  **      mapped using  n (the feature  map of the **         \n" : "" );
    output << ( (          advanced ) ? "                  **      second order  polynomial kernel) to give **         \n" : "" );
    output << ( (          advanced ) ? "                  **      the  composite  map n(w.m(x)),  which is **         \n" : "" );
    output << ( (          advanced ) ? "                  **      the feature map embodied by K(x,y).      **         \n" : "" );
//    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
//    output << ( (          advanced ) ? "                  ** Note: in the above example (or similar) it is **         \n" : "" );
//    output << ( (          advanced ) ? "                  **      useful  to apply  -kcy  when  optimising **         \n" : "" );
//    output << ( (          advanced ) ? "                  **      kernel hyper-parameters to prevent slow- **         \n" : "" );
//    output << ( (          advanced ) ? "                  **      down due to thrashing when recalculating **         \n" : "" );
//    output << ( (          advanced ) ? "                  **      the inner kernel K0 (-kcy caches this).  **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -ktx i          - obtain (transfer) this kernel from ML i.           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- MLM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -ktk i          - which layer kernel is set above (-1 output, deflt).\n" : "" );
    output << ( ( basic || advanced ) ? "                           Use -ktk -1 to adjust the inheritance type (must be\n" : "" );
    output << ( ( basic || advanced ) ? "                           a type 8xx kernel, default is 802).                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- gentype specific options                      --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** When dealing  with generic targets  MLs there **         \n" : "" );
    output << ( (          advanced ) ? "                  ** may also be kernel defined on target space to **         \n" : "" );
    output << ( (          advanced ) ? "                  ** measure  similarity.   To modify  this kernel **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the above commands but with the prefix -e.    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** For example:                                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **  -ekt sets output kernel type                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **  -eks sets parameter r0 for kernel            **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** etc.                                          **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -e...   sets kernel parameters for output kernels.                   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- RFF random feature similarity options         --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** When dealing with RFF with feature tuning the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** similarity between rndom features is measured **         \n" : "" );
    output << ( (          advanced ) ? "                  ** using this kernel accessed by the -r prefix.  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** For example:                                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **  -rkt sets RFF kernel type                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **  -rks sets parameter r0 for kernel            **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** etc.                                          **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -r...   sets kernel parameters for output kernels.                   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Automatic parameter tuning options (after kernel selection):                  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -bal            - for all classes i, set Ci = N/Ni.                  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tkL   m        - Automatically tune kernel parameters (well, some of\n" : "" );
    output << ( (          advanced ) ? "                           them) to maximise log-likelihood.  m is the maximum\n" : "" );
    output << ( (          advanced ) ? "                           length-scale parameter.                            \n" : "" );
    output << ( (          advanced ) ? "         -tkloo m        - Like -tkL, but minimises leave-one-out error.      \n" : "" );
    output << ( (          advanced ) ? "         -tkrec m        - Like -tkL, but minimises recall error.             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           NB: if you don't want a particular parameter tuned,\n" : "" );
    output << ( (          advanced ) ? "                               use eg  -kd 1.2c. The  c here  makes the  value\n" : "" );
    output << ( (          advanced ) ? "                               nominally consant, so it will be left alone.   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tcL   m        - Like -tkL but for C praameter.                     \n" : "" );
    output << ( (          advanced ) ? "         -tcloo m        - Like -tkloo but for C parameter.                   \n" : "" );
    output << ( (          advanced ) ? "         -tcrec m        - Like -tkrec but for C parameter.                   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -teL   m        - Like -tkL but for eps parameter.                   \n" : "" );
    output << ( (          advanced ) ? "         -teloo m        - Like -tkloo but for eps parameter.                 \n" : "" );
    output << ( (          advanced ) ? "         -terec m        - Like -tkrec but for eps parameter.                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tceL   m       - Like -tkL but for C and eps parameter.             \n" : "" );
    output << ( (          advanced ) ? "         -tceloo m       - Like -tkloo but for C and eps parameter.           \n" : "" );
    output << ( (          advanced ) ? "         -tcerec m       - Like -tkrec but for C and eps parameter.           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tkeL   m       - Like -tkL but for K and eps parameters.            \n" : "" );
    output << ( (          advanced ) ? "         -tkeloo m       - Like -tkloo but for K and eps parameters.          \n" : "" );
    output << ( (          advanced ) ? "         -tkerec m       - Like -tkrec but for K and eps parameters.          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tkcL   m       - Like -tkL but for K and C parameters.              \n" : "" );
    output << ( (          advanced ) ? "         -tkcloo m       - Like -tkloo but for K and C parameters.            \n" : "" );
    output << ( (          advanced ) ? "         -tkcrec m       - Like -tkrec but for K and C parameters.            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tkceL   m      - Like -tkL but for K, C and eps parameters.         \n" : "" );
    output << ( (          advanced ) ? "         -tkceloo m      - Like -tkloo but for K, C and eps parameters.       \n" : "" );
    output << ( (          advanced ) ? "         -tkcerec m      - Like -tkrec but for K, C and eps parameters.       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** These  options set  parameters in  such a way **         \n" : "" );
    output << ( (          advanced ) ? "                  ** that  they  will  change  automatically  when **         \n" : "" );
    output << ( (          advanced ) ? "                  ** relevant parameters (N,kern) are changed.  So **         \n" : "" );
    output << ( (          advanced ) ? "                  ** for example -NlA will modify learning options **         \n" : "" );
    output << ( (          advanced ) ? "                  ** appropriately when additionl training vectors **         \n" : "" );
    output << ( (          advanced ) ? "                  ** are  added or  removed.  More  importantly it **         \n" : "" );
    output << ( (          advanced ) ? "                  ** will  automatically   modify  the  parameters **         \n" : "" );
    output << ( (          advanced ) ? "                  ** during n-fold-error analysis, for example.    **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -cA             - C/N = 1/(N*mean(kern(i,i))) (updated auto).        \n" : "" );
    output << ( (          advanced ) ? "         -cB             - C/N = 1/(N*median(kern(i,i))) (updated auto).      \n" : "" );
    output << ( (          advanced ) ? "         -cAN            - C/N = 1/mean(kern(i,i)) (updated auto).            \n" : "" );
    output << ( (          advanced ) ? "         -cBN            - C/N = 1/median(kern(i,i)) (updated auto).          \n" : "" );
    output << ( (          advanced ) ? "         -cX  x          - C/N = x/N (updated auto).                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -NlA nu C       - automatically  set  linear  bias  forcing  for  the\n" : "" );
    output << ( (          advanced ) ? "                           1-class and  CS++-SVM given  nu and C as  for those\n" : "" );
    output << ( (          advanced ) ? "                           formulations,  with  N and  n as  for  the  current\n" : "" );
    output << ( (          advanced ) ? "                           trained SVM.  For  the CS++-SVM this  is equivalent\n" : "" );
    output << ( (          advanced ) ? "                           to:                                                \n" : "" );
    output << ( (          advanced ) ? "                           -c C(n-1)/(N.nu) -w sqrt((n-1)/(2.(n-2))) -Nld d C \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           where d is  as set  by the  -Acz d  call.  For  the\n" : "" );
    output << ( (          advanced ) ? "                           1-class SVM this is equivalent to:                 \n" : "" );
    output << ( (          advanced ) ? "                           -c C/(N.nu) -w 0 -Nl -C                            \n" : "" );
    output << ( (          advanced ) ? "                           (update auto).                                     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -cua            - turns off auto update if currently in use.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Grid search parameter selection (after automatic parameter tuning):           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -g   ...        - select  given parameters  using a  grid  search  to\n" : "" );
    output << ( (          advanced ) ? "                           minimise some error measure.  Arguments are:       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           -g nargs $evalstring $setstring ...                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           where evalstring is the string used when evaluating\n" : "" );
    output << ( (          advanced ) ? "                           a  particular  choice and  setstring  is used  when\n" : "" );
    output << ( (          advanced ) ? "                           setting the final optimal  parameter choice.  nargs\n" : "" );
    output << ( (          advanced ) ? "                           sets the number  of parameters being  tuned, and in\n" : "" );
    output << ( (          advanced ) ? "                           the strings  themselves  the  variable  var(0,n) is\n" : "" );
    output << ( (          advanced ) ? "                           used to represent  these parameters,  where n is an\n" : "" );
    output << ( (          advanced ) ? "                           argnum except 0.  So for example:                  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "             -g 2 \"-c y -kd z -tx\" \"-c y -kd z\" ...                           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           tells svmheavy to  do a grid search  over C and the\n" : "" );
    output << ( (          advanced ) ? "                           kernel parameter d (r2), as represented by var(0,1)\n" : "" );
    output << ( (          advanced ) ? "                           (y) and  var(0,2) (z)  respectively,and  then set C\n" : "" );
    output << ( (          advanced ) ? "                           and d to the  optimal value resulting  from this as\n" : "" );
    output << ( (          advanced ) ? "                           measured using leave-one-out error (-tx).          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           The  ranges are  set in  quadruples,  one for  each\n" : "" );
    output << ( (          advanced ) ? "                           var(0,n), after the first three arguments, namely: \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           ... = t n m M I                                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           where t sets the argument type, n is the arg num, m\n" : "" );
    output << ( (          advanced ) ? "                           is the minimum  value, M the maximum  value, with I\n" : "" );
    output << ( (          advanced ) ? "                           steps.  Valid options for the type t are:          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           zb - parameter is integer, linear increments.      \n" : "" );
    output << ( (          advanced ) ? "                           zl - parameter is integer, logarithmic increments. \n" : "" );
    output << ( (          advanced ) ? "                           za - parameter is integer, exponential increments. \n" : "" );
    output << ( (          advanced ) ? "                           zc - parameter is integer, inverse logistic incr.  \n" : "" );
    output << ( (          advanced ) ? "                           zr - parameter is integer, random increms (-g only)\n" : "" );
    output << ( (          advanced ) ? "                           fb - parameter is real, linear increments.         \n" : "" );
    output << ( (          advanced ) ? "                           fl - parameter is real, logarithmic increments.    \n" : "" );
    output << ( (          advanced ) ? "                           fa - parameter is real, exponential increments.    \n" : "" );
    output << ( (          advanced ) ? "                           fc - parameter is real, inverse logistic incr.     \n" : "" );
    output << ( (          advanced ) ? "                           fr - parameter is real, random increments (-g only)\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           so the grid search  is done on t in  the range 0,1,\n" : "" );
    output << ( (          advanced ) ? "                           and t = 0.01 => x = m,  t = 0.99 => x = M, t = 0 =>\n" : "" );
    output << ( (          advanced ) ? "                           x < -1e12, t = 1 => x > 1e12.                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           In the above example we might say:                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "             -g 2 \"-c y -kd z -tx\" \"-c y -kd z\" fl 1 1e-2 1e2 10 zb 2 1 5 5   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           which tells svmheavy to do a grid search over C and\n" : "" );
    output << ( (          advanced ) ? "                           kernel parameter  d, with C ranging  over 10 values\n" : "" );
    output << ( (          advanced ) ? "                           on a log scale from 0.01 to 100 and d Selected_from\n" : "" );
    output << ( (          advanced ) ? "                           1,2,3,4,5, find  the optimal selection  to minimise\n" : "" );
    output << ( (          advanced ) ? "                           leave-one-out error, then set C and d to the values\n" : "" );
    output << ( (          advanced ) ? "                           so selected.                                       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           In the  case of a  non-unique minimum,  the minimum\n" : "" );
    output << ( (          advanced ) ? "                           closest  to the  centre  of the  grid is  selected.\n" : "" );
    output << ( (          advanced ) ? "                           This is based on  the assumption that  the grid has\n" : "" );
    output << ( (          advanced ) ? "                           been chosen  such that the central  values are more\n" : "" );
    output << ( (          advanced ) ? "                           practical (or  likely) than the edge  values.  This\n" : "" );
    output << ( (          advanced ) ? "                           distance is stored as var(1,0).                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           These  functions can  do more  than just  parameter\n" : "" );
    output << ( (          advanced ) ? "                           selection - they  can also  do global  optimisation\n" : "" );
    output << ( (          advanced ) ? "                           more generally.  For example the command:          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "             -gd 2 \"-tM y^2+(z-1)^2\" \"-echo y -echo z\" fb 1 -2 3 1 fb 2 -2 4 1\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           will minimise the function y^2+(z-1)^2 using DIRect\n" : "" );
    output << ( (          advanced ) ? "                           and echo the result (note that y = var(0,1) and z =\n" : "" );
    output << ( (          advanced ) ? "                           var(0,2); and that  x = var(0,0) cannot  be used as\n" : "" );
    output << ( (          advanced ) ? "                           it is reserved).  See -tM for details.  Another eg:\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "             -gb 1 \"-fu 2 22 y -Zx -tM z\" \"-echo y\" fb 1 0 1 1                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           or, equivalently:                                  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "             -gb 1 \"-tM testfn(22,y)\" \"-echo y\" fb 1 0 1 1                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           will  minimise  test  function  22  using  Bayesian\n" : "" );
    output << ( (          advanced ) ? "                           optimisation.  Finally, in mex,  if f is a function\n" : "" );
    output << ( (          advanced ) ? "                           handle - eg:                                       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             f = @(x) x(1)^2+(x(2)-1)^2                       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           (note that it takes a vector argument) then you can\n" : "" );
    output << ( (          advanced ) ? "                           minimise in matlab using for example:              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                            svmmatlab('-gd 2 \"-fWM 4 0 [ y z ] -tM var(0,4)\"  \n" : "" );
    output << ( (          advanced ) ? "                             \"-echo y -echo z\" fb 1 -2 3 1 fb 2 -2 4 1',1,f)  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Alternatively if you have a function bayestest.m:  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             function x = bayesTest(y,z)                      \n" : "" );
    output << ( (          advanced ) ? "                             x = y^2+(z-1)^2;                                 \n" : "" );
    output << ( (          advanced ) ? "                             end                                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Then you could use:                                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             f = @(x) bayestest(x(1),x(2))                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Apart from grid optimisation the following exist:  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gd  ...        - like  above, but uses DIRect global optimiser.     \n" : "" );
    output << ( (          advanced ) ? "         -gN  ...        - like  above, but uses Nelder-Mead local optimiser. \n" : "" );
    output << ( (          advanced ) ? "         -gb  ...        - like  above, but uses Bayesian optimisation.       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Overall algorithm  (steps in brackets  for Bayesian\n" : "" );
    output << ( (          advanced ) ? "                           or model-based only):                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                          (0. Set up models for Bayesian optimisation.)       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           1. Set up  projections.  The core  optimiser always\n" : "" );
    output << ( (          advanced ) ? "                              sees  a finite  dimensional  problem  on [0,1]^d\n" : "" );
    output << ( (          advanced ) ? "                              with linear scaling, specifically:              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - min_{x in [0,1]^d} f(q(p(x)))                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              where:                                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - p : [0,1]^d  ->  R^d is  a finite  dimensional\n" : "" );
    output << ( (          advanced ) ? "                                projection operator  defined by fb,fl,... etc.\n" : "" );
    output << ( (          advanced ) ? "                                z... variants just round to nearest integer.  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - q : R^d -> F is present  for random projection\n" : "" );
    output << ( (          advanced ) ? "                                and functional optimisation. See -gp -gP, ....\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           2. Run $pstring defined by -gtp.   This can be used\n" : "" );
    output << ( (          advanced ) ? "                              to set  non-standard  kernels  etc for  Bayesian\n" : "" );
    output << ( (          advanced ) ? "                              optimisation and  functional optimisation.   The\n" : "" );
    output << ( (          advanced ) ? "                              following indices are set (-1 where/if n/a):    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,0): model for Bayesian optimisation.   \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,1): noise model for Bayesian optim..   \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,2): weighting projection template.     \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,5): source data model (env-GP,diff-GP).\n" : "" );
    output << ( (          advanced ) ? "                              - var(90,6): difference data model (diff-GP).   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              var(90,2)  and  var(90,3) are  used  for  random\n" : "" );
    output << ( (          advanced ) ? "                              projection and functional  analysis, where q has\n" : "" );
    output << ( (          advanced ) ? "                              the form:                                       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - q(x)(t) = [ x_0.q_0(t) + x_1.q_1(t) + ... ]   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              where:                                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - x is the vector p(x) in R^d.                  \n" : "" );
    output << ( (          advanced ) ? "                              - q_i is a draw from ML var(90,3).              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              Typically ML  var(90,3) will be  a random vector\n" : "" );
    output << ( (          advanced ) ? "                              distribution, a GP  (function distribution) or a\n" : "" );
    output << ( (          advanced ) ? "                              set of basis functions (eg Bernstein polys).    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           3. Random  projection:  draw  q_0,  q_1,  ...  from\n" : "" );
    output << ( (          advanced ) ? "                              defined distribution (template ML var(90,3)).   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           4. Run $mstring defined by  -gtP.  This can be used\n" : "" );
    output << ( (          advanced ) ? "                              used  for  things  like  tweaking  the  template\n" : "" );
    output << ( (          advanced ) ? "                              distributions between inner-loop opts, so for eg\n" : "" );
    output << ( (          advanced ) ? "                              you could start with a very smooth SE kernel and\n" : "" );
    output << ( (          advanced ) ? "                              gradually make  it sharper.  The  following vars\n" : "" );
    output << ( (          advanced ) ? "                              are defined here (-1 if n/a):                   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,0): model for Bayesian optimisation.   \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,1): noise model for Bayesian optim.    \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,2): weighting projection template.     \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,3): current q(t) function.             \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,4): iteration  count  (0 first  time, 1\n" : "" );
    output << ( (          advanced ) ? "                                           second time etc... up to -gpr).    \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,5): source data model (env,diff-GP).   \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,6): difference data model (diff-GP).   \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,7): raw   iteration   counter   (unlike\n" : "" );
    output << ( (          advanced ) ? "                                           var(90,4)  this  starts  at  1  and\n" : "" );
    output << ( (          advanced ) ? "                                           increments  for  every  evaluation,\n" : "" );
    output << ( (          advanced ) ? "                                           including random initial tests etc)\n" : "" );
    output << ( (          advanced ) ? "                              - var(90,8): raw start  time (seconds,  with ref\n" : "" );
    output << ( (          advanced ) ? "                                           to some arbitrary point in time.   \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,9): x augmentation model (if used).    \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,10): equality constraint model (\").    \n" : "" );
    output << ( (          advanced ) ? "                              - var(90,11): inequality constraint model (\").  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              Note  that   changes  here  affect   the  *next*\n" : "" );
    output << ( (          advanced ) ? "                              projection step 3, not the current one.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           5. Inner  optimiser to  solve  using relevant  alg,\n" : "" );
    output << ( (          advanced ) ? "                              where evaluation of f calls $evalstring:        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - min_{x in [0,1]^d} f(q(p(x)))                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              Available variables are as-per step 4.          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           6. Repeat from step 3 as defined by -gpr, ie:      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              - -gpr 0: run steps 3-5 once.                   \n" : "" );
    output << ( (          advanced ) ? "                              - -gpr 1: run steps 3-5 twice.                  \n" : "" );
    output << ( (          advanced ) ? "                              - -gpr 2: run steps 3-5 thrice.                 \n" : "" );
    output << ( (          advanced ) ? "                                   ...                                        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           Notes:                                             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - non-trivial results are  returned for model based\n" : "" );
    output << ( (          advanced ) ? "                             methods in the following format:                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         { y v [cg1..cgm] x' q [xx1..xxn] xf [xf1..xfn] xff xf3 t [tg1..tgm] }\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             where:                                           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             + y is the usual result. You can set y=null which\n" : "" );
    output << ( (          advanced ) ? "                               will prevent the sample from being added to the\n" : "" );
    output << ( (          advanced ) ? "                               function model (eg if c(x)<0).                 \n" : "" );
    output << ( (          advanced ) ? "                             + v is the variance of the measurement noise.    \n" : "" );
    output << ( (          advanced ) ? "                             + [ cg1 .. cgn ] for inequality constraints (want\n" : "" );
    output << ( (          advanced ) ? "                               cgi(x) > 0 for i=1,2,...,n.  See -cmgt).       \n" : "" );
    output << ( (          advanced ) ? "                             + x' if non-null, this indicates where the actual\n" : "" );
    output << ( (          advanced ) ? "                               evaluation happened (if y=f(x'), not y=f(x)).  \n" : "" );
    output << ( (          advanced ) ? "                             + q if non-null and nz, stop optim immediately.  \n" : "" );
    output << ( (          advanced ) ? "                             + [ xx1 ... xxn ] is side-channel data (see -gmsc\n" : "" );
    output << ( (          advanced ) ? "                               etc for information.                           \n" : "" );
    output << ( (          advanced ) ? "                             + xf is for  rank observations  (that is,  rather\n" : "" );
    output << ( (          advanced ) ? "                               than  an  observation   g(x) = y,  this  is  an\n" : "" );
    output << ( (          advanced ) ? "                               observation g(x) - g(xf) = y.                  \n" : "" );
    output << ( (          advanced ) ? "                             + [ xf1 ... xfn ] is side-channel data on xf.    \n" : "" );
    output << ( (          advanced ) ? "                             + xff is for gradient observations.              \n" : "" );
    output << ( (          advanced ) ? "                             + xf3 is for augmented observations.             \n" : "" );
    output << ( (          advanced ) ? "                             + t observation type (0 n/a, +1 >=, -1 <=, 2 ==).\n" : "" );
    output << ( (          advanced ) ? "                             + tcg like t but for [cg1..cgn].                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             Not all additional data is required. Use null for\n" : "" );
    output << ( (          advanced ) ? "                             default operation ( or [ ] for vectors).         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - be wary  of using  -fo, -foe, -AAi...  and -tI...\n" : "" );
    output << ( (          advanced ) ? "                             here as they will  not work as  expected (vectors\n" : "" );
    output << ( (          advanced ) ? "                             taken out of  files in one iteration will  not be\n" : "" );
    output << ( (          advanced ) ? "                             put back for the next).                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - $evalstring etc work  as function calls.  Changes\n" : "" );
    output << ( (          advanced ) ? "                             to (non-global)  variables made  during each call\n" : "" );
    output << ( (          advanced ) ? "                             will not  be saved.  If you  want to  return vars\n" : "" );
    output << ( (          advanced ) ? "                             from these calls use global variables - eg:      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "             -gb 2 \"-tM y^2+(z-1)^2\" \"-fWG 10 y -fWG 11 z\" fb 1 -2 3 1        \n" : "" );
    output << ( (          advanced ) ? "                                                                   fb 2 -2 4 1\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             will store  y and z  in global  vars that  can be\n" : "" );
    output << ( (          advanced ) ? "                             retrieved, for example using:                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                    -fWg 1 var(0,10) -fWg 2 var(0,11)         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - multi-objective  optimisation  is possible  using\n" : "" );
    output << ( (          advanced ) ? "                             the  Bayesian optimiser  and  an appropriate  IMP\n" : "" );
    output << ( (          advanced ) ? "                             (see -gbq below) and the -tm ... option, e.g.    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                    -tm [ var(1,37) var(1,42) ] -tc 5         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             evaluates   performance    with   5-fold   cross-\n" : "" );
    output << ( (          advanced ) ? "                             validation and  seeks to  minimise both  negative\n" : "" );
    output << ( (          advanced ) ? "                             accuracy   (var(1,37))  and   negative   sparsity\n" : "" );
    output << ( (          advanced ) ? "                             (var(1,42))  (which is  equivalent to  maximising\n" : "" );
    output << ( (          advanced ) ? "                             accuracy and sparsity.  The Pareto set is written\n" : "" );
    output << ( (          advanced ) ? "                             to logfilename.pareto).                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - complete results are stored in grid files:       \n" : "" );
    output << ( (          advanced ) ? "                             ....xgrid - x values tested (inputs)             \n" : "" );
    output << ( (          advanced ) ? "                             ....fgrid - f(x) values evaluated (outputs)      \n" : "" );
    output << ( (          advanced ) ? "                             ....mgrid - f(x) adjust for feasibility/unscented\n" : "" );
    output << ( (          advanced ) ? "                             ....igrid - variable indexes                     \n" : "" );
    output << ( (          advanced ) ? "                             ....sgrid - suplementary (depend on optimiser)   \n" : "" );
    output << ( (          advanced ) ? "                             ....hgrid - hypervolume up to iteration          \n" : "" );
    output << ( (          advanced ) ? "                             ....qgrid - stability scores                     \n" : "" );
    output << ( (          advanced ) ? "                             In each  file  the optimum value (or values)  are\n" : "" );
    output << ( (          advanced ) ? "                             marked with  an asterix.  Further,  mean/variance\n" : "" );
    output << ( (          advanced ) ? "                             over multiple runs are written to ....fgrid_mean,\n" : "" );
    output << ( (          advanced ) ? "                             ....fgrid_var, ....mgrid_mean  and ....mgrid_var.\n" : "" );
    output << ( (          advanced ) ? "                             Pareto only variant (eg  xpareto, not  xgrid) are\n" : "" );
    output << ( (          advanced ) ? "                             also written (where relevant) containing just the\n" : "" );
    output << ( (          advanced ) ? "                             pareto set data.                                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             Supplementary results are (bayesian):            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                    [ tstart = start time of iteration (sec) ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ tend   = end time of iteration (sec)   ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ nrec   = rec. number in this batch     ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ beta   = beta value for this batch     ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ mu     = mu(x) for this recomm.        ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ sigma  = sigma(x) for this recomm.     ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ UCB    = UCB(x) for this recomm.       ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ LCB    = LCB(x) for this recomm.       ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ DVAR   = DVAR(x) for this recomm.      ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ UDIST  = UDIST(x) for this recomm.     ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ r      = r(x) for this recomm.         ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ dtime  = DIRect run time (sec).        ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ mutime = mu GP training time (sec).    ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ sitime = sigma GP training time (sec). ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ ftime  = function evaluation time (sec)]\n" : "" );
    output << ( (          advanced ) ? "                                    [ gridi  = grid index of point (if grid) ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ gridy  = grid value of point (if grid) ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ B      = RKHS norm of posterior mean   ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ mig    = max info gain of posterior    ]\n" : "" );
    output << ( (          advanced ) ? "                                    [ fidcost= fidelity cost used            ]\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             noting that mu is modelled on -f(x); and:        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                    UCB(x)   = mu(x) + sqrt(beta).sigma(x)    \n" : "" );
    output << ( (          advanced ) ? "                                    LCB(x)   = mu(x) - sqrt(beta).sigma(x)    \n" : "" );
    output << ( (          advanced ) ? "                                    DVAR(x)  = 2.sqrt(beta).sigma(x)          \n" : "" );
    output << ( (          advanced ) ? "                                    UDIST(x) = -fmin - -f(x)                  \n" : "" );
    output << ( (          advanced ) ? "                                    r(x)     = min(UDIST(X),DVAR(X))          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - also saved in ....xygrid,  which is in y x format\n" : "" );
    output << ( (          advanced ) ? "                             ready to train another  model, and xytgrid, which\n" : "" );
    output << ( (          advanced ) ? "                             includes variance for GP training.               \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - you can  recurse these  (grid search  within grid\n" : "" );
    output << ( (          advanced ) ? "                             search) to arbitrary depth.                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           - when evaluating the setstring and  after the vars\n" : "" );
    output << ( (          advanced ) ? "                             that have been optimised are stored:             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                    var(50,0): optimal x vector.              \n" : "" );
    output << ( (          advanced ) ? "                                    var(51,0): optimal f(x).                  \n" : "" );
    output << ( (          advanced ) ? "                                    var(52,0): optimal index.                 \n" : "" );
    output << ( (          advanced ) ? "                                    var(53,0): optimal supplement result.     \n" : "" );
    output << ( (          advanced ) ? "                                    var(54,0): optimal hypervolume.           \n" : "" );
    output << ( (          advanced ) ? "                                    var(55,0): mean f(x) (see -gr).           \n" : "" );
    output << ( (          advanced ) ? "                                    var(56,0): reserved for mean hypervolume. \n" : "" );
    output << ( (          advanced ) ? "                                    var(57,0): mean index (time to min f(x)). \n" : "" );
    output << ( (          advanced ) ? "                                    var(58,0): mean iterations to softmin.    \n" : "" );
    output << ( (          advanced ) ? "                                    var(59,0): mean iterations to hardmin.    \n" : "" );
    output << ( (          advanced ) ? "                                               (variances in var(..,65536)).  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                    var(6x,...): as  for var(5x,...),  but all\n" : "" );
    output << ( (          advanced ) ? "                                                 pareto-optimal results.      \n" : "" );
    output << ( (          advanced ) ? "                                    var(7x,...): as  for var(6x,...),  but all\n" : "" );
    output << ( (          advanced ) ? "                                                 results are included.        \n" : "" );
    output << ( (          advanced ) ? "                                    var(8x,0): as for var(7x,...) in one.     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                    var(90,0): SMBO model index (or -1).      \n" : "" );
    output << ( (          advanced ) ? "                                    var(90,1): SMBO sigma model index (or -1).\n" : "" );
    output << ( (          advanced ) ? "                                    var(90,2): optimal function index (or -1).\n" : "" );
    output << ( (          advanced ) ? "                                    var(90,3): functional model.              \n" : "" );
    output << ( (          advanced ) ? "                                    var(90,4): iteration count.               \n" : "" );
    output << ( (          advanced ) ? "                                    var(90,5): source data model (env,dif-GP).\n" : "" );
    output << ( (          advanced ) ? "                                    var(90,6): difference data model (dif-GP).\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             Note that var(50,0) and var(53,0) are expanded.  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Non-trivial optimisation examples:            **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1. Minimise   y^2 + (z-1)^2  using   Bayesian **         \n" : "" );
    output << ( (          advanced ) ? "                  **    optimisation:                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gmd 0.01 -gbH 3 -gb 2 \"-tM y^2+(z-1)^2\"      **         \n" : "" );
    output << ( (          advanced ) ? "                  **  \"-echo y -echo z\" fb 1 -2 3 1 fb             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2 -2 4 1                                      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **  - -gmd 0.01 sets the  (nominal) noise in the **         \n" : "" );
    output << ( (          advanced ) ? "                  **    target; and while the target is noiseless, **         \n" : "" );
    output << ( (          advanced ) ? "                  **    it is best to select non-zero to avoid bad **         \n" : "" );
    output << ( (          advanced ) ? "                  **    conditioning on the kernel matrix.         **         \n" : "" );
    output << ( (          advanced ) ? "                  **  - -gbH 3 selects GP-UCB optimisation.        **         \n" : "" );
    output << ( (          advanced ) ? "                  **  - -tM ... specifies the target function.     **         \n" : "" );
    output << ( (          advanced ) ? "                  **  - fb 1 ...  fb 2 ... specifies the variables **         \n" : "" );
    output << ( (          advanced ) ? "                  **    (y and z) and their ranges.                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2. As  for example  1,  but  this time  using **         \n" : "" );
    output << ( (          advanced ) ? "                  **    random   projections  (REMBO   style)  for **         \n" : "" );
    output << ( (          advanced ) ? "                  **    demonstrative purposes:                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gmd 0.01 -gp urand([ -2 -2 ],[ 2 2 ]) -gbH 3 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gb 2 \"-tM derefv(y,0)^2+(derefv(y,1)-1)^2\"   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** \"-echo y\" fb 1 -2 3 1 fb 2 -2 4 1             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    In  this variant  the  optimiser  searches **         \n" : "" );
    output << ( (          advanced ) ? "                  **    over y,z, but the actual target becomes:   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **          f( y.r0 + z.r1 )                     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    where r0 and r1 are  both vectors from the **         \n" : "" );
    output << ( (          advanced ) ? "                  **    (uniform) distribution U([-2,-2],[2,2]) as **         \n" : "" );
    output << ( (          advanced ) ? "                  **    specified by -gp ...  The number of random **         \n" : "" );
    output << ( (          advanced ) ? "                  **    vectors ri  in this sum  and the  range of **         \n" : "" );
    output << ( (          advanced ) ? "                  **    their weights  are controlled by  the args **         \n" : "" );
    output << ( (          advanced ) ? "                  **    in -gb ...                                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Note  that the  function minimised  is now **         \n" : "" );
    output << ( (          advanced ) ? "                  **    written in terms of  the components of y - **         \n" : "" );
    output << ( (          advanced ) ? "                  **    ie derefv(y,0) and  derefv(y,1) - which is **         \n" : "" );
    output << ( (          advanced ) ? "                  **    a  vector  (y   here  because   the  first **         \n" : "" );
    output << ( (          advanced ) ? "                  **    in the -gb ... expression is y = var(0,1). **         \n" : "" );
    output << ( (          advanced ) ? "                  **    In general it will be var(0,i), where i is **         \n" : "" );
    output << ( (          advanced ) ? "                  **    provided by the first var defined in -gb   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 3. As for  example 1,  but using  Kirschner's **         \n" : "" );
    output << ( (          advanced ) ? "                  **    method  of  repeated  1-d  subspaces  (see **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Adaptive and Safe Bayesian Optimization in **         \n" : "" );
    output << ( (          advanced ) ? "                  **    High   Dimensions    via   One-Dimensional **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Subspaces):                                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gpr 3 -gmd 0.01 -gp urand([ -2 -2 ],[ 2 2 ]) **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbH 3 -gb 1                                  **         \n" : "" );
    output << ( (          advanced ) ? "                  ** \"-tM derefv(y,0)^2+(derefv(y,1)-1)^2\"         **         \n" : "" );
    output << ( (          advanced ) ? "                  ** \"-echo y\" fb 1 -2 3 1                         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    The main distinction between this and eg 2 **         \n" : "" );
    output << ( (          advanced ) ? "                  **    is that -gpr 3 tells  the optimiser to run **         \n" : "" );
    output << ( (          advanced ) ? "                  **    an additional 3 times  in sequence (K-1 in **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Kirschner)  with  a new  random  direction **         \n" : "" );
    output << ( (          advanced ) ? "                  **    from the previous  best.  We use -g 1 here **         \n" : "" );
    output << ( (          advanced ) ? "                  **    to specify  line-search,  but you  can use **         \n" : "" );
    output << ( (          advanced ) ? "                  **    more to specify narg-dim subspace search.  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 4. Functional  optimisation using the  method **         \n" : "" );
    output << ( (          advanced ) ? "                  **    of   random  subspaces    (target  is   to **         \n" : "" );
    output << ( (          advanced ) ? "                  **    replicate the sin(x) on range [0,1]):      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gf 1 -gpr 5 -gmd 0.01 -gP 0.5 -gbH 3 -gb 1   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** \"-tM norm2(sin(2*pi()*x)-y)\" \"-echo y\"        **         \n" : "" );
    output << ( (          advanced ) ? "                  ** fb 1 -10 10 1 -Zx -qw var(90,2) -Zx Zinteract **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    In this case,  rather than -gp  to specify **         \n" : "" );
    output << ( (          advanced ) ? "                  **    a distribution for a  random-vector basis, **         \n" : "" );
    output << ( (          advanced ) ? "                  **    we use  -gP 0.5 to  specify  a GP  with SE **         \n" : "" );
    output << ( (          advanced ) ? "                  **    kernel  with  lengthscale  0.5 from  which **         \n" : "" );
    output << ( (          advanced ) ? "                  **    random  functions  are to  be drawn.  Note **         \n" : "" );
    output << ( (          advanced ) ? "                  **    also that  -gf 1 specifies that  functions **         \n" : "" );
    output << ( (          advanced ) ? "                  **    are to be treated as scalar functions, not **         \n" : "" );
    output << ( (          advanced ) ? "                  **    zero-variance  distributions, allowing  us **         \n" : "" );
    output << ( (          advanced ) ? "                  **    evaluate  norm2(sin(2*pi()*x)-y) to  real. **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Finally,  -Zinteract  allows you  to  test **         \n" : "" );
    output << ( (          advanced ) ? "                  **    the resulting function.                    **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Generic parameter search options.             **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gao fn         - rather than define the function above just use this\n" : "" );
    output << ( (          advanced ) ? "                           function.  Note that  the arguments  here are  just\n" : "" );
    output << ( (          advanced ) ? "                           x,y,z,..., in the order defined.                   \n" : "" );
    output << ( (          advanced ) ? "         -gan            - normal operation (reverses -gao).  This is default.\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gy  t          - max training  time for  search alg (in  seconds - 0\n" : "" );
    output << ( (          advanced ) ? "                           for no limit, default).                            \n" : "" );
    output << ( (          advanced ) ? "         -gxs [...]      - initial value of x vector (default []).            \n" : "" );
    output << ( (          advanced ) ? "         -gfm l          - min value for function, stop if f<=l (deflt -inf). \n" : "" );
    output << ( (          advanced ) ? "         -gfu m          - max value of function, stop if f>=l (deflt +inf).  \n" : "" );
    output << ( (          advanced ) ? "         -gfM l          - soft minimum value of  function (don't stop, but is\n" : "" );
    output << ( (          advanced ) ? "                           used by some variants of for example Bayesian opt).\n" : "" );
    output << ( (          advanced ) ? "         -gfU l          - soft max value  for function.  If  exceeded, result\n" : "" );
    output << ( (          advanced ) ? "                           clipped (that is, res = max(f(x),l) (deflt +inf).  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gr  n          - number of repeats (default 1).  If > 1 then results\n" : "" );
    output << ( (          advanced ) ? "                           for the  final repeat  are  returned,  except fgrid\n" : "" );
    output << ( (          advanced ) ? "                           values are replaced by [ fmean, fvar ], and fres is\n" : "" );
    output << ( (          advanced ) ? "                           meaningless.                                       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gnp            - set no projection (default).                       \n" : "" );
    output << ( (          advanced ) ? "         -gp  $fn        - set  projection.  If  set,  what  you are  actually\n" : "" );
    output << ( (          advanced ) ? "                           optimising   is  f(p(x)),  where  p   is  a  random\n" : "" );
    output << ( (          advanced ) ? "                           projection of type:                                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              p(x) = [ x_0.p_0 + x_1.p_1 + ... ]              \n" : "" );
    output << ( (          advanced ) ? "                                     [ x_1                     ]              \n" : "" );
    output << ( (          advanced ) ? "                                     [ x_2                     ]              \n" : "" );
    output << ( (          advanced ) ? "                                     [ ...                     ]              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           where p_i  is taken from  the distribution $fn  (eg\n" : "" );
    output << ( (          advanced ) ? "                           you could used grand([0 0],M:[1 0 ; 0 1]) for norml\n" : "" );
    output << ( (          advanced ) ? "                           random  vectors, or urand([ 0 0 0 ],[ 1 1 1 ])  for\n" : "" );
    output << ( (          advanced ) ? "                           uniform. Note that in this scheme the first element\n" : "" );
    output << ( (          advanced ) ? "                           in -g may be anything valued  (but is vector valued\n" : "" );
    output << ( (          advanced ) ? "                           in the example).                                   \n" : "" );
    output << ( (          advanced ) ? "         -gP  g          - like -gp but in this case p_i are functions sampled\n" : "" );
    output << ( (          advanced ) ? "                           from a GP with RBF kernel of lengthscale g.        \n" : "" );
    output << ( (          advanced ) ? "         -gPk...         - set kernel parameters on GP for -gP.               \n" : "" );
    output << ( (          advanced ) ? "         -gpb            - like -gp, but p_i are Bernstein basis polynomials. \n" : "" );
    output << ( (          advanced ) ? "         -gpB n          - like -gpb, but with  schedule.  Degree of Bernstein\n" : "" );
    output << ( (          advanced ) ? "                           starts at n, then increases  with every repeat (set\n" : "" );
    output << ( (          advanced ) ? "                           by -gpr).  Note that in this case random repeats do\n" : "" );
    output << ( (          advanced ) ? "                           not give an alternate re-projection. The max degree\n" : "" );
    output << ( (          advanced ) ? "                           is just the number of variables.                   \n" : "" );
    output << ( (          advanced ) ? "         -gph            - set RKHS projection.  Like -gp, except in this case\n" : "" );
    output << ( (          advanced ) ? "                           the function is in a random RKHS, that is:         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                              p(x) = [ sum_i x_i K(xx_i,x) ]                  \n" : "" );
    output << ( (          advanced ) ? "                                     [ x_1                 ]                  \n" : "" );
    output << ( (          advanced ) ? "                                     [ x_2                 ]                  \n" : "" );
    output << ( (          advanced ) ? "                                     [ ...                 ]                  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           where xx_i  are selected uniform  randomly [0,1] to\n" : "" );
    output << ( (          advanced ) ? "                           required dimension.                                \n" : "" );
    output << ( (          advanced ) ? "         -gphk...        - set kernel parameters on RKHS for -gph.            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gpd d          - for functional optimisation  (-gP, -gpb) by default\n" : "" );
    output << ( (          advanced ) ? "                           the function is of one variable.  This lets you set\n" : "" );
    output << ( (          advanced ) ? "                           d variables instead  (default 1).  Variables are x,\n" : "" );
    output << ( (          advanced ) ? "                           y,... (var(0,0), var(0,1), ...) and range [0,1].   \n" : "" );
    output << ( (          advanced ) ? "         -gpr n          - number of sequential random projections. If NZ then\n" : "" );
    output << ( (          advanced ) ? "                           n random subspaces will  be found in sequence, with\n" : "" );
    output << ( (          advanced ) ? "                           each having as \"point zero\" the previous best.  For\n" : "" );
    output << ( (          advanced ) ? "                           RKHS this acts  like n restarts (but  make sure you\n" : "" );
    output << ( (          advanced ) ? "                           clear the model between restarts (-gmt 1 or 2).    \n" : "" );
    output << ( (          advanced ) ? "         -gf  n          - selects how functions are treated by the kernel:   \n" : "" );
    output << ( (          advanced ) ? "                           0 - fns are treated as zero-mean distributions.    \n" : "" );
    output << ( (          advanced ) ? "                           1 - fns are treated as scalar functions @():f(x).  \n" : "" );
    output << ( (          advanced ) ? "         -gc  n          - include bias step:                                 \n" : "" );
    output << ( (          advanced ) ? "                           0 - fns as described previously.                   \n" : "" );
    output << ( (          advanced ) ? "                           1 - for first iteration, final p_... replaced by C.\n" : "" );
    output << ( (          advanced ) ? "         -gC  C          - constant used in -gc 1.                            \n" : "" );
    output << ( (          advanced ) ? "         -gns n          - number of pts in approx integration for scalar fns.\n" : "" );
    output << ( (          advanced ) ? "         -gkm m          - if m != 1 then function is actually an (indefinite)\n" : "" );
    output << ( (          advanced ) ? "                           m-kernel f(x1,x2,...,xm).                          \n" : "" );
    output << ( (          advanced ) ? "         -gkt t          - sample type:                                       \n" : "" );
    output << ( (          advanced ) ? "                           0: no constraints.                                 \n" : "" );
    output << ( (          advanced ) ? "                           1: positive (def) function (kernel) by spectr cut. \n" : "" );
    output << ( (          advanced ) ? "                           2: positive (def) function (kernel) by spectr flip.\n" : "" );
    output << ( (          advanced ) ? "                           3: negative (def) function (kernel) by spectr cut. \n" : "" );
    output << ( (          advanced ) ? "                           4: negative (def) function (kernel) by spectr flip.\n" : "" );
    output << ( (          advanced ) ? "                           5: symmetric indefinite kernel (otherwise like 0). \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gtp  $pstring  - string  to be  evaluated  at start  of  optim.  Put\n" : "" );
    output << ( (          advanced ) ? "                           fancy kernel setups etc here.  Evaluated once, just\n" : "" );
    output << ( (          advanced ) ? "                           before random projection (c/f -gpr).               \n" : "" );
    output << ( (          advanced ) ? "         -gtP  $mstring  - to be evaluated after  each random projection.  Put\n" : "" );
    output << ( (          advanced ) ? "                           kernel tweaking steps here.   Evaluated after inner\n" : "" );
    output << ( (          advanced ) ? "                           optimisation and random projections (c/f -gpr).    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gtx $xfn       - if set, the x  stored and logged is  not the x that\n" : "" );
    output << ( (          advanced ) ? "                           was  evaluated but  rather xfn(x).   null (default)\n" : "" );
    output << ( (          advanced ) ? "                           disables this  feature.  This is  evaluated *after*\n" : "" );
    output << ( (          advanced ) ? "                           $evalstring, which is handy for nested bayesian.   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -g+ [ $fn1 .. ] - Add penalty sum_i max(0,$fni(x))  to the objective.\n" : "" );
    output << ( (          advanced ) ? "                           For example -g+ [ absinf(x)-B ]  will add a penalty\n" : "" );
    output << ( (          advanced ) ? "                           term if ||p(x)||_inf > B.  This  can be helpful for\n" : "" );
    output << ( (          advanced ) ? "                           constraining projected searches.                   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gref $name     - name of this search (used when plotting results).  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gplot          - plot graph of all grid-searches in this batch.     \n" : "" );
    output << ( (          advanced ) ? "         -gpln name      - name of output of plot (deflt logfile.opt.ps/pdf). \n" : "" );
    output << ( (          advanced ) ? "         -gpld name      - name of datafile of plot (default logfile.opt....).\n" : "" );
    output << ( (          advanced ) ? "         -gplT title     - graph title.                                       \n" : "" );
    output << ( (          advanced ) ? "         -gplt t         - type of plot:                                      \n" : "" );
    output << ( (          advanced ) ? "                           0: terminal.                                       \n" : "" );
    output << ( (          advanced ) ? "                           1: .ps file.                                       \n" : "" );
    output << ( (          advanced ) ? "                           2: .pdf file (default).                            \n" : "" );
    output << ( (          advanced ) ? "                           3: mex window.                                     \n" : "" );
    output << ( (          advanced ) ? "         -gplm m         - set lower end of y range (dflt 1, auto if m>M).    \n" : "" );
    output << ( (          advanced ) ? "         -gplM M         - set upper end of y range (dflt 0, auto if m>M).    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Grid-search specific options.                 **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -ggm n          - number of zooms.  After  the optimum for a grid has\n" : "" );
    output << ( (          advanced ) ? "                           been  found a  zoom  involves  doing an  additional\n" : "" );
    output << ( (          advanced ) ? "                           grid-search with a  finer grid over a smaller range\n" : "" );
    output << ( (          advanced ) ? "                           around  the  previous  solution.  This  is  done  n\n" : "" );
    output << ( (          advanced ) ? "                           times (default 0).                                 \n" : "" );
    output << ( (          advanced ) ? "         -ggi f          - width  of  zoomed grid  is width  of previous  grid\n" : "" );
    output << ( (          advanced ) ? "                           multiplied  by f (real,  < 1).  Grid is  trimmed to\n" : "" );
    output << ( (          advanced ) ? "                           lie inside previous range.  Default 0.3333.        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** DIRect global optimiser options.              **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gdc m          - max number of cube divisions (default 200).        \n" : "" );
    output << ( (          advanced ) ? "         -gdf m          - max number of function evaluations (default 1000). \n" : "" );
    output << ( (          advanced ) ? "         -gde e          - epsilon factor (default 1e-4).                     \n" : "" );
    output << ( (          advanced ) ? "         -gda t          - algorithm.  0 is original (default), 1 Gablowsky.  \n" : "" );
    output << ( (          advanced ) ? "         -gdy t          - max training time over-ride for DIRect.  Applies to\n" : "" );
    output << ( (          advanced ) ? "                           the direct algorithm only, so when DIRect is called\n" : "" );
    output << ( (          advanced ) ? "                           by another  algorithm (eg by a  Bayesian optimiser)\n" : "" );
    output << ( (          advanced ) ? "                           then  this controls  the time  spent in  the DIRect\n" : "" );
    output << ( (          advanced ) ? "                           calls  and -gy  controls  the  total overall  time.\n" : "" );
    output << ( (          advanced ) ? "                           Value is in seconds, default is zero (no override).\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Nelder-Mead optimiser options.                **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gNa e          - minimum f value (default -HUGE_VAL).               \n" : "" );
    output << ( (          advanced ) ? "         -gNb e          - relative f value tolerance (default 0).            \n" : "" );
    output << ( (          advanced ) ? "         -gNc e          - absolute f value tolerance (default 0).            \n" : "" );
    output << ( (          advanced ) ? "         -gNd e          - relative x value tolerance (default 0).            \n" : "" );
    output << ( (          advanced ) ? "         -gNg e          - relative x value tolerance (default 0).            \n" : "" );
    output << ( (          advanced ) ? "         -gNe m          - max number of function evaluations (default 1000). \n" : "" );
    output << ( (          advanced ) ? "         -gNf t          - algorithm:   0  is  subplex  (default), 1  original\n" : "" );
    output << ( (          advanced ) ? "                           Nelder-Mead algorithm.                             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Stopping criteria are as follows:             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - f goes below minimum f value.               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - a step happens were  f changes by less than **         \n" : "" );
    output << ( (          advanced ) ? "                  **   the relative f tolerance times |f|.         **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - a step happens there the absolute change in **         \n" : "" );
    output << ( (          advanced ) ? "                  **   |f| is less than the absolute f tolderance. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - a step happens where x changes by less than **         \n" : "" );
    output << ( (          advanced ) ? "                  **   the relative x tolerance times ||x||.       **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - a step happens there the absolute change in **         \n" : "" );
    output << ( (          advanced ) ? "                  **   |x[i]| for any i is  less than the absolute **         \n" : "" );
    output << ( (          advanced ) ? "                  **    x tolderance.                              **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - the max number  of function  evaluations is **         \n" : "" );
    output << ( (          advanced ) ? "                  **   exceeded.                                   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - the max training time is exceeded.          **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Model-based optimisation options.             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** (this includes Bayesian optimisation)         **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gms            - Select single-objective optimisation model (dflt). \n" : "" );
    output << ( (          advanced ) ? "         -gmo            - Select multi-objective optimisation model.         \n" : "" );
    output << ( (          advanced ) ? "         -gma n          - set dim of default GPR for multi-objective optim.  \n" : "" );
    output << ( (          advanced ) ? "         -gmr            - add noise to model observations.                   \n" : "" );
    output << ( (          advanced ) ? "         -gmR            - don't add noise to model observations (default).   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmgtc n        - number of inequality constraints c(x) >= 0.        \n" : "" );
    output << ( (          advanced ) ? "                           0 - normal operation (default).                    \n" : "" );
    output << ( (          advanced ) ? "                           n - see above.                                     \n" : "" );
    output << ( (          advanced ) ? "                           Equality  constraints  are   enforced  (in  BO)  by\n" : "" );
    output << ( (          advanced ) ? "                           multiplying   the   acquisition  function   by  the\n" : "" );
    output << ( (          advanced ) ? "                           posterior probability  that the  constraint is met.\n" : "" );
    output << ( (          advanced ) ? "                           For EI, this is exactly cEI.                       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmsc n         - enable/disable side-channel learning:              \n" : "" );
    output << ( (          advanced ) ? "                           0 - normal operation.                              \n" : "" );
    output << ( (          advanced ) ? "                           n - user returns { y v [ xx1 ... xxn ] }, where xxi\n" : "" );
    output << ( (          advanced ) ? "                               is \"side-channel\"  data.  The model  learned is\n" : "" );
    output << ( (          advanced ) ? "                               then built on [ x ~  xx1 ~ ... ~ xxn ],  not x.\n" : "" );
    output << ( (          advanced ) ? "                               Then, using kernel voodoo, we can do stuff like\n" : "" );
    output << ( (          advanced ) ? "                               build kernels on K([x ~ xx ...],[x' ~ xx' ...])\n" : "" );
    output << ( (          advanced ) ? "                               =  Ka(x,x').Kb(xx,xx').   For  the  acquisition\n" : "" );
    output << ( (          advanced ) ? "                               function SMBO  maintains  models xxi = qi(x) to\n" : "" );
    output << ( (          advanced ) ? "                               infer [ x ~ xx1 ~ ... ~ xxn ] from x. See -tMx.\n" : "" );
    output << ( (          advanced ) ? "         -gmsn n         - noise transfer model:                              \n" : "" );
    output << ( (          advanced ) ? "                           0 = standard. When modelling qi(x), the variance of\n" : "" );
    output << ( (          advanced ) ? "                               the estimate is transferred  to the variance of\n" : "" );
    output << ( (          advanced ) ? "                               the overall  model,  and  thus included  in the\n" : "" );
    output << ( (          advanced ) ? "                               acquisiton function (default).                 \n" : "" );
    output << ( (          advanced ) ? "                           1 = naively optimistic. Ignore variance from qi(x),\n" : "" );
    output << ( (          advanced ) ? "                               making the  model naively  optimistic regarding\n" : "" );
    output << ( (          advanced ) ? "                               the accuracy of qi(x).                         \n" : "" );
    output << ( (          advanced ) ? "         -gmsy n         - noise x method:                                    \n" : "" );
    output << ( (          advanced ) ? "                           0 = standard, as described above (default).        \n" : "" );
    output << ( (          advanced ) ? "                           n = convert [ x ~  x' ~ x'' ~ ... ] to  the \"naive\"\n" : "" );
    output << ( (          advanced ) ? "                               form  [ x n:x' 2n:x''  ... ],  so  side-channel\n" : "" );
    output << ( (          advanced ) ? "                               data is naively treated as normal data.        \n" : "" );
    output << ( (          advanced ) ? "         -gmsw [ n ]     - change default models for the side-channels q(x).  \n" : "" );
    output << ( (          advanced ) ? "         -gmsa n         - set dim for default q(x) model.                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmT [ xt ]     - set (sparse vector) template  for model data.  Data\n" : "" );
    output << ( (          advanced ) ? "                           added to  the model is  x overlaid onto  xt, so for\n" : "" );
    output << ( (          advanced ) ? "                           example  -gmT [ ~ 1 ] means  that if a  vector x is\n" : "" );
    output << ( (          advanced ) ? "                           added into the model as [ x ~ 1 ]. This can be used\n" : "" );
    output << ( (          advanced ) ? "                           in multi-task learning, with -kS to signify a split\n" : "" );
    output << ( (          advanced ) ? "                           kernel, -ks 2 to indicate two  parts to the kernel,\n" : "" );
    output << ( (          advanced ) ? "                           -ki 0 ... to set usual covariance (over x), and -ki\n" : "" );
    output << ( (          advanced ) ? "                           1 -kg 48 -kr v to  set the variance  between tasks.\n" : "" );
    output << ( (          advanced ) ? "                           If -gmw has been used to spacify an ML with data of\n" : "" );
    output << ( (          advanced ) ? "                           the form [ xi ~ 0 ] then:                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           K([xi ~ ti],[xj ~ tj]) = K0(xi,xj)   if ti == tj   \n" : "" );
    output << ( (          advanced ) ? "                                                  = v.K0(xi,xj) if ti != tj   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           will do transfer learning via multi-task kernel.   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmd s          - set noise variance for default GPR model.          \n" : "" );
    output << ( (          advanced ) ? "         -gmsd s         - set noise variance for default q(x) model.         \n" : "" );
    output << ( (          advanced ) ? "         -gmgtd s        - set noise variance for default c(x) model.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmbgn n        - set default GPR model posterior variance terms. See\n" : "" );
    output << ( (          advanced ) ? "                           -bgn for details.                                  \n" : "" );
    output << ( (          advanced ) ? "         -gmsbgn n       - set default q(x) model postrior variance terms.    \n" : "" );
    output << ( (          advanced ) ? "         -gmgtbgn n      - set default c(x) model postrior variance terms.    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmg g          - set length scale for default GPR kernel.           \n" : "" );
    output << ( (          advanced ) ? "         -gmsg g         - set length scale for default q(x) kernel.          \n" : "" );
    output << ( (          advanced ) ? "         -gmgtg g        - set length scale for default c(x) kernel.          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmgg g         - set ARD length scale vector for default GPR kernel.\n" : "" );
    output << ( (          advanced ) ? "         -gmsgg g        - set ARD length scale vector for dfault q(x) kernel.\n" : "" );
    output << ( (          advanced ) ? "         -gmgtgg g       - set ARD length scale vector for dfault c(x) kernel.\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmk...         - set default kernel parameters on GP for GPR model. \n" : "" );
    output << ( (          advanced ) ? "         -gmsk...        - set default kernel paras on GP for side-channel.   \n" : "" );
    output << ( (          advanced ) ? "         -gmgtk...       - set default kernel paras on GP for inequality.     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmn n          - sigma estimation model:                            \n" : "" );
    output << ( (          advanced ) ? "                           0 - mu and  sigma approximated by a single  ML that\n" : "" );
    output << ( (          advanced ) ? "                               gets updated after each \"batch\".               \n" : "" );
    output << ( (          advanced ) ? "                           1 - mu and sigma approximated by separate MLs.  The\n" : "" );
    output << ( (          advanced ) ? "                               mu model  is updated  after  each  \"batch\", the\n" : "" );
    output << ( (          advanced ) ? "                               sigma model after each  experiment (during each\n" : "" );
    output << ( (          advanced ) ? "                               batch).                                        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmLf n         - model  logging frequency  (0 for no  logging, -1 to\n" : "" );
    output << ( (          advanced ) ? "                           log after last iteration only, default 0).         \n" : "" );
    output << ( (          advanced ) ? "         -gmLF n         - model logging format (deflt 2, see -plot for info).\n" : "" );
    output << ( (          advanced ) ? "         -gmLn n         - model base-name (default smbomodel).            .  \n" : "" );
    output << ( (          advanced ) ? "         -gmhplb f       - baseline function, if plotted (default null).      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Trivial example of side-channel learning:     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** ./svmheavyv7.exe -gmsc 1 -gmks 2 -gmki 0      **         \n" : "" );
    output << ( (          advanced ) ? "                  **      -gmkA -gmkt 3 -gmkg 0.1 -gmki 1 -gmkt 3  **         \n" : "" );
    output << ( (          advanced ) ? "                  **      -gmkg 0.1 -gbt 50 -gb 2 \"-tM y^2+(z-1)^2 **        \n" : "" );
    output << ( (          advanced ) ? "                  **      -tMx [y^2+(z-1)^2]\" \"-echo y -echo z\"    **         \n" : "" );
    output << ( (          advanced ) ? "                  **      fb 1 -2 3 10 fb 2 -2 4 10                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** In this case the side-channel is just the     **         \n" : "" );
    output << ( (          advanced ) ? "                  ** function itself.  In practice side-channels   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** will be more subtle (correlates for example), **         \n" : "" );
    output << ( (          advanced ) ? "                  ** but the principle is the same.  Note that:    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tM y^2+(z-1)^2 is the function we're mining. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tMx [y^2+(z-1)^2] is the side-channel.       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gmsc 1 specifies a single side-channel       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gmks 2 we need two kernels in the model, one **         \n" : "" );
    output << ( (          advanced ) ? "                  **         for x, one for the side-channel.      **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gmki 0 -gmkA -gmkt 3 -gmkg 0.1 kernel for x. **         \n" : "" );
    output << ( (          advanced ) ? "                  **         Note that -gmkA specifies that K is   **         \n" : "" );
    output << ( (          advanced ) ? "                  **         split directly after this, so the     **         \n" : "" );
    output << ( (          advanced ) ? "                  **         next kernel in the dictionary is for  **         \n" : "" );
    output << ( (          advanced ) ? "                  **         the side-channel.  -gmkA specifies    **         \n" : "" );
    output << ( (          advanced ) ? "                  **         that the kernels are added, but we    **         \n" : "" );
    output << ( (          advanced ) ? "                  **         could use -gmkS for multiplication.   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gmki 1 -gmkt 3 -gmkg 0.1 kernel for the side **         \n" : "" );
    output << ( (          advanced ) ? "                  **         channel.                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** All kernel length-scales are independentally  **         \n" : "" );
    output << ( (          advanced ) ? "                  ** tuned by default, including for the side-     **         \n" : "" );
    output << ( (          advanced ) ? "                  ** channel model (not specified here, defaults   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** to RBF kernel, but can be tuned using         **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gmsk... options).                            **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmma n         - Automatic tuning for optimisation model.           \n" : "" );
    output << ( (          advanced ) ? "         -gmmb n         - Automatic tuning for noise variance model.         \n" : "" );
    output << ( (          advanced ) ? "         -gmmc n         - Automatic tuning for source model (see -gmx).      \n" : "" );
    output << ( (          advanced ) ? "         -gmmd n         - Automatic tuning for difference model (see -gmx).  \n" : "" );
    output << ( (          advanced ) ? "         -gmsmd n        - Automatic tuning for q(x) model (see -gmsc).       \n" : "" );
    output << ( (          advanced ) ? "         -gmgtmd n       - Automatic tuning for c(x) model (see -gmgtc).      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           The above tuning is of  (some of the) kernel params\n" : "" );
    output << ( (          advanced ) ? "                           on a heuristic basis using rudimentary grid search.\n" : "" );
    output << ( (          advanced ) ? "                           Note that the source model is only tuned once.  The\n" : "" );
    output << ( (          advanced ) ? "                           n value controls what is to be minimised:          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           0: no automatic tuning                             \n" : "" );
    output << ( (          advanced ) ? "                           1: negative-log-likelihood minimisation (default). \n" : "" );
    output << ( (          advanced ) ? "                           2: leave-one-out error minimisation.               \n" : "" );
    output << ( (          advanced ) ? "                           3: recall minimisation.                            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmw n          - by default the objective is modelled by a GPR.  You\n" : "" );
    output << ( (          advanced ) ? "                           can replace this with ML n using this command. Note\n" : "" );
    output << ( (          advanced ) ? "                           that for multi-objective  optimisation this must be\n" : "" );
    output << ( (          advanced ) ? "                           vector-valued.                                     \n" : "" );
    output << ( (          advanced ) ? "                        ** You can also use this for  transfer learning.  Data\n" : "" );
    output << ( (          advanced ) ? "                           already in this model is treated as observations of\n" : "" );
    output << ( (          advanced ) ? "                           y = -f(x) (NOTE THE NEGATIVE SIGN THERE).  See also\n" : "" );
    output << ( (          advanced ) ? "                           -gmx for more on transfer learning.                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmt n          - model basis:                                       \n" : "" );
    output << ( (          advanced ) ? "                           0 - model f(p(x)) using p(x) (default).            \n" : "" );
    output << ( (          advanced ) ? "                           1 - model f(p(x)) using p(x), clear after subspace.\n" : "" );
    output << ( (          advanced ) ? "                           2 - model f(p(x)) using x, clear after subspace.   \n" : "" );
    output << ( (          advanced ) ? "                           3 - model f(p(x)) using x.                         \n" : "" );
    output << ( (          advanced ) ? "         -gmrff N        - use RFF for model.  If N = 0 then no rff; otherwise\n" : "" );
    output << ( (          advanced ) ? "                           we use random fourier features model with N feats. \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmq n          - direction oracle mode (for -gp, -gP, -gpr):        \n" : "" );
    output << ( (          advanced ) ? "                           0 - direction is sample from gradient GP at current\n" : "" );
    output << ( (          advanced ) ? "                               best solution, as per GP model (default).      \n" : "" );
    output << ( (          advanced ) ? "                           1 - direction  is gradient  of model GP  at current\n" : "" );
    output << ( (          advanced ) ? "                               best solution, as per GP model.                \n" : "" );
    output << ( (          advanced ) ? "                           2 - direction is random sample.                    \n" : "" );
    output << ( (          advanced ) ? "                           3 - mode 0 for primary axis, mode 2 for the rest.  \n" : "" );
    output << ( (          advanced ) ? "                           4 - mode 1 for primary axis, mode 2 for the rest.  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmx n          - controls how data already in model is treated (that\n" : "" );
    output << ( (          advanced ) ? "                           is, if you use  -gmw n where ML n  has data already\n" : "" );
    output << ( (          advanced ) ? "                           added).  Options are:                              \n" : "" );
    output << ( (          advanced ) ? "                           0 - assume data from target model (default).       \n" : "" );
    output << ( (          advanced ) ? "                           1 - use env-GP as per Joy1/Shi21.                  \n" : "" );
    output << ( (          advanced ) ? "                           2 - use diff-GP as per Shi21.                      \n" : "" );
    output << ( (          advanced ) ? "         -gmxa a         - alpha0 value for env-GP.                           \n" : "" );
    output << ( (          advanced ) ? "         -gmxb b         - beta0 value for env-GP.                            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gmy n          - Kernel transfer learning from ML n.                \n" : "" );
    output << ( (          advanced ) ? "         -gmya n         - Method for kernel  transfer, if -gmy used.  Options\n" : "" );
    output << ( (          advanced ) ? "                           as per -kt 8xx, so:                                \n" : "" );
    output << ( (          advanced ) ? "                           800 - trivial (K(x,y) = Kn(x,y)).                  \n" : "" );
    output << ( (          advanced ) ? "                           801 - m-norm (free kernel) transfer (default).     \n" : "" );
    output << ( (          advanced ) ? "                           802 - moment (Der and Lee) transfer.               \n" : "" );
    output << ( (          advanced ) ? "                           804 - K-learn transfer.                            \n" : "" );
    output << ( (          advanced ) ? "                           805 - K2-learn transfer.                           \n" : "" );
    output << ( (          advanced ) ? "                           806 - Multi-layer transfer.                        \n" : "" );
    output << ( (          advanced ) ? "         -gmyb n         - Kernel transfer normalisation:                     \n" : "" );
    output << ( (          advanced ) ? "                           0 - no normalisation.                              \n" : "" );
    output << ( (          advanced ) ? "                           1 - normalisation on (default).                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** env-GP:  - source model copied from the ML of **         \n" : "" );
    output << ( (          advanced ) ? "                  **            -gmw n, though  you can do further **         \n" : "" );
    output << ( (          advanced ) ? "                  **            tuning via var(90,5). Model asumed **         \n" : "" );
    output << ( (          advanced ) ? "                  **            to be already trained.             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** diff-GP: - source model as-per env-GP.        **         \n" : "" );
    output << ( (          advanced ) ? "                  **          - difference model  also copied from **         \n" : "" );
    output << ( (          advanced ) ? "                  **            -gmw n, but  data added  and model **         \n" : "" );
    output << ( (          advanced ) ? "                  **            retrained throughout (var(90,6)).  **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Bayesian optimiser options.                   **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbH n          - method used to select points in optimisation:      \n" : "" );
    output << ( (          advanced ) ? "                           0  - MO (mean only minimisation).                  \n" : "" );
    output << ( (          advanced ) ? "                           1  - EI (expected improvement). Default.           \n" : "" );
    output << ( (          advanced ) ? "                           2  - PI (probability of improvement).              \n" : "" );
    output << ( (          advanced ) ? "                           3  - GP-UCB as per Brochu (recommended GP-UCB).*   \n" : "" );
    output << ( (          advanced ) ? "                           4  - GP-UCB |D| finite as per Srinivas.            \n" : "" );
    output << ( (          advanced ) ? "                           5  - GP-UCB |D| infinite as per Srinivas.          \n" : "" );
    output << ( (          advanced ) ? "                           6  - GP-UCB p based on Brochu.                     \n" : "" );
    output << ( (          advanced ) ? "                           7  - GP-UCB p |D| finite based on Srinivas.        \n" : "" );
    output << ( (          advanced ) ? "                           8  - GP-UCB p |D| infinite based on Srinivas.      \n" : "" );
    output << ( (          advanced ) ? "                           9  - PE (pure exploration - variance only).$       \n" : "" );
    output << ( (          advanced ) ? "                           10 - PEc (pure exploration - total variance only).@\n" : "" );
    output << ( (          advanced ) ? "                           11 - GP-UCB with user-defined beta_t (see -gbv).   \n" : "" );
    output << ( (          advanced ) ? "                           12 - Thompson sampling.#                           \n" : "" );
    output << ( (          advanced ) ? "                           13 - GP-UCB RKHS as per Srinivas.                  \n" : "" );
    output << ( (          advanced ) ? "                           14 - GP-UCB RKHS as Chowdhury.#                    \n" : "" );
    output << ( (          advanced ) ? "                           15 - GP-UCB RKHS as Bogunovic.~                    \n" : "" );
    output << ( (          advanced ) ? "                           16 - Thompson sampling (unity scaling on variance).\n" : "" );
    output << ( (          advanced ) ? "                           17 - GP-UCB as per Kandasamy (multifidelity 2017). \n" : "" );
    output << ( (          advanced ) ? "                           18 - Human will be prompted to input x.            \n" : "" );
    output << ( (          advanced ) ? "                           19 - HE (human-level exploitation beta = 0.01).    \n" : "" );
    output << ( (          advanced ) ? "                           20 - GP-UCB as per BO-Muse (single AI).^           \n" : "" );
    output << ( (          advanced ) ? "                           * beta_n = 2.log((n^{2+dim/2}).(pi^2)/(3.delta))   \n" : "" );
    output << ( (          advanced ) ? "                           $ variance of model only.                          \n" : "" );
    output << ( (          advanced ) ? "                           @ total variance of model and contraints.          \n" : "" );
    output << ( (          advanced ) ? "                           # Chowdhury, On Kernelised Multi-Arm Bandits, Alg 2\n" : "" );
    output << ( (          advanced ) ? "                           ~ Bogunovic, Misspecified GP Bandit Optim., Lemma 1\n" : "" );
    output << ( (          advanced ) ? "                           ^ Intendid to be combined with human prompt.       \n" : "" );
    output << ( (          advanced ) ? "         -gbj n          - number  of random  start/seed  points  (default  -1\n" : "" );
    output << ( (          advanced ) ? "                           translates to d+1, where d is the dimension).      \n" : "" );
    output << ( (          advanced ) ? "         -gbt m          - max  iteration  count for  search algorithm  (0 for\n" : "" );
    output << ( (          advanced ) ? "                           no limit, default -1  translates to 15dm where d is\n" : "" );
    output << ( (          advanced ) ? "                           and m is the number of  objectives, -2 to stop when\n" : "" );
    output << ( (          advanced ) ? "                           min_x err(x) <= maxerr).                           \n" : "" );
    output << ( (          advanced ) ? "         -gbe e          - maxerr as required with -gbt -2 (see Kirschner et.,\n" : "" );
    output << ( (          advanced ) ? "                           default 0.1).                                      \n" : "" );
    output << ( (          advanced ) ? "         -gbz z          - zero  tolerance for  search algorithm  (def 1e-12).\n" : "" );
    output << ( (          advanced ) ? "                           Used when assessing if sigma^2 == 0.               \n" : "" );
    output << ( (          advanced ) ? "         -gbZ z          - penalise inner-loop if stddev(x)<z (default 0).    \n" : "" );
    output << ( (          advanced ) ? "         -gbTm m         - Thompson sampling sample mode:                     \n" : "" );
    output << ( (          advanced ) ? "                           1 - use regular grid or random samples.            \n" : "" );
    output << ( (          advanced ) ? "                           3 - use JIT sample (default).                      \n" : "" );
    output << ( (          advanced ) ? "         -gbTn n         - number of samples for -gbTm 1  (+ve for fixed grid,\n" : "" );
    output << ( (          advanced ) ? "                           -ve for random grid, 0 (default) for random grid of\n" : "" );
    output << ( (          advanced ) ? "                           10jd^2 points, where d is the  dim, j the number of\n" : "" );
    output << ( (          advanced ) ? "                           prior samples already in the model).               \n" : "" );
    output << ( (          advanced ) ? "         -gbTs n         - sample type for -gbTm 1.  See -Stt for details.    \n" : "" );
    output << ( (          advanced ) ? "         -gbTx n         - x sample type for -gbTm 1.  See -Stx for details.  \n" : "" );
    output << ( (          advanced ) ? "         -gbTv sigscale  - Set variance scale for JIT Thompson (deflt 0.1).   \n" : "" );
    output << ( (          advanced ) ? "         -gbeu n         - If 0 (default) then nothing. If 1 then the user can\n" : "" );
    output << ( (          advanced ) ? "                           interactively  override  the recommendation  and/or\n" : "" );
    output << ( (          advanced ) ? "                           the evaluation.                                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gba n          - if >=  0 then  this is  used to  seed the  RNG when\n" : "" );
    output << ( (          advanced ) ? "                           generating initial  points (see -gbj,  default 42).\n" : "" );
    output << ( (          advanced ) ? "                           -2 means seed with time, -1 means no seed.         \n" : "" );
    output << ( (          advanced ) ? "         -gbb n          - RNG seed right before main optimisation loop if >=0\n" : "" );
    output << ( (          advanced ) ? "                           Default 69.  -2 means seed with time, -1 no seed.  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbG n          - do  grid-search,  where ML n  defines grid  data in\n" : "" );
    output << ( (          advanced ) ? "                           terms of x (dimensions  must agree  with definition\n" : "" );
    output << ( (          advanced ) ? "                           in -gb). If y is NULL then it is ignored: otherwise\n" : "" );
    output << ( (          advanced ) ? "                           it will  be treated  as a  prior  observation.  The\n" : "" );
    output << ( (          advanced ) ? "                           y values in source will be filled by BO.           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           NB: if all data in  the ML has non-null  y then all\n" : "" );
    output << ( (          advanced ) ? "                               data will be treated as  prior observations and\n" : "" );
    output << ( (          advanced ) ? "                               the  BO  will  revert  to  standard  (non-grid)\n" : "" );
    output << ( (          advanced ) ? "                               operation.                                     \n" : "" );
    output << ( (          advanced ) ? "                           NB: when evaluating var(5,5) is pre-loaded with the\n" : "" );
    output << ( (          advanced ) ? "                               grid index.                                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbD d          - delta factor used in GP-UCB method (default 0.1).  \n" : "" );
    output << ( (          advanced ) ? "         -gbzz zeta      - zeta factor in EI method (deflt 0.  0.01 works ok).\n" : "" );
    output << ( (          advanced ) ? "         -gbk n          - nu factor Srinivas GP-UCB (deft 0.2, see Srivinas).\n" : "" );
    output << ( (          advanced ) ? "         -gbx n          - |D| (size  of search  space grid) for  gpUCB finite\n" : "" );
    output << ( (          advanced ) ? "                           (default -1, which  means set to size  of grid data\n" : "" );
    output << ( (          advanced ) ? "                           (if available) or 10 (which is arbitrary)).        \n" : "" );
    output << ( (          advanced ) ? "         -gbo a          - a constant for Srinivas |D|-infinite gpUCB (def 1).\n" : "" );
    output << ( (          advanced ) ? "         -gbB b          - b constant for Srinivas |D|-infinite gpUCB (def 1).\n" : "" );
    output << ( (          advanced ) ? "         -gbr r          - r constant for Srinivas  |D|-infinite gpUCB (def 1,\n" : "" );
    output << ( (          advanced ) ? "                           which is correct in all cases due to scaling).     \n" : "" );
    output << ( (          advanced ) ? "         -gbRR R         - R constant for -gbH 12,13,14 (def 1).              \n" : "" );
    output << ( (          advanced ) ? "         -gbBB R         - B constant for -gbH 12,13,14 (-1 for auto, deflt). \n" : "" );
    output << ( (          advanced ) ? "         -gbu p          - p value for GP-UCB p variants (default 2).         \n" : "" );
    output << ( (          advanced ) ? "         -gbv $fn        - user function for beta in  GP-UCB user-defined.  In\n" : "" );
    output << ( (          advanced ) ? "                           this function:                                     \n" : "" );
    output << ( (          advanced ) ? "                           - var(0,1) (y) = iteration number.                 \n" : "" );
    output << ( (          advanced ) ? "                           - var(0,2) (z) = x dimension.                      \n" : "" );
    output << ( (          advanced ) ? "                           - var(0,3) (v) = delta.                            \n" : "" );
    output << ( (          advanced ) ? "                           - var(0,4) (w) = |D| specified by -gbx.            \n" : "" );
    output << ( (          advanced ) ? "                           - var(0,5) (g) = a as specified by -gbo.           \n" : "" );
    output << ( (          advanced ) ? "                           For   multi-recommendation    via   multi-objective\n" : "" );
    output << ( (          advanced ) ? "                           optimisation on (mu,sigma) use -gbv null. This will\n" : "" );
    output << ( (          advanced ) ? "                           apply  multi-objective  optimisation to  (mu,sigma)\n" : "" );
    output << ( (          advanced ) ? "                           and the  resultant Pareto  set will all be  used as\n" : "" );
    output << ( (          advanced ) ? "                           recommendations  (null  means  beta  undefined,  so\n" : "" );
    output << ( (          advanced ) ? "                           select for all values of beta).                    \n" : "" );
    output << ( (          advanced ) ? "                           For  multi-recommendation via  multi-strategy  make\n" : "" );
    output << ( (          advanced ) ? "                           this a vector of (-gbH) method numbers, eg:        \n" : "" );
    output << ( (          advanced ) ? "                              [ [ 17 ... ] 18 [ 1 ... ] ]                     \n" : "" );
    output << ( (          advanced ) ? "                           where  ... is  a list  of  possible  parameters, in\n" : "" );
    output << ( (          advanced ) ? "                           order:                                             \n" : "" );
    output << ( (          advanced ) ? "                              p, beta_fn, modD,delta,nu,a,b,r,batch_size,     \n" : "" );
    output << ( (          advanced ) ? "                              batch_method,R                                  \n" : "" );
    output << ( (          advanced ) ? "                           For multi-rec, multi-obj, use eg:                  \n" : "" );
    output << ( (          advanced ) ? "                              -gbv [ betafn1 betafn2 ... ]                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbp [ n0 n1 ... ]- Add penalty  terms to  the acquisition  function.\n" : "" );
    output << ( (          advanced ) ? "                           This is helpful if there are non-linear constraints\n" : "" );
    output << ( (          advanced ) ? "                           on the feasible region.  Each element of the vector\n" : "" );
    output << ( (          advanced ) ? "                           should be an  ML.  The total penalty  is the sum of\n" : "" );
    output << ( (          advanced ) ? "                           the  outputs of  all MLs.  Penalty  should  be near\n" : "" );
    output << ( (          advanced ) ? "                           zero  in the  feasible region,  very large  outside\n" : "" );
    output << ( (          advanced ) ? "                           (that  is, a  penalty for  a minimisation problem).\n" : "" );
    output << ( (          advanced ) ? "                           Default value is an empty vector.                  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbts $estring  - string  to be evaluated  before each (inner)  iter.\n" : "" );
    output << ( (          advanced ) ? "                           var(90,4) is the inner iteration count.            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbq n          - use IMP to pre-process  the output of the GP.  This\n" : "" );
    output << ( (          advanced ) ? "                           is required for multi-objective optimisation as the\n" : "" );
    output << ( (          advanced ) ? "                           improvement function requires a scalar. Essentially\n" : "" );
    output << ( (          advanced ) ? "                           mean :=  imp(mean,var).  Processing  done using IMP\n" : "" );
    output << ( (          advanced ) ? "                           with ML number  n (c/f -qw n) which  must be an IMP\n" : "" );
    output << ( (          advanced ) ? "                           type  object.  Note  that the  acquisition function\n" : "" );
    output << ( (          advanced ) ? "                           defined by -gbH will still be applied after this(to\n" : "" );
    output << ( (          advanced ) ? "                           do passthrough use -gbH 0).  Some IMPs  will update\n" : "" );
    output << ( (          advanced ) ? "                           the variance if  required.  For EHI use -gbH 0, for\n" : "" );
    output << ( (          advanced ) ? "                           SVM mono-surrogate use for example -gbH 3.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbmm n         - if using  Bayesian multi-step then  for all but the\n" : "" );
    output << ( (          advanced ) ? "                           first  recommendation in  each batch  pre-process x\n" : "" );
    output << ( (          advanced ) ? "                           using xi ->  f([ xi x0 ]), where f is  the function\n" : "" );
    output << ( (          advanced ) ? "                           of the  ML n  defined  here  and x0  is  the  first\n" : "" );
    output << ( (          advanced ) ? "                           recommendation in  this batch and the  dimension of\n" : "" );
    output << ( (          advanced ) ? "                           of xi is determined by -gbpd d (see below).        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbim {0,1,2,3} - iteration count method.  In GP-UCB beta calculation\n" : "" );
    output << ( (          advanced ) ? "                           an iteration  counter t  is used.   The value  of t\n" : "" );
    output << ( (          advanced ) ? "                           (var(0,1)  in -gbv equation/vector)  is  controlled\n" : "" );
    output << ( (          advanced ) ? "                           by this setting:                                   \n" : "" );
    output << ( (          advanced ) ? "                           0 - t  =  (N/B)+1  initially, t ->  t+1 after  each\n" : "" );
    output << ( (          advanced ) ? "                               batch, where  B is the batch size (size of -gbv\n" : "" );
    output << ( (          advanced ) ? "                               vector, or  1 by default) and N  is the initial\n" : "" );
    output << ( (          advanced ) ? "                               number of  training vectors in  the GP model (0\n" : "" );
    output << ( (          advanced ) ? "                               by default  unless you have  pre-training pts).\n" : "" );
    output << ( (          advanced ) ? "                               Default, consistent with the GP-UCB-PE method. \n" : "" );
    output << ( (          advanced ) ? "                           1 - t = N+1  initially,  t -> t+B after  each batch\n" : "" );
    output << ( (          advanced ) ? "                               (t is incremented by 1  for each recommendation\n" : "" );
    output << ( (          advanced ) ? "                               within a  batch).  This is  consistent with the\n" : "" );
    output << ( (          advanced ) ? "                               GP-BUCB method.                                \n" : "" );
    output << ( (          advanced ) ? "                           2 - like 1, but t  actually used to  calculate beta\n" : "" );
    output << ( (          advanced ) ? "                               is the t value at the start of this batch (that\n" : "" );
    output << ( (          advanced ) ? "                               is,  B*floor(t/B)).  Use  for GP-UCB  finite or\n" : "" );
    output << ( (          advanced ) ? "                               GP-UCB martingale.                             \n" : "" );
    output << ( (          advanced ) ? "                           3 - like 1, but t starts as 1.                     \n" : "" );
    output << ( (          advanced ) ? "         -gbs ibs        - intrinsic number  of recommendations  (see method 2\n" : "" );
    output << ( (          advanced ) ? "                           for multi-recommendation below).                   \n" : "" );
    output << ( (          advanced ) ? "         -gbm ims        - method for intrinsic batch:                        \n" : "" );
    output << ( (          advanced ) ? "                           0: use max mean, det(covar)^(1/(2*ibs)) (default). \n" : "" );
    output << ( (          advanced ) ? "                           1: use ave mean, det(covar)^(1/(2*ibs)).           \n" : "" );
    output << ( (          advanced ) ? "                           2: use min mean, det(covar)^(1/(2*ibs)).           \n" : "" );
    output << ( (          advanced ) ? "                           3: use max mean, sqrt(ibs/Tr(inv(covar))).         \n" : "" );
    output << ( (          advanced ) ? "                           4: use ave mean, sqrt(ibs/Tr(inv(covar))).         \n" : "" );
    output << ( (          advanced ) ? "                           5: use min mean, sqrt(ibs/Tr(inv(covar))).         \n" : "" );
    output << ( (          advanced ) ? "         -gbpp q         - pre-process DIRect output using ML q (see below).  \n" : "" );
    output << ( (          advanced ) ? "         -gbpd d         - dimension of pre-process input (see below).        \n" : "" );
    output << ( (          advanced ) ? "         -gbpl lv        - min vector for pre-process input (see below).      \n" : "" );
    output << ( (          advanced ) ? "         -gbpu uv        - max vector for pre-process input (see below).      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbfid n        - if n>0 then  this does multi-fidelity  optimisation\n" : "" );
    output << ( (          advanced ) ? "                           as in Kandasamy, BO with Continuous Approximations.\n" : "" );
    output << ( (          advanced ) ? "                           n in the number  of fidelities, as  selected by the\n" : "" );
    output << ( (          advanced ) ? "                           final  variable in the  optimisation, which  ranges\n" : "" );
    output << ( (          advanced ) ? "                           over {1/n,2/n,...,1}. The two kernels - K(x,x') and\n" : "" );
    output << ( (          advanced ) ? "                           K(z,z'), are set by:                               \n" : "" );
    output << ( (          advanced ) ? "                           -ks 2              (two kernels in the definition) \n" : "" );
    output << ( (          advanced ) ? "                           -ki 1 -kS          (select first kernel, set split)\n" : "" );
    output << ( (          advanced ) ? "                           ...                (set kernel K(x,x') parameters) \n" : "" );
    output << ( (          advanced ) ? "                           -ki 2              (select second kernel)          \n" : "" );
    output << ( (          advanced ) ? "                           ...                (set kernel K(z,z') parameters) \n" : "" );
    output << ( (          advanced ) ? "         -gbfn  d        - number of fidelity variables (last d vars, dflt 1).\n" : "" );
    output << ( (          advanced ) ? "         -gbfp  penfun   - set fidelity cost function c(z).  Dflt 1.          \n" : "" );
    output << ( (          advanced ) ? "         -gbfv  penvar   - set fidelity variance (added to usual variance).   \n" : "" );
    output << ( (          advanced ) ? "         -gbfb  b        - set  fidelity budget.   If -1  (default) then  this\n" : "" );
    output << ( (          advanced ) ? "                           does nothing.  If >0 (and startpoints, maxitcnt are\n" : "" );
    output << ( (          advanced ) ? "                           also default) then b/10 is  used for initialization\n" : "" );
    output << ( (          advanced ) ? "                           points (ie sum_i c(x_i) <= b) and the remainder for\n" : "" );
    output << ( (          advanced ) ? "                           optimization.                                      \n" : "" );
    output << ( (          advanced ) ? "         -gbfk  kappa    - kappa (default 1, see Kandasamy).                  \n" : "" );
    output << ( (          advanced ) ? "         -gbfo  f        - fidelity overwrite.                                \n" : "" );
    output << ( (          advanced ) ? "                           0: use fidelity generated by algorithm.            \n" : "" );
    output << ( (          advanced ) ? "                           1: ask human to set fidelity (with recommendation).\n" : "" );
    output << ( (          advanced ) ? "                           2: randomly select fidelity <= reommendation.      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbsp p         - set >= 1 to enforce  mu_{1:p}-stability, where this\n" : "" );
    output << ( (          advanced ) ? "                           is the maximum allowed value of p (default 0).     \n" : "" );
    output << ( (          advanced ) ? "         -gbsP p         - minimum value of p (default 1).                    \n" : "" );
    output << ( (          advanced ) ? "         -gbsA A         - upper bound on output variation.                   \n" : "" );
    output << ( (          advanced ) ? "         -gbsB B         - maximum input perturbation.                        \n" : "" );
    output << ( (          advanced ) ? "         -gbsF F         - total output range (max-min).                      \n" : "" );
    output << ( (          advanced ) ? "         -gbsr b         - policy balance (0 = conservat (deflt), 1 = risky). \n" : "" );
    output << ( (          advanced ) ? "         -gbsz z         - zero reference point for f (default 0).            \n" : "" );
    output << ( (          advanced ) ? "         -gbss c         - use sigmoid compresion on stability scores (def 0).\n" : "" );
    output << ( (          advanced ) ? "         -gbst t         - threshold for sigmoid compression (def 0.8).       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Notes on Stable Methods:                      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - sigmoid compression  can make EI  work, but **         \n" : "" );
    output << ( (          advanced ) ? "                  **   not very well.  Without compression however **         \n" : "" );
    output << ( (          advanced ) ? "                  **   the  algorithm tends  to stick  at unstable **         \n" : "" );
    output << ( (          advanced ) ? "                  **   peaks where high gain swamps low stability. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - strongly suggest using  GP-UCB, which works **         \n" : "" );
    output << ( (          advanced ) ? "                  **   very  well.  The  expected  return (mu)  is **         \n" : "" );
    output << ( (          advanced ) ? "                  **   scaled, but the variance isn't.             **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gbuu {0,1}     - Unscented  optimisation on (1) or off  (0,default).\n" : "" );
    output << ( (          advanced ) ? "                           See Nogueira et al, Unscented Bayesian Optimisation\n" : "" );
    output << ( (          advanced ) ? "                           for Safe Robot Grasping.                           \n" : "" );
    output << ( (          advanced ) ? "         -gbuk k         - Set k value (0 or -3, 0 default).                  \n" : "" );
    output << ( (          advanced ) ? "         -gbuS S         - sqrt(Sigma), square-root of matrix variance of x.  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Notes on Multi-Recommendation Methods:        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Method 1 (multi-objective optimisation):      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** To use multi-objective optimisation to define **         \n" : "" );
    output << ( (          advanced ) ? "                  ** multiple    recommendations,     where    the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** recommendations  correspond  to  the  Pareto- **         \n" : "" );
    output << ( (          advanced ) ? "                  ** optimal solutions of:                         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **  max(-mu(x),sigma(x))                         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** (the negative  arising because  we are trying **         \n" : "" );
    output << ( (          advanced ) ? "                  ** to minimise our target here) use the command: **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbH 11 -gbv null                             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where beta =  null is shorthand  for beta not **         \n" : "" );
    output << ( (          advanced ) ? "                  ** defined, so  solve for  all beta.  -gbH 11 is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** GP-UCB with  user-defined  beta_t (so -gbv is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** used instead).                                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Method 2 (intrinsic batch):                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** In the standard methods  the two variables to **         \n" : "" );
    output << ( (          advanced ) ? "                  ** be optimised in the  inner loop are mu(x) and **         \n" : "" );
    output << ( (          advanced ) ? "                  ** sigma(x).  Method 2 replaces these with:      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **               [   mu(x_0)   ]                 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** mu(x) -> max( [   mu(x_1)   ] )               **         \n" : "" );
    output << ( (          advanced ) ? "                  **               [     ...     ]                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **               [ mu(x_{d-1}) ]                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                 [ covar_00 covar_01 ... ]     **         \n" : "" );
    output << ( (          advanced ) ? "                  ** sigma(x) -> det([ covar_10 covar_11 ... ])^r  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                 [    ...      ...   ... ]     **         \n" : "" );
    output << ( (          advanced ) ? "                  ** (r = 1/2d)                                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** To select this  use -gbs d.  Can  be combined **         \n" : "" );
    output << ( (          advanced ) ? "                  ** with other methods (assuming no constraints). **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Use -gbm 1 to slct mu(x) -> min(...) instead. **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** If you want to apply constraints to this use: **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - use -gbpp to  specify the  ML defining  the **         \n" : "" );
    output << ( (          advanced ) ? "                  **   them, so DIRect  optimises a(f(x)), where a **         \n" : "" );
    output << ( (          advanced ) ? "                  **   is the usual  activation function  and f is **         \n" : "" );
    output << ( (          advanced ) ? "                  **   specified by ml number q.                   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - use -gbpd to specify  the dimension  of the **         \n" : "" );
    output << ( (          advanced ) ? "                  **   input to f.                                 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - use -gbpl/-gbpu to specify lower  and upper **         \n" : "" );
    output << ( (          advanced ) ? "                  **   bnd vector (respectively)  on the x vector. **         \n" : "" );
    output << ( (          advanced ) ? "                  **   In this case the variables specified in the **         \n" : "" );
    output << ( (          advanced ) ? "                  **   original -gb call are the outputs of f(x).  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Method 3 (hybrid multi-strategy):             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** To define  multiple GP-UCB  strategies select **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbH 11 (GP-UCB with user defined beta_t) and **         \n" : "" );
    output << ( (          advanced ) ? "                  ** then let -gbv be a vector, each corresponding **         \n" : "" );
    output << ( (          advanced ) ? "                  ** to an equation for calculating beta.  eg:     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbH 11                                       **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbv [ 2*log((x^(2+(y/2)))*zeta(2)/z) ;       **         \n" : "" );
    output << ( (          advanced ) ? "                  **        2*log((x^(3+(y/2)))*zeta(3)/z) ;       **         \n" : "" );
    output << ( (          advanced ) ? "                  **        2*log((x^(4+(y/2)))*zeta(4)/z) ]       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** gives  three equations  for beta  (p=2,3,4 in **         \n" : "" );
    output << ( (          advanced ) ? "                  ** this  case)  that   will  each  result  in  a **         \n" : "" );
    output << ( (          advanced ) ? "                  ** separate  recommendation.   Alternatively you **         \n" : "" );
    output << ( (          advanced ) ? "                  ** can define  beta_t indirectly  by making each **         \n" : "" );
    output << ( (          advanced ) ? "                  ** element of  the -gbv vector a  vector of  the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** form:                                         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** [ methd {p betfn |D| nu delt a b r ibs ims} ] **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where method selects an option asper -gbH and **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the  remaining   (optional)   arguments   the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** various parameters therein.  For example:     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbH 11                                       **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbv [ [ 3 ] ;                                **         \n" : "" );
    output << ( (          advanced ) ? "                  **        [ 6 3 ] ;                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **        [ 6 4 ] ]                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** is equivalent to the  first example.  You can **         \n" : "" );
    output << ( (          advanced ) ? "                  ** leave \"gaps\" using []  (don't overwrite).  So **         \n" : "" );
    output << ( (          advanced ) ? "                  ** for example to set nu = 0.9 for the first two **         \n" : "" );
    output << ( (          advanced ) ? "                  ** in the above you would use:                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbH 11                                       **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbv [ [ 3 [ ] [ ] [ ] 0.9 ] ;                **         \n" : "" );
    output << ( (          advanced ) ? "                  **        [ 6 3 [ ] [ ] 0.9 ] ;                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **        [ 6 4 ] ]                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** You can intersperse these techniques.  eg:    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbH 11                                       **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbv [ [ 3 [ ] [ ] [ ] 0.9 ] ;                **         \n" : "" );
    output << ( (          advanced ) ? "                  **        2*log((x^(5+(y/2)))*zeta(5)/z) ;       **         \n" : "" );
    output << ( (          advanced ) ? "                  **        [ 6 3 [ ] [ ] 0.9 ] ;                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **        [ 6 4 ] ]                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** defines four recommendation methods; and also **         \n" : "" );
    output << ( (          advanced ) ? "                  ** that you can include method 1 as part of this **         \n" : "" );
    output << ( (          advanced ) ? "                  ** so for example:                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbH 11                                       **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gbv [ [ 3 [ ] [ ] [ ] 0.9 ] ;                **         \n" : "" );
    output << ( (          advanced ) ? "                  **        2*log((x^(5+(y/2)))*zeta(5)/z) ;       **         \n" : "" );
    output << ( (          advanced ) ? "                  **        null ;                                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **        [ 6 3 [ ] [ ] 0.9 ] ;                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **        [ 6 4 ] ]                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** defines  five   recommendation  methods,  the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** third of which is itself multi-recommendation **         \n" : "" );
    output << ( (          advanced ) ? "                  ** method 1.                                     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Finally,  you can  control how  the model  is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** updated  between each round (element  in -gbv **         \n" : "" );
    output << ( (          advanced ) ? "                  ** vector) using  the -gmn option.  If -gmn 0 is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** used (default) both mu and sigma use the same **         \n" : "" );
    output << ( (          advanced ) ? "                  ** model (GP) updated batchwise,  whereas -gmn 1 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** uses separate  models, the  mu model  updated **         \n" : "" );
    output << ( (          advanced ) ? "                  ** batchwise and  the sigma model  updated after **         \n" : "" );
    output << ( (          advanced ) ? "                  ** each recommendation (halucinated samples).    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Standard methods: GP-                         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **UCB-PE: -gbH 11 -gmn 1 -gbv [ [ 3 ] ; [ 9 ] ...]**        \n" : "" );
    output << ( (          advanced ) ? "                  **BUCB: -gbH 11 -gmn 1 -gbim 1 -gbv [ [ 3 ] ; [ 3 ]**       \n" : "" );
    output << ( (          advanced ) ? "                  **                                         ... ] **         \n" : "" );
    output << ( (          advanced ) ? "                  ** UCB-multi: -gbH 11 -gbv null                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where  the number  of recommendations  is the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** total number of elements in the -gbv vector.  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Multi-recommendation with constraints:        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** We  take  as our  example  problem  the 2-dim **         \n" : "" );
    output << ( (          advanced ) ? "                  ** optimisation  problem given  previously  (see **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -gb). The flags below should come just before **         \n" : "" );
    output << ( (          advanced ) ? "                  ** this statement.   The following  examples are **         \n" : "" );
    output << ( (          advanced ) ? "                  ** for a  batch size of  2 (recommendations  per **         \n" : "" );
    output << ( (          advanced ) ? "                  ** batch)  with the  restriction that  the first **         \n" : "" );
    output << ( (          advanced ) ? "                  ** element  of all  recommendations must  be the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** same).                                        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** GP-UCB-det (based on method 2 above) ver 1:   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1. Set up map function:                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 2 -Zx -z fnb                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -blx M:[1,0,0;0,1,0;1,0,0;0,0,1]*x         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    where -z fnb is a  block function and -blx **         \n" : "" );
    output << ( (          advanced ) ? "                  **    sets the map. -qw 2 sets it as ML block 2. **         \n" : "" );
    output << ( (          advanced ) ? "                  **    This implements the map:                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [ [ x00 ] ]   [ 1 0 0 ] [ z0 ]             **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [ [ x01 ] ]   [ 0 1 0 ] [ z1 ]             **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [         ] = [       ] [ z2 ]             **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [ [ x10 ] ]   [ 1 0 0 ]                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [ [ x11 ] ]   [ 0 0 1 ]                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    The inner DIRect optimiser in the Bayesian **         \n" : "" );
    output << ( (          advanced ) ? "                  **    optimisation  optimises over  z, while the **         \n" : "" );
    output << ( (          advanced ) ? "                  **    GP model is built over x.                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2. Set up the Bayesian optimiser:             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 0 -Zx                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbH 3 -gbs 2 -gbm 0 -gbpp 2 -gbpd 3       **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbpl [ -4 -4 -4 ] -gbpu [ 4 4 4 ]         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Bayesian optimiser using method 3 (GP-UCB) **         \n" : "" );
    output << ( (          advanced ) ? "                  **   (setby -gbH 3), 2 intrinsic recommendations **         \n" : "" );
    output << ( (          advanced ) ? "                  **    per  batch (set  by -gbs 2),  balanced max **         \n" : "" );
    output << ( (          advanced ) ? "                  **    mean / determinant  (set by -gbm 0), using **         \n" : "" );
    output << ( (          advanced ) ? "                  **    ML 2  as  a  map  to  enforce  constraint, **         \n" : "" );
    output << ( (          advanced ) ? "                  **    pre-process  dimension (input  to ML 2) of **         \n" : "" );
    output << ( (          advanced ) ? "                  **    3 (-gbpd 3), with lower  and upper  bounds **         \n" : "" );
    output << ( (          advanced ) ? "                  **    for direct set by -gbpl and -gbpu.         **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** GP-UCB-det (based on method 2 above) ver 2:   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1. Set up penalty function:                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 2 -Zx -z fnb                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -blx 1000*(([1;0;1;0]*x)^2)                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    where -z fnb is a  block function and -blx **         \n" : "" );
    output << ( (          advanced ) ? "                  **    sets the map. -qw 2 sets it as ML block 2. **         \n" : "" );
    output << ( (          advanced ) ? "                  **    This is a quadratic penalty for failure to **         \n" : "" );
    output << ( (          advanced ) ? "                  **    satisfy the constraint.                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2. Set up the Bayesian optimiser:             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 0 -Zx                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbH 3 -gbs 2 -gbm 0 -gbp [ 2 ] -gbpd 4    **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbpl [ -4 -4 -4 -4 ] -gbpu [ 4 4 4 4 ]    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Bayesian optimiser using method 3 (GP-UCB) **         \n" : "" );
    output << ( (          advanced ) ? "                  **   (setby -gbH 3), 2 intrinsic recommendations **         \n" : "" );
    output << ( (          advanced ) ? "                  **    per  batch (set  by -gbs 2),  balanced max **         \n" : "" );
    output << ( (          advanced ) ? "                  **    mean / determinant  (set by -gbm 0), using **         \n" : "" );
    output << ( (          advanced ) ? "                  **    ML 2 as  a penalty to  enforce constraint, **         \n" : "" );
    output << ( (          advanced ) ? "                  **    pre-process  dimension (input  to ML 2) of **         \n" : "" );
    output << ( (          advanced ) ? "                  **    4 (-gbpd 4) - you need this -  with lower/ **         \n" : "" );
    output << ( (          advanced ) ? "                  **    upper bounds for direct set by -gbpl/-gbpu.**         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** GP-UCB-cPE:                                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1. Set up map function:                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 2 -Zx -z fnb                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -blx M:[0,1,0;1,0,0]*x                     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    where -z fnb is a  block function and -blx **         \n" : "" );
    output << ( (          advanced ) ? "                  **    sets the map. -qw 2 sets it as ML block 2. **         \n" : "" );
    output << ( (          advanced ) ? "                  **    This implements the map:                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [ x10 ] = [ 0 1 0 ] [ z0  ]                **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [ x11 ]   [ 1 0 0 ] [ x01 ]                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                        [ x11 ]                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    The second recommendation  is generated by **         \n" : "" );
    output << ( (          advanced ) ? "                  **    the  DIRect  optimiser  optimising  on  z, **         \n" : "" );
    output << ( (          advanced ) ? "                  **    while the GP model is built over x.        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2. Set up the Bayesian optimiser:             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 0 -Zx                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbH 11 -gmn 1 -gbv [ [ 3 ] ; [ 9 ] ]      **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbpp 2 -gbpd 1                            **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbpl [ -4 ] -gbpu [ 4 ]                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Bayesian  optimiser using  above GP-UCB-PE **         \n" : "" );
    output << ( (          advanced ) ? "                  **    using ML 2 as a map to enforce constraint, **         \n" : "" );
    output << ( (          advanced ) ? "                  **    pre-process  dimension (input  to ML 2) of **         \n" : "" );
    output << ( (          advanced ) ? "                  **    1 for  non-first recommendation (-gbpd 1), **         \n" : "" );
    output << ( (          advanced ) ? "                  **    with lower and upper bounds for direct set **         \n" : "" );
    output << ( (          advanced ) ? "                  **    by -gbpl and -gbpu.                        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** GP-cBUCB:                                     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1. Set up map function:                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 2 -Zx -z fnb                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -blx M:[0,1,0;1,0,0]*x                     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    where -z fnb is  a block function and -blx **         \n" : "" );
    output << ( (          advanced ) ? "                  **    sets the map. -qw 2 sets it as ML block 2. **         \n" : "" );
    output << ( (          advanced ) ? "                  **    This implements the map:                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [ x10 ] = [ 0 1 0 ] [ z0  ]                **         \n" : "" );
    output << ( (          advanced ) ? "                  **    [ x11 ]   [ 1 0 0 ] [ x01 ]                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                        [ x11 ]                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    The second  recommendation is generated by **         \n" : "" );
    output << ( (          advanced ) ? "                  **    the  DIRect  optimiser  optimising  on  z, **         \n" : "" );
    output << ( (          advanced ) ? "                  **    while the GP model is built over x.        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2. Set up the Bayesian optimiser:             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 0 -Zx                              **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbH 11 -gmn 1 -gbim 1 -gbv [ [ 3 ] ; [ 3 ] ]**       \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbpp 2 -gbpd 1                            **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbpl [ -4 ] -gbpu [ 4 ]                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    Bayesian  optimiser  using  above  GP-BUCB **         \n" : "" );
    output << ( (          advanced ) ? "                  **    using ML 2 as a map to enforce constraint, **         \n" : "" );
    output << ( (          advanced ) ? "                  **    pre-process  dimension (input  to ML 2) of **         \n" : "" );
    output << ( (          advanced ) ? "                  **    1 for non-first  recommendation (-gbpd 1), **         \n" : "" );
    output << ( (          advanced ) ? "                  **    with lower and upper bounds for direct set **         \n" : "" );
    output << ( (          advanced ) ? "                  **    by -gbpl and -gbpu.                        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** GP-cBO:                                       **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** This method uses an outer loop for the common **         \n" : "" );
    output << ( (          advanced ) ? "                  ** variable (y  in this case) and  an inner loop **         \n" : "" );
    output << ( (          advanced ) ? "                  ** to recommend  batch of other  variables (z in **         \n" : "" );
    output << ( (          advanced ) ? "                  ** this case:                                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1. Set up GP model for inner loop:            **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 1 -Zx -z gpr -d 0.01               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2. Set up GP model for outer loop:            **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 2 -Zx -z gpr -d 0.01               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 3. Inner loop is called  as a function, which **         \n" : "" );
    output << ( (          advanced ) ? "                  **    we define here.  This optimises over y and **         \n" : "" );
    output << ( (          advanced ) ? "                  **    z, but  the range  of y  is restricted  so **         \n" : "" );
    output << ( (          advanced ) ? "                  **    that y  = var(0,100).   Note that  this is **         \n" : "" );
    output << ( (          advanced ) ? "                  **    basically GP-UCB-PE.  Note also the use of **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -fret to return var(0,1),  var(0,2) (y and **         \n" : "" );
    output << ( (          advanced ) ? "                  **    z), var(51,0) (f(y,z)) and var(53,10) (the **         \n" : "" );
    output << ( (          advanced ) ? "                  **    upper bound for the outer loop).           **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -fM 42 { -gbt 1 -gbH 11 -gmw 1 -gmn 1  **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -gbv [ [ 3 ] ; [ 9 ] ] -gb 2 \"-tM          **         \n" : "" );
    output << ( (          advanced ) ? "                  **    y^2+(z-1)^2\" \"fret 0 1 -fret 0 2           **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -fret 51 0 -fret 53 10\" fb 1 var(0,100)    **         \n" : "" );
    output << ( (          advanced ) ? "                  **    var(0,100) 1 fb 2 -2 4 1 }                 **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 4. Run the outer loop.   This will return the **         \n" : "" );
    output << ( (          advanced ) ? "                  **    optimal  result in y  and z.  Note  use of **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -fret on y and  z in $evalstring  as these **         \n" : "" );
    output << ( (          advanced ) ? "                  **    would otherwise be lost and -tMv to modify **         \n" : "" );
    output << ( (          advanced ) ? "                  **    variance using the upper bound.  Note also **         \n" : "" );
    output << ( (          advanced ) ? "                  **    that  in  $setstring   y  and  z  must  be **         \n" : "" );
    output << ( (          advanced ) ? "                  **    retrieved from var(50,...) and returned    **         \n" : "" );
    output << ( (          advanced ) ? "                  **    using -fret.                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -Zx -qw 0                                  **         \n" : "" );
    output << ( (          advanced ) ? "                  **-Zx -gbt 20 -gbH 3 -gmw 2 -gtx [ var(0,100) ; z ] **      \n" : "" );
    output << ( (          advanced ) ? "                  **    -gb 1 \"-MM 42 -Zx -tM var(51,0) -tMv       **         \n" : "" );
    output << ( (          advanced ) ? "                  **    var(53,10)-0.01\" \"-fW 1 var(50,0) -fW 2    **         \n" : "" );
    output << ( (          advanced ) ? "                  **    var(50,1) -fret 0 1 -fret 0 2\" fb 100 -2 3 **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 5. Report result:                             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -echo y -echo z                            **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -gBbH n         - EHI  calculation  method  for  multi-recommendation\n" : "" );
    output << ( (          advanced ) ? "                           via  multi-objective  optimisation.   See  -ie  for\n" : "" );
    output << ( (          advanced ) ? "                           possible values.                                   \n" : "" );
    output << ( (          advanced ) ? "         -gB...          - Options for multi-recommendation via multi-objctive\n" : "" );
    output << ( (          advanced ) ? "                           optimisation.   These control the  Bayesian (inner,\n" : "" );
    output << ( (          advanced ) ? "                           direct and generic) options used in the inner loop.\n" : "" );
    output << ( (          advanced ) ? "                           -gB.. canbe -gBy, -gBdc, -gBdf, -gBde, -gBda, -gBbt\n" : "" );
    output << ( (          advanced ) ? "                           -gBdy, -gBbj, -gBfm or -gBfM (eg -gBy  controls the\n" : "" );
    output << ( (          advanced ) ? "                           inner-loop training time, c/f see -gy).            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Transfer learning via kernels:                                                \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -x n [ i j .. ] [ wi wj .. ] - if the  current ML is  a binary/scalar\n" : "" );
    output << ( (          advanced ) ? "                           SVM from which MLs i,j,... inherit their kernel via\n" : "" );
    output << ( (          advanced ) ? "                           kernel 801 (with no additional nonlinearities) then\n" : "" );
    output << ( (          advanced ) ? "                           this  function will  define a  kernel with  n bases\n" : "" );
    output << ( (          advanced ) ? "                           seen from SVMs i,j,...):                           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                             K(x,y) = sum_ij alpha_i alpha_j K(x,y,zi,zj)     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           that is tuned to optimise performance.  This choice\n" : "" );
    output << ( (          advanced ) ? "                           is regularised  by C  (upper bound on  alpha_i) and\n" : "" );
    output << ( (          advanced ) ? "                           epsilon  (linear sum  on alpha) for  current ML  to\n" : "" );
    output << ( (          advanced ) ? "                           ensure the diagonals on the inherited kernel are 1.\n" : "" );
    output << ( (          advanced ) ? "                           The cost to be minimised is:                       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                            wi Ri + wj Rj + ... (Ri is the risk for ML i)     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           To do anti-learning for a given ML negative weight.\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -xo  {0,1,2,3}  - training method:                                   \n" : "" );
    output << ( (          advanced ) ? "                           0 - naive gradient descent.                        \n" : "" );
    output << ( (          advanced ) ? "                           1 - Nelder-Mead (nlopt).                           \n" : "" );
    output << ( (          advanced ) ? "                           2 - Subplex (nlopt).                               \n" : "" );
    output << ( (          advanced ) ? "                           3 - SLQP (nlopt, default).                         \n" : "" );
    output << ( (          advanced ) ? "         -xi  n          - max training iterations (default 20).              \n" : "" );
    output << ( (          advanced ) ? "         -xt  t          - max training time (default 0 = nothing).           \n" : "" );
    output << ( (          advanced ) ? "         -xs  s          - solution tolerance (alpha step, default 0.01).     \n" : "" );
    output << ( (          advanced ) ? "         -xl  lr         - learning rate (default -1 for Newton step).  Newton\n" : "" );
    output << ( (          advanced ) ? "                           only works for scalar type MLs.                    \n" : "" );
    output << ( (          advanced ) ? "         -xa  {0,1}      - alpha range:                                       \n" : "" );
    output << ( (          advanced ) ? "                           0 - [-10,10]                                       \n" : "" );
    output << ( (          advanced ) ? "                           1 - [0,10]                                         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -xR {0,1,2,3,4,5}-regularisation type (default 1):                   \n" : "" );
    output << ( (          advanced ) ? "                           0 - no regularisation.                             \n" : "" );
    output << ( (          advanced ) ? "                           1 - regularisation to make kernel diagonals ~1.    \n" : "" );
    output << ( (          advanced ) ? "                           2 - quadratic regularisation C/2 sum_i alpha_i^2.  \n" : "" );
    output << ( (          advanced ) ? "                           3 - linear regularisation C sum_i |alpha_i|.       \n" : "" );
    output << ( (          advanced ) ? "                           4 - regularise with (1-||alpha||_1)^2.             \n" : "" );
    output << ( (          advanced ) ? "                           5 - regularise with (1-sum(alpha))^2.              \n" : "" );
    output << ( (          advanced ) ? "         -xC  C          - regularisation constant (default 1).               \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -xr  {0,1,2,3}  - randomisation method for x vectors:                \n" : "" );
    output << ( (          advanced ) ? "                           0 - x_i ~ N(0,v).                                  \n" : "" );
    output << ( (          advanced ) ? "                           1 - x_i ~ U(0,v).                                  \n" : "" );
    output << ( (          advanced ) ? "                           2 - x_i is Rademacher (random -v,+v) (default).    \n" : "" );
    output << ( (          advanced ) ? "                           3 - x_i is Rademacher (random -1,+1) (default).    \n" : "" );
    output << ( (          advanced ) ? "         -xh  {0,1}      - if set 1 then set z_0 = 1 (default 0).             \n" : "" );
    output << ( (          advanced ) ? "         -xrv v          - variance used in -xr.  Use -1 to select per-element\n" : "" );
    output << ( (          advanced ) ? "                           variance that matches existing dataset (deflt -1). \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         Example of use:   ./svmheavyv7.exe -qw 3 -z r -kt 2 -kd 3 -Zx        \n" : "" );
    output << ( (          advanced ) ? "                      -qw 1 -z c -Zx -R q -c 10 -kt 801 -ktx 3 -AA xor.txt -Zx\n" : "" );
    output << ( (          advanced ) ? "                      -qw 2 -z r -Zx -R q -c 10 -kt 801 -ktx 3 -AA and.txt -Zx\n" : "" );
    output << ( (          advanced ) ? "                      -qw 3 -Zx -xr 2 -xC 2 -xs 1e-6 -xi 100 -xo 3            \n" : "" );
    output << ( (          advanced ) ? "                      -x 20 [ 1 2 ] [ 1 1 ] -Zx                               \n" : "" );
    output << ( (          advanced ) ? "                      -qw 1 -tx -s temp1.svm -Zx -qw 2 -tx -s temp2.svm -Zx   \n" : "" );
    output << ( (          advanced ) ? "                      -qw 3 -x temp3.svm                                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                         - line 1 defines ML 3 to be an SVR with a third-order\n" : "" );
    output << ( (          advanced ) ? "                           polynomial kernel.                                 \n" : "" );
    output << ( (          advanced ) ? "                         - line 2 defines ML 1 to be an SVC with an inheritted\n" : "" );
    output << ( (          advanced ) ? "                           kernel K(x,y) =  sum_pq beta_p beta_q K(zp,zq,x,y),\n" : "" );
    output << ( (          advanced ) ? "                           where (beta_p,zp)  are defined by ML 3,  trained on\n" : "" );
    output << ( (          advanced ) ? "                           the xor.txt dataset.                               \n" : "" );
    output << ( (          advanced ) ? "                         - line 3 defines ML 2 to be an SVR with an inheritted\n" : "" );
    output << ( (          advanced ) ? "                           kernel K(x,y) =  sum_pq beta_p beta_q K(zp,zq,x,y),\n" : "" );
    output << ( (          advanced ) ? "                           where (beta_p,zp)  are defined by ML 3,  trained on\n" : "" );
    output << ( (          advanced ) ? "                           the and.txt dataset.                               \n" : "" );
    output << ( (          advanced ) ? "                         - lines 4-5 use transfer learning to learn beta (note\n" : "" );
    output << ( (          advanced ) ? "                           -qw 3 selects the relevant ML). 20 zps are randomly\n" : "" );
    output << ( (          advanced ) ? "                           generated with Rademacher random components (-xr 2)\n" : "" );
    output << ( (          advanced ) ? "                           and a regularisation constant of 2 is used (-xC 2).\n" : "" );
    output << ( (          advanced ) ? "                           SLQP optimisation is used (-xo 3) to find beta with\n" : "" );
    output << ( (          advanced ) ? "                           stopping  criteria ||dbeta||_inf < 1e-6  (-xs 1e-6)\n" : "" );
    output << ( (          advanced ) ? "                           or 100 iterations (-xi 100).   Beta are selected to\n" : "" );
    output << ( (          advanced ) ? "                           optimise MLs 1 and 2 as  per -x 20 [ 1 2 ] [ 1 1 ],\n" : "" );
    output << ( (          advanced ) ? "                           with equal weighting.                              \n" : "" );
    output << ( (          advanced ) ? "                         - lines 6-7 test performance and save results.       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Feature selection options (after parameter tuning):                           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fsx            - feature selection via hill climbing, min LOO error.\n" : "" );
    output << ( (          advanced ) ? "         -fsr            - feature selection via hill climbing, min recal err.\n" : "" );
    output << ( (          advanced ) ? "         -fsc n          - feature  selection via  hill  climbing, min  n-fold\n" : "" );
    output << ( (          advanced ) ? "                           cross validation error.                            \n" : "" );
    output << ( (          advanced ) ? "         -fsC m n        - feature  selection via  hill  climbing, min  n-fold\n" : "" );
    output << ( (          advanced ) ? "                           cross-validation error, randomised, m repetitions. \n" : "" );
    output << ( (          advanced ) ? "         -fsf $file      - feature selection via hill climbing, min test err. \n" : "" );
    output << ( (          advanced ) ? "         -fsF i j $file  - feature selection via hill  climbing, min test err,\n" : "" );
    output << ( (          advanced ) ? "                           ignoring i  vectors  at start,  testing  at most  j\n" : "" );
    output << ( (          advanced ) ? "                           vectors (-1 if all).                               \n" : "" );
    output << ( (          advanced ) ? "         -fss n          - set number  of sweeps (0  default) for  this set of\n" : "" );
    output << ( (          advanced ) ? "                           feature  selection.  If  >1 then,  after the  first\n" : "" );
    output << ( (          advanced ) ? "                           hill-climb(descent) the  algorithm will follow with\n" : "" );
    output << ( (          advanced ) ? "                           hill-descent(climb)   starting  with   the  optimal\n" : "" );
    output << ( (          advanced ) ? "                           features found previously.   The alternating climb,\n" : "" );
    output << ( (          advanced ) ? "                           descent sequence will run n times.                 \n" : "" );
    output << ( (          advanced ) ? "         -fsd            - start  with  existing  features  for  this  set  of\n" : "" );
    output << ( (          advanced ) ? "                           feature selection rather than from scratch.        \n" : "" );
    output << ( (          advanced ) ? "         -fsD            - undoes -fsd.                                       \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Use -fS...  to use hill  descent rather  than **         \n" : "" );
    output << ( (          advanced ) ? "                  ** hill climbing.  Suffixes as per -tx etc.      **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Fuzzy ML Support (after feature selection):                                   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** So-called  fuzzy MLs  use functions  inspired **         \n" : "" );
    output << ( (          advanced ) ? "                  ** by fuzzy  logic to  set the individual  C and **         \n" : "" );
    output << ( (          advanced ) ? "                  ** epsilon  weights  for  each  training  vector **         \n" : "" );
    output << ( (          advanced ) ? "                  ** based  on   some  estimate  of   how  much  a **         \n" : "" );
    output << ( (          advanced ) ? "                  ** particular  vector  \"belongs\"  to its  class. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** The degree of belonging  is calculated by the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** membership function,  typically  based on the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** relative distances to the class centre of the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** class  to which  the vector  belongs and  the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** distances  to  other  classes.   The  inbuilt **         \n" : "" );
    output << ( (          advanced ) ? "                  ** membership functions are:                     **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "    q1 = 0.5+((exp(f*(d_d-d_l)/d)-exp(-f))/(2*(exp(f)-exp(-f))))              \n" : "" );
    output << ( (          advanced ) ? "    q2 = ((2*(0.5+((exp(f*(d_d-d_l)/d)-exp(-f))/(2*(exp(f)-exp(-f))))))-1)^m  \n" : "" );
    output << ( (          advanced ) ? "    q3 = 0.5+((1-(d_l/(r_l+f))/2)                                             \n" : "" );
    output << ( (          advanced ) ? "    q4 = 0.5*(1+tanh(f*((2*g_x)+m)))                                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** (Keller and  Hunt, modified Keller  and Hunt, **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Lin  and  Wang, and  cluster-based).  In  all **         \n" : "" );
    output << ( (          advanced ) ? "                  ** cases, for each training vector pair (x,y):   **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "    q   = var(2,0) = either t (C weight) or s (epsilon weight), pre-fuzzing.  \n" : "" );
    output << ( (          advanced ) ? "    d_l = var(2,1) = distance from x to the mean of class y.                  \n" : "" );
    output << ( (          advanced ) ? "    d_d = var(2,2) = min distance from x to the mean of any other class !y.   \n" : "" );
    output << ( (          advanced ) ? "    d   = var(2,3) = distance between the mean of classes y and !y.           \n" : "" );
    output << ( (          advanced ) ? "    r_l = var(2,4) = radius of  smallest  sphere  centred at  mean of  class y\n" : "" );
    output << ( (          advanced ) ? "                     containing all elements of class y.                      \n" : "" );
    output << ( (          advanced ) ? "    r_d = var(2,5) = radius of  smallest  sphere  centred at mean  of class !y\n" : "" );
    output << ( (          advanced ) ? "                     containing all elements of class !y.                     \n" : "" );
    output << ( (          advanced ) ? "    g_x = var(2,6) = output of 1-class ML trained  with all vectors of class y\n" : "" );
    output << ( (          advanced ) ? "    q1  = var(2,7) = Keller and Hunt membership.                              \n" : "" );
    output << ( (          advanced ) ? "    q2  = var(2,8) = Modified Keller and Hunt membership.                     \n" : "" );
    output << ( (          advanced ) ? "    q3  = var(2,9) = Lin and Wang membership.                                 \n" : "" );
    output << ( (          advanced ) ? "    q4  = var(2,10)= cluster-based membership.                                \n" : "" );
    output << ( (          advanced ) ? "    f   = var(3,0) = user parameters set below.                               \n" : "" );
    output << ( (          advanced ) ? "    m   = var(3,1) = user parameters set below.                               \n" : "" );
    output << ( (          advanced ) ? "    nu  = var(3,2) = nu value used for clustering.                            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -fzt $fn        - Apply fuzzy to C weights using function given.     \n" : "" );
    output << ( (          advanced ) ? "         -fzs $fn        - Apply fuzzy to epsiln weights using function given \n" : "" );
    output << ( (          advanced ) ? "         -fztk...        - Modify kernel function  to be used in fuzzification\n" : "" );
    output << ( (          advanced ) ? "                           of C weights.   ... is any  of the kernel functions\n" : "" );
    output << ( (          advanced ) ? "                           above (so for  example -fztkt 3 uses the RBF kernel\n" : "" );
    output << ( (          advanced ) ? "                           for  fuzzification).  Note  that the kernel used is\n" : "" );
    output << ( (          advanced ) ? "                           initially set  to the  ML kernel,  and this  simply\n" : "" );
    output << ( (          advanced ) ? "                           modifies it.                                       \n" : "" );
    output << ( (          advanced ) ? "         -fztf f         - Set user parameter f in  -fzt function (default 1).\n" : "" );
    output << ( (          advanced ) ? "                           (should be small +ve number for Lin and Wang).     \n" : "" );
    output << ( (          advanced ) ? "         -fztm m         - Set user parameter m in -fzt function (default 1). \n" : "" );
    output << ( (          advanced ) ? "         -fztNlA nu      - Set nu for 1-class SVM if needed for -fzt (df 0.5).\n" : "" );
    output << ( (          advanced ) ? "         -fzsk...        - Modify kernel function to  be used in fuzzification\n" : "" );
    output << ( (          advanced ) ? "                           of eps weights.  ... is any of the kernel functions\n" : "" );
    output << ( (          advanced ) ? "                           above (so for example  -fztkt 3 uses the RBF kernel\n" : "" );
    output << ( (          advanced ) ? "                           for fuzzification).   Note that the  kernel used is\n" : "" );
    output << ( (          advanced ) ? "                           initially set  to the  ML kernel,  and this  simply\n" : "" );
    output << ( (          advanced ) ? "                           modifies it.                                       \n" : "" );
    output << ( (          advanced ) ? "         -fzsf f         - Set user parameter f in  -fzs function (default 1).\n" : "" );
    output << ( (          advanced ) ? "                           (should be small +ve number for Lin and Wang).     \n" : "" );
    output << ( (          advanced ) ? "         -fzsm m         - Set user parameter m in -fzs function (default 1). \n" : "" );
    output << ( (          advanced ) ? "         -fzsNlA nu      - Set nu for 1-clas SVM if needed for -fzs (dft 0.5).\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Bootstrap ML Support (after fuzzy ML):                                        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** To approximate variance  calculations you can **         \n" : "" );
    output << ( (          advanced ) ? "                  ** use (pseuso)-boostrapping.                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1. Set up your ML (say ML 0).                 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2. Make B copies of your ML (say 1..B):       **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -qc 1 0 -Zx                                **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -qc 2 0 -Zx                                **         \n" : "" );
    output << ( (          advanced ) ? "                  **      ...                                      **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -qc B 0 -Zx                                **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 3. \"Bootstrap\" ML copies:                     **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -qw 1 -boot -Zx                            **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -qw 2 -boot -Zx                            **         \n" : "" );
    output << ( (          advanced ) ? "                  **      ...                                      **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -qw B -boot -Zx                            **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 4. Set up multi-block averaging block (B+1):  **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -qw B+1                                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -mba 1 1                                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -mba 2 2                                   **         \n" : "" );
    output << ( (          advanced ) ? "                  **      ...                                      **         \n" : "" );
    output << ( (          advanced ) ? "                  **    -mba B B                                   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 5. ML B+1 now calculates  variance and allows **         \n" : "" );
    output << ( (          advanced ) ? "                  **    (800) kernel transfer inc kernel variance. **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -boot           - pseudo-bootstrap ML by taking all (non-constrained)\n" : "" );
    output << ( (          advanced ) ? "                           training vectors (there  are m), randomly selecting\n" : "" );
    output << ( (          advanced ) ? "                           m with replacement, then constraining the rest.    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Performance estimation options (after bootstrap):                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** The result will be stored in u=var(1,1). **              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tQ             - Store the actual outputs in var(1,5), var(1,6).    \n" : "" );
    output << ( (          advanced ) ? "         -tnQ            - Don't store the actual output (default).           \n" : "" );
    output << ( (          advanced ) ? "         -tQx            - Log actual outputs to file (default).              \n" : "" );
    output << ( (          advanced ) ? "         -tnQx           - Don't log actual outputs.                          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** MEX: in mex you want to use -tnQx or it will be slow **  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tvar           - Save x variance to .var file when testing.         \n" : "" );
    output << ( (          advanced ) ? "         -tT  {0,1}      - Enable threads when cross-validating (default 0):  \n" : "" );
    output << ( (          advanced ) ? "                           0: disable.                                        \n" : "" );
    output << ( (          advanced ) ? "                           1: enable.                                         \n" : "" );
    output << ( (          advanced ) ? "                           Note that threads usually don't help here!         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tn  n          - Sets performance measure:                          \n" : "" );
    output << ( (          advanced ) ? "                           0: error (default)                                 \n" : "" );
    output << ( (          advanced ) ? "                           1: precision.                                      \n" : "" );
    output << ( (          advanced ) ? "                           2: recall.                                         \n" : "" );
    output << ( (          advanced ) ? "                           3: F1 score.                                       \n" : "" );
    output << ( (          advanced ) ? "                           4: AUC score.                                      \n" : "" );
    output << ( (          advanced ) ? "                           5: sparsity.                                       \n" : "" );
    output << ( (          advanced ) ? "         -tm  $fn        - Sets performance measurement function, based on:   \n" : "" );
    output << ( (          advanced ) ? "                           var(1,2)  = error (default - see below).           \n" : "" );
    output << ( (          advanced ) ? "                           var(1,3)  = count vector.                          \n" : "" );
    output << ( (          advanced ) ? "                           var(1,4)  = confusion matrix.                      \n" : "" );
    output << ( (          advanced ) ? "                           var(1,37) = accuracy   (1-error   for   classifier,\n" : "" );
    output << ( (          advanced ) ? "                                       1/(error+eps) for regressor).          \n" : "" );
    output << ( (          advanced ) ? "                           var(1,38) = precision (if binary classifier).      \n" : "" );
    output << ( (          advanced ) ? "                           var(1,39) = recall (if binary classifier).         \n" : "" );
    output << ( (          advanced ) ? "                           var(1,40) = F1 score (if binary classifier).       \n" : "" );
    output << ( (          advanced ) ? "                           var(1,41) = AUC score (if binary classifier).      \n" : "" );
    output << ( (          advanced ) ? "                           var(1,42) = sparsity of ML (0 dense -> 1 sparse).  \n" : "" );
    output << ( (          advanced ) ? "                           all other variables are described below.           \n" : "" );
    output << ( (          advanced ) ? "         -tM  $fn        - Like  -tm, but  this then  directly  evaluates  the\n" : "" );
    output << ( (          advanced ) ? "                           performance  measurement   function  straight  away\n" : "" );
    output << ( (          advanced ) ? "                           without  first  evaluating  performance.  That  is,\n" : "" );
    output << ( (          advanced ) ? "                           this  function can  load anything  into the  result\n" : "" );
    output << ( (          advanced ) ? "                           variable,  allowing   non-standard  usage   of  for\n" : "" );
    output << ( (          advanced ) ? "                           example using -g to minimise an arbitrary function.\n" : "" );
    output << ( (          advanced ) ? "         -tU  $fn $pmpt  - Evaluate $fn and store in var(1,1), evaluate $pmpt,\n" : "" );
    output << ( (          advanced ) ? "                           prompt the user with $pmpt (var(1,1)),  ask user to\n" : "" );
    output << ( (          advanced ) ? "                           give a final result (which may be a function).     \n" : "" );
    output << ( (          advanced ) ? "         -tMd d r $fn    - Like -tM,  but  evaluates  L2 distance  between  r,\n" : "" );
    output << ( (          advanced ) ? "                           which must be  an RKHS Vector, and $fn,  which must\n" : "" );
    output << ( (          advanced ) ? "                           be  a   function  of   var(0,0),   var(0,1),   ...,\n" : "" );
    output << ( (          advanced ) ? "                           var(0,d-1), where d is the dimension.              \n" : "" );
    output << ( (          advanced ) ? "         -tMD d $fn $fn  - Like  -tMd but  between two  functions, integrating\n" : "" );
    output << ( (          advanced ) ? "                           over [0,1]^d.                                      \n" : "" );
    output << ( (          advanced ) ? "         -tMv v          - Appends variance  to the result.  This  is used for\n" : "" );
    output << ( (          advanced ) ? "                           Bayesian optimisation  - it operates  by converting\n" : "" );
    output << ( (          advanced ) ? "                           the  result r  to a set  { r v }.  This  must  come\n" : "" );
    output << ( (          advanced ) ? "                           after the result is evaluated (eg after -tM).  Note\n" : "" );
    output << ( (          advanced ) ? "                           that -tC stores variance in var(1,46).             \n" : "" );
    output << ( (          advanced ) ? "         -tMx x          - Appends side-channel vect. x to result (see -gmsc).\n" : "" );
    output << ( (          advanced ) ? "                           Rhe result r to a set { r 0 x }. x must be a vector\n" : "" );
    output << ( (          advanced ) ? "                           for this to work.                                  \n" : "" );
    output << ( (          advanced ) ? "         -tMvx v x       - combine above, so result is { r v x }.             \n" : "" );
    output << ( (          advanced ) ? "                           Rhe result r to a set { r 0 x }. x must be a vector\n" : "" );
    output << ( (          advanced ) ? "         -tMMv $fn v     - combines -tM and -tMv.                             \n" : "" );
    output << ( (          advanced ) ? "         -tMMx $fn x     - combines -tM and -tMx.                             \n" : "" );
    output << ( (          advanced ) ? "         -tMMvx $fn v x  - combines -tM and -tMvw.                            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tMpy   fname x - evaluate python script fname with scalar x.*       \n" : "" );
    output << ( (          advanced ) ? "         -tMpyv  fname x - evaluate python script fname with vector x.*       \n" : "" );
    output << ( (          advanced ) ? "         -tMpyf  fname x - evaluate python script fname with function x.*     \n" : "" );
    output << ( (          advanced ) ? "                           *executes \"python3  fname x > pyres.txt\"  and grabs\n" : "" );
    output << ( (          advanced ) ? "                            result from pyres.txt.                            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tMexe  fname x - evaluate executable fname with scalar x.           \n" : "" );
    output << ( (          advanced ) ? "         -tMexev fname x - evaluate executable fname with vector x.           \n" : "" );
    output << ( (          advanced ) ? "         -tMexef fname x - evaluate executable fname with function x.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -tl             - Calculate  negative  (of) log-likelihood.  This  is\n" : "" );
    output << ( ( basic || advanced ) ? "                           well defined for  the GPR and formally  defined (by\n" : "" );
    output << ( ( basic || advanced ) ? "                           analogy) for SVM and LSV.                          \n" : "" );
    output << ( ( basic || advanced ) ? "         -tmg            - Calculate max information gain.  See -tl notes.    \n" : "" );
    output << ( ( basic || advanced ) ? "         -ta             - Calculate the RKHS norm.  See -tl notes.           \n" : "" );
    output << ( ( basic || advanced ) ? "         -tx             - Calculate  leave-one-out error.  In  the regression\n" : "" );
    output << ( ( basic || advanced ) ? "                           case the result is the RMS leave-one-error.        \n" : "" );
    output << ( ( basic || advanced ) ? "         -tr             - Calculate recall error.  In the regression case the\n" : "" );
    output << ( ( basic || advanced ) ? "                           result is the RMS recall error.                    \n" : "" );
    output << ( ( basic || advanced ) ? "         -tc  n          - Calculate the n-fold cross-validation error (in the\n" : "" );
    output << ( ( basic || advanced ) ? "                           regression case RMS cross-fold error).             \n" : "" );
    output << ( ( basic || advanced ) ? "         -tC  m n        - Calculate the n-fold cross-validation error (in the\n" : "" );
    output << ( ( basic || advanced ) ? "                           regression   case  RMS   cross-fold   error)   with\n" : "" );
    output << ( ( basic || advanced ) ? "                           randomisation (ie. shuffle points before starting).\n" : "" );
    output << ( ( basic || advanced ) ? "                           This variant will repeat m times and average result\n" : "" );
    output << ( ( basic || advanced ) ? "                           over all runs.  Note  that the result of  this will\n" : "" );
    output << ( ( basic || advanced ) ? "                           vary from run to run due to randomisation.         \n" : "" );
    output << ( ( basic || advanced ) ? "         -tf  $file      - validate ML using file, result sent to file.res.   \n" : "" );
    output << ( ( basic || advanced ) ? "         -tF  i j $file  - validate ML  using file,  result sent  to file.res,\n" : "" );
    output << ( ( basic || advanced ) ? "                           ignoring  i vectors  at start,  testing  at  most j\n" : "" );
    output << ( ( basic || advanced ) ? "                           vectors (-1 if all).                               \n" : "" );
    output << ( ( basic || advanced ) ? "         -tb i j m v     - test performance of ML  with added \"noise\" features\n" : "" );
    output << ( ( basic || advanced ) ? "                           in x vectors.  Tests range  from i->j such features\n" : "" );
    output << ( ( basic || advanced ) ? "                           where each feature is gaussian random N(m,v).      \n" : "" );
    output << ( ( basic || advanced ) ? "         -tV  [d] [[x]]  - test training vectors (see -AV).                   \n" : "" );
    output << ( (          advanced ) ? "         -tg  N d f v    - generate and test training data. N pairs generated,\n" : "" );
    output << ( (          advanced ) ? "                           vectors have dim d, function is f with noise var v,\n" : "" );
    output << ( (          advanced ) ? "                           features N(0,1).                                   \n" : "" );
    output << ( (          advanced ) ? "         -tG  N d f v    - generate and test training data. N pairs generated,\n" : "" );
    output << ( (          advanced ) ? "                           vectors have dim d, function is f with noise var v,\n" : "" );
    output << ( (          advanced ) ? "                           features U(0,1).                                   \n" : "" );
    output << ( (          advanced ) ? "         -tgc N d f v c  - like -tg, but only uses vectors x for which c(x)=1.\n" : "" );
    output << ( (          advanced ) ? "         -tGc N d f v c  - like -tG, but only uses vectors x for which c(x)=1.\n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** If a test stalls,  it may be  useful to reset **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the ML prior to performing each optimisation. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Use the z suffix to do this.  Conversely, use **         \n" : "" );
    output << ( (          advanced ) ? "                  ** the f suffix  to force  no-reset even for MLs **         \n" : "" );
    output << ( (          advanced ) ? "                  ** where this might affect the actual error.     **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** To  obtain raw ML  outputs for recdiv  tests, **         \n" : "" );
    output << ( (          advanced ) ? "                  ** use the B suffix.                             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** To test only on first cross batch use Z.      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Other suffixes for -tf and -tF as per -AA.    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** The complete list of suffixed options are:    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tx, -txz, -txB, -txzB                        **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tr, -trz, -trB, -trzB                        **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tc, -tcz, -tcB, -tczB                        **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tC, -tCz, -tCB, -tCzB                        **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tfe,-tfi,-tfl,-tfu,-tfel,-tfeu,-tfil,-tfiu,  **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tFe,-tFi,-tFr,-tFl,-tFu,-tFel,-tFeu,-tFil,   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tFiu,-tFrl,-tFru                             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tfeB,-tfiB,-tflB,-tfuB,-tfelB,-tfeuB,-tfilB,-tfiuB, **  \n" : "" );
    output << ( (          advanced ) ? "                  ** -tFeB,-tFiB,-tFrB,-tFlB,-tFuB,-tFelB,-tFeuB,-tFilB,  **  \n" : "" );
    output << ( (          advanced ) ? "                  ** -tFiuB,-tFrlB,-tFruB                          **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tV  (plus I/R variants)                      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Single class:                                 **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -tfl, -tFl etc: these  will assume base truth **         \n" : "" );
    output << ( (          advanced ) ? "                  ** labels are present in  training file and test **         \n" : "" );
    output << ( (          advanced ) ? "                  ** against these.                                **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** GP Pareto:                                    **         \n" : "" );
    output << ( (          advanced ) ? "                  ** GP Pareto  acts as  a classifier  (interior / **         \n" : "" );
    output << ( (          advanced ) ? "                  ** exterior).  Hence  if test file does not have **         \n" : "" );
    output << ( (          advanced ) ? "                  ** class information use -tfl filename 1.        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Logfiles written:                             **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** *.method.cfm: confusion matrix.               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** *.method.cnt: vector of numbers in each class.**         \n" : "" );
    output << ( (          advanced ) ? "                  ** *.method.clr: error for each class.           **         \n" : "" );
    output << ( (          advanced ) ? "                  ** *.method.res: actual output g(x) for vectors. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** *.method.cla: classification h(g(x)) " ".     **         \n" : "" );
    output << ( (          advanced ) ? "                  ** *.method.gra: classification gradients.       **         \n" : "" );
    output << ( (          advanced ) ? "                  ** *.method.sum: results summary, containing:    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **   Accuracy: this is 1-error for classifier or **         \n" : "" );
    output << ( (          advanced ) ? "                  **             1/(RMSerror+1e-6) for regressor.  **         \n" : "" );
    output << ( (          advanced ) ? "                  **   Precision: binary only, otherwise -1.       **         \n" : "" );
    output << ( (          advanced ) ? "                  **   Recall: binary only, otherwise -1.          **         \n" : "" );
    output << ( (          advanced ) ? "                  **   F1 score: binary only, otherwise -1.        **         \n" : "" );
    output << ( (          advanced ) ? "                  **   AUC: binary only, otherwise -1.             **         \n" : "" );
    output << ( (          advanced ) ? "                  **   Sparsity: meaning depends on ML.            **         \n" : "" );
    output << ( (          advanced ) ? "                  **   Error: classification error for classifier, **         \n" : "" );
    output << ( (          advanced ) ? "                  **             RMSerror for regressor, see above.**         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Note:- * is the base logname set by -L or -LL **         \n" : "" );
    output << ( (          advanced ) ? "                  **      - method is LOO for -tx, recall for -tr, **         \n" : "" );
    output << ( (          advanced ) ? "                  **        nfoldcross for -tc,  filename for -tf, **         \n" : "" );
    output << ( (          advanced ) ? "                  **        and test for -tV (and -tW below).      **         \n" : "" );
    output << ( (          advanced ) ? "                  **      - for -tc and  -tC a suffix  is added to **         \n" : "" );
    output << ( (          advanced ) ? "                  **        .res, .cla and .gra  to indicate which **         \n" : "" );
    output << ( (          advanced ) ? "                  **        repetition output  refers to (eg .res0 **         \n" : "" );
    output << ( (          advanced ) ? "                  **        is the  output for the first  round of **         \n" : "" );
    output << ( (          advanced ) ? "                  **        tests run, .res1 for the second etc).  **         \n" : "" );
    output << ( (          advanced ) ? "                  **      - if  MEX  is  present  these  are  also **         \n" : "" );
    output << ( (          advanced ) ? "                  **        written to  Matlab variables  with the **         \n" : "" );
    output << ( (          advanced ) ? "                  **        same  name,  except that  all .'s  are **         \n" : "" );
    output << ( (          advanced ) ? "                  **        replaced by _'s.                       **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- MEX (Matlab) only options                     --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -tW  $yvar $xvar -test training vectors (see -AW)                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Reporting options (after performance estimation):                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -s   $file      - write ML to $file.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -a   $file      - write alpha vector to $file.                       \n" : "" );
    output << ( ( basic || advanced ) ? "         -b   $file      - write bias value to $file.                         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** -a and -b also write to mex if present        **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -echo $fn       - echoes $fn to cerr and summary file.               \n" : "" );
    output << ( (          advanced ) ? "         -ECHO $fn       - echoes  $fn (evaluated and  finalised) to cerr  and\n" : "" );
    output << ( (          advanced ) ? "                           summary file.                                      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -plotfn xmin xmax ymin ymax fn xvar file format - plot fn  over range\n" : "" );
    output << ( (          advanced ) ? "                           and put in file with desired format:               \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           xvar: 0,1,2,3,4,5,6 for x,y,z,v,w,g,h respectively.\n" : "" );
    output << ( (          advanced ) ? "                           file: filename                                     \n" : "" );
    output << ( (          advanced ) ? "                           format: 0 for text, 1 ps, 2 pdf, 3 mex.            \n" : "" );
    output << ( (          advanced ) ? "                           ymin>ymax: select auto y range                     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -surffn xmin xmax ymin ymax zmin zmax fn xvar yvar file format - surf\n" : "" );
    output << ( (          advanced ) ? "                           plot fn  over range  and put  in file  with desired\n" : "" );
    output << ( (          advanced ) ? "                           format:                                            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                           xvar: 0,1,2,3,4,5,6 for x,y,z,v,w,g,h respectively.\n" : "" );
    output << ( (          advanced ) ? "                           file: filename                                     \n" : "" );
    output << ( (          advanced ) ? "                           format: 0 for text, 1 ps, 2 pdf, 3 mex.            \n" : "" );
    output << ( (          advanced ) ? "                           ymin>ymax: select auto y range                     \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -K0             - evaluate K0().                                     \n" : "" );
    output << ( (          advanced ) ? "         -K1  [x]        - evaluate K1(x).                                    \n" : "" );
    output << ( (          advanced ) ? "         -K2  [x] [y]    - evaluate K2(x,y).                                  \n" : "" );
    output << ( (          advanced ) ? "         -K3  [x] [y] [u]- evaluate K3(x,y,u).                                \n" : "" );
    output << ( (          advanced ) ? "      -K4 [x] [y] [u] [v]- evaluate K4(x,y,u,v).                              \n" : "" );
    output << ( (          advanced ) ? "         -Km  m [x0] ... - evaluate Km(x0,...).                               \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -ak  [x]        - return [ K(x,x).K(xi,xi) - K(x,xi)^2 ]_i.          \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -hU  [x]        - test vector (see -AU) and echo result.             \n" : "" );
    output << ( (          advanced ) ? "         -hY  x          - test vector (see -AY) and echo result.             \n" : "" );
    output << ( (          advanced ) ? "         -hZ  x m        - like -hY but for ML m.                             \n" : "" );
    output << ( (          advanced ) ? "         -hV  [[x]]      - test vectors (see -AV) and echo results.           \n" : "" );
    output << ( (          advanced ) ? "         -hW  i          - test training vector i and echo result.            \n" : "" );
    output << ( (          advanced ) ? "         -hX             - test all training vectors and echo result.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -hhU  [x]       - test g(x)^2 (see -AU) and echo result.             \n" : "" );
    output << ( (          advanced ) ? "         -hhY  x         - test g(x)^2 (see -AY) and echo result.             \n" : "" );
    output << ( (          advanced ) ? "         -hhZ  x m       - like -hhY but for ML m.                            \n" : "" );
    output << ( (          advanced ) ? "         -hhV  [[x]]     - test g(x)^2s (see -AV) and echo results.           \n" : "" );
    output << ( (          advanced ) ? "         -hhW  i         - test training g(x)^2 i and echo result.            \n" : "" );
    output << ( (          advanced ) ? "         -hhX            - test all training g(x)^2s and echo result.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -hUe y [x]      - error test vector (see -AU) and echo result.       \n" : "" );
    output << ( (          advanced ) ? "         -hYe y x        - error test vector (see -AY) and echo result.       \n" : "" );
    output << ( (          advanced ) ? "         -hWe i          - error test training vector i and echo result.      \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -hUE y [x]      - error test grad vector (see -AU) and echo result.  \n" : "" );
    output << ( (          advanced ) ? "         -hYE y x        - error test grad vector (see -AY) and echo result.  \n" : "" );
    output << ( (          advanced ) ? "         -hWE i          - error test grad training vector i and echo result. \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -hUv [x]        - variance test vector (see -AU) and echo result.    \n" : "" );
    output << ( (          advanced ) ? "         -hYv x          - variance test vector (see -AY) and echo result.    \n" : "" );
    output << ( (          advanced ) ? "         -hZv x m        - like -hYv but for ML m.                            \n" : "" );
    output << ( (          advanced ) ? "         -hVv [[x]]      - variance test vectors (see -AV) and echo results.  \n" : "" );
    output << ( (          advanced ) ? "         -hWv i          - variance test training vector i and echo result.   \n" : "" );
    output << ( (          advanced ) ? "         -hXv            - variance test all training vectors and echo result.\n" : "" );
    output << ( (          advanced ) ? "         -hVV [[x]]      - covariance matrix version of -hVv                  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -hUc [x] [y]    - covariance test vectors and echo result.           \n" : "" );
    output << ( (          advanced ) ? "         -hYc x y        - covariance test vectors and echo result.           \n" : "" );
    output << ( (          advanced ) ? "         -hZc x y m      - covariance test vectors and echo result.           \n" : "" );
    output << ( (          advanced ) ? "         -hWc i j        - covariance test vectors and echo result.           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -hYvn x v       - noisy variance test vector and echo result.        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -hP [x] p mu B n- test Pr(1:p) stability, ||.||_n norm.              \n" : "" );
    output << ( (          advanced ) ? "         -hp [x] p mu B  - test Pr(1:p) stability, rotated ||.||_inf norm.    \n" : "" );
    output << ( (          advanced ) ? "         -hPi  i p mu B n- test Pr(1:p) stability, ||.||_n norm.              \n" : "" );
    output << ( (          advanced ) ? "         -hpi  i p mu B  - test Pr(1:p) stability, rotated ||.||_inf norm.    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** For -hU, -hV, -hW, -hX results are stored in: **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** var(1,8)  = result of evaluation.             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** var(1,9)  = classification (sgn).             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** var(1,10) = output of ML (not just sgn).      **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Gradients of  g, var and cov can  be accessed **         \n" : "" );
    output << ( (          advanced ) ? "                  ** (when available) using augmented format.  For **         \n" : "" );
    output << ( (          advanced ) ? "                  ** example -hU  [ x :::: 6:1  ] the  gradient of **         \n" : "" );
    output << ( (          advanced ) ? "                  ** g(x), and -hUv [ x :::: 6:2 ] the variance of **         \n" : "" );
    output << ( (          advanced ) ? "                  ** d^2/dx^2 g(x).                                **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -plot i l u     - graph g(x) for x(i) in [ l u ]. Note this works via\n" : "" );
    output << ( (          advanced ) ? "                           a system call to run  gnuplot (and eps2pdf, pdfcrop\n" : "" );
    output << ( (          advanced ) ? "                           if plot type is pdf).                              \n" : "" );
    output << ( (          advanced ) ? "         -surf i lx ux j ly uy - graph  g(x)  for  x(i) in [ lx ux ],  x(j) in\n" : "" );
    output << ( (          advanced ) ? "                           [ ly uy ].                                         \n" : "" );
    output << ( (          advanced ) ? "                           if plot type is pdf).                              \n" : "" );
    output << ( (          advanced ) ? "         -hpln name      - name of output of plot (default logfile.ps/pdf).   \n" : "" );
    output << ( (          advanced ) ? "         -hpld name      - name of datafile of plot (default logfile.pdat).   \n" : "" );
    output << ( (          advanced ) ? "         -hpls t         - version of g(x) plotted:                           \n" : "" );
    output << ( (          advanced ) ? "                           0: plot g(x) default.                              \n" : "" );
    output << ( (          advanced ) ? "                           1: plot g(x)^2 default.                            \n" : "" );
    output << ( (          advanced ) ? "         -hplt t         - type of plot:                                      \n" : "" );
    output << ( (          advanced ) ? "                           0: terminal.                                       \n" : "" );
    output << ( (          advanced ) ? "                           1: .ps file.                                       \n" : "" );
    output << ( (          advanced ) ? "                           2: .pdf file (default).                            \n" : "" );
    output << ( (          advanced ) ? "                           2: mex window.                                     \n" : "" );
    output << ( (          advanced ) ? "         -hplD t         - type of data plot:                                 \n" : "" );
    output << ( (          advanced ) ? "                           0: just x/y.                                       \n" : "" );
    output << ( (          advanced ) ? "                           1: include training samples (default).             \n" : "" );
    output << ( (          advanced ) ? "                           2: include baseline.                               \n" : "" );
    output << ( (          advanced ) ? "                           3: include baseline and training samples.          \n" : "" );
    output << ( (          advanced ) ? "         -hplv t         - type of ML:                                        \n" : "" );
    output << ( (          advanced ) ? "                           0: variance meaningless, so don't include.         \n" : "" );
    output << ( (          advanced ) ? "                           1: include variance in plot (default).             \n" : "" );
    output << ( (          advanced ) ? "         -hplb f         - baseline function, if plotted (default 0).         \n" : "" );
    output << ( (          advanced ) ? "         -hplm m         - set lower end of y range (dflt 1, auto if m>M).    \n" : "" );
    output << ( (          advanced ) ? "         -hplM M         - set upper end of y range (dflt 0, auto if m>M).    \n" : "" );
    output << ( (          advanced ) ? "         -hplx xt        - set sparse vector template for x.                  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  -- MEX (Matlab) only options                     --         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -hM  $dst v     - Set MATLAB variable $dst = v (evaluated).          \n" : "" );
    output << ( (          advanced ) ? "         -hN  n $dst     - Set  var(1,n) =  MATLAB variable  $dst.  If type is\n" : "" );
    output << ( (          advanced ) ? "                           ambiguous favour existing type of var(1,n).        \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Multipass and input switching options (asynchronous):                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Zx             - Multiple training  passes are  possible by  putting\n" : "" );
    output << ( ( basic || advanced ) ? "                           commands into blocks,  separated by -Zx.  The first\n" : "" );
    output << ( ( basic || advanced ) ? "                           block starts  things going,  and subsequent  blocks\n" : "" );
    output << ( ( basic || advanced ) ? "                           can  adjust  parameters,  add/remove  points,   run\n" : "" );
    output << ( ( basic || advanced ) ? "                           various  tests etc.   Blocks are  separated  by the\n" : "" );
    output << ( ( basic || advanced ) ? "                           -Zx  flag.  Note  that a  separate logfile  will be\n" : "" );
    output << ( ( basic || advanced ) ? "                           written for each block and may overwrite.          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Zmute          - mute standard error.                               \n" : "" );
    output << ( ( basic || advanced ) ? "         -ZMute          - mute standard out.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "         -ZMUTE          - mute standard error and standard out.              \n" : "" );
    output << ( ( basic || advanced ) ? "         -Zunmute        - un-mute standard error.                            \n" : "" );
    output << ( ( basic || advanced ) ? "         -ZunMute        - un-mute standard out.                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -ZunMUTE        - un-mute standard error and standard out.           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Zs  s          - seed random number generator  with seed s.  To seed\n" : "" );
    output << ( (          advanced ) ? "                           with time use -Zs time.                            \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Zinteract      - start interactive (god) mode operation.            \n" : "" );
    output << ( (          advanced ) ? "         -Zgod           - enable entry to god-mode during optimisation if key\n" : "" );
    output << ( (          advanced ) ? "                           pressed (default).                                 \n" : "" );
    output << ( (          advanced ) ? "         -Zdawkins       - disable god-mode during optimisation.              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Zif b args     - if condition b is true (non-zero) then process args\n" : "" );
    output << ( (          advanced ) ? "                           args should be enclosed in {}.                     \n" : "" );
    output << ( (          advanced ) ? "         -Zifelse b t f  - if condition b is true (non-zero) then process args\n" : "" );
    output << ( (          advanced ) ? "                           t, otherwise process args f.                       \n" : "" );
    output << ( (          advanced ) ? "         -Zwhile b args  - while condition b is true (non-zero), process args \n" : "" );
    output << ( (          advanced ) ? "         -Zrep i args    - process args i times (count var(0,1000)=0,1,...).  \n" : "" );
    output << ( (          advanced ) ? "         -Zwait b        - pause until conditions b become true (non-zero).   \n" : "" );
    output << ( (          advanced ) ? "         -Zusleep n      - Sleep for n usec (accuracy is system dependent).   \n" : "" );
    output << ( (          advanced ) ? "         -Zmsleep n      - Sleep for n msec (accuracy is system dependent).   \n" : "" );
    output << ( (          advanced ) ? "         -Zsleep  n      - Sleep for n sec  (accuracy is system dependent).   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Zw  $file      - process args from $file (push).                    \n" : "" );
    output << ( (          advanced ) ? "         -Zk             - process args from cin (push).                      \n" : "" );
    output << ( (          advanced ) ? "         -Zc  n          - process args from shared stream n.                 \n" : "" );
    output << ( (          advanced ) ? "         -Zwf $file $of  - process args from $file, feedback to $of (push).   \n" : "" );
    output << ( (          advanced ) ? "         -Zkf            - process args from cin, feedback cout (\").          \n" : "" );
    output << ( (          advanced ) ? "         -Zaw $file      - pop first, then -Zw $file.                         \n" : "" );
    output << ( (          advanced ) ? "         -Zak            - pop first, then -Zk.                               \n" : "" );
    output << ( (          advanced ) ? "         -Zac n          - pop first, then -Zc.                               \n" : "" );
    output << ( (          advanced ) ? "         -Zawf $file $of - pop first, then -Zwf $file $of.                    \n" : "" );
    output << ( (          advanced ) ? "         -Zakf           - pop first, then -Zkf.                              \n" : "" );
    output << ( (          advanced ) ? "         -Za             - abandom input stream and return to previous (pop). \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Zcw n $pstring - push string onto shared stream n.                  \n" : "" );
    output << ( (          advanced ) ? "         -ZcW n arg      - push evaluated arg onto shared stream n.           \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** When svmheavy is  run, the command  line args **         \n" : "" );
    output << ( (          advanced ) ? "                  ** are placed in an istream  and the buffer gets **         \n" : "" );
    output << ( (          advanced ) ? "                  ** pushed onto the \"input  stack\".  The commands **         \n" : "" );
    output << ( (          advanced ) ? "                  ** -Zw,-Zk,-Zu create new istreams and push them **         \n" : "" );
    output << ( (          advanced ) ? "                  ** onto the same stack.  Input is taken from the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** istream on top of the stack until that stream **         \n" : "" );
    output << ( (          advanced ) ? "                  ** is exhausted,  at which point  it pops it off **         \n" : "" );
    output << ( (          advanced ) ? "                  ** and moves  to the  next one  down.  Once  the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** last  istream is  popped  off  the stack  the **         \n" : "" );
    output << ( (          advanced ) ? "                  ** program will exit.                            **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Feedback is a method  of passing results back **         \n" : "" );
    output << ( (          advanced ) ? "                  ** when  using UDP/TCP  streaming.  If selected, **         \n" : "" );
    output << ( (          advanced ) ? "                  ** some results/output  (eg -echo)  will be sent **         \n" : "" );
    output << ( (          advanced ) ? "                  ** back up  the stream.  These results  can also **         \n" : "" );
    output << ( (          advanced ) ? "                  ** be sent to files or cout.                     **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Shared  streams  (-Zc and  -Zac above)  allow **         \n" : "" );
    output << ( (          advanced ) ? "                  ** commands  to be  passed between  threads.  To **         \n" : "" );
    output << ( (          advanced ) ? "                  ** write a string  to a shared stream  that will **         \n" : "" );
    output << ( (          advanced ) ? "                  ** appear  as  input  to   whichever  thread  is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** accessing this stream (ie has -Zc or -Zac for **         \n" : "" );
    output << ( (          advanced ) ? "                  ** relevant  stream  number)  use -Zcw  (put raw **         \n" : "" );
    output << ( (          advanced ) ? "                  ** string into stream) or  -ZcW (evaluated).  To **         \n" : "" );
    output << ( (          advanced ) ? "                  ** pass variables use either -ZcW (note possible **         \n" : "" );
    output << ( (          advanced ) ? "                  ** loss  of  precision)  or   global  variables. **         \n" : "" );
    output << ( (          advanced ) ? "                  ** Notes:                                        **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - if stream  is empty  then thread  will wait **         \n" : "" );
    output << ( (          advanced ) ? "                  **   indefinitely  for something to  be put into **         \n" : "" );
    output << ( (          advanced ) ? "                  **   it.  Thus  to terminate  a stream  you must **         \n" : "" );
    output << ( (          advanced ) ? "                  **   end the stream with eg.:                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  **     -Zcw n \"-Zx -Za\"                          **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** - The operations -Zcw and -ZcW are atomic, so **         \n" : "" );
    output << ( (          advanced ) ? "                  **   you  can  have  multiple   threads  passing **         \n" : "" );
    output << ( (          advanced ) ? "                  **   commands (preferably  ending with -Zx) to a **         \n" : "" );
    output << ( (          advanced ) ? "                  **   single stream.                              **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Zf  $file      - create an empty file of given name.                \n" : "" );
    output << ( (          advanced ) ? "         -Zp  $file      - pause until $file exists.                          \n" : "" );
    output << ( (          advanced ) ? "         -Zff $file v    - create file and write v to it.                     \n" : "" );
    output << ( (          advanced ) ? "         -Zpp $file n    - pause until $file exists and read var(0,n) to it.  \n" : "" );
#ifdef ENABLE_THREADS
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Background  training  will  optimise  the  ML **         \n" : "" );
    output << ( (          advanced ) ? "                  ** while waiting for input  from the user.  Note **         \n" : "" );
    output << ( (          advanced ) ? "                  ** that  there is  no guarantee  that the  ML is **         \n" : "" );
    output << ( (          advanced ) ? "                  ** optimal  at  any given  time when  background **         \n" : "" );
    output << ( (          advanced ) ? "                  ** training is enabled - use fnA(h,14) to **         \n" : "" );
    output << ( (          advanced ) ? "                  ** see this information at any time.             **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "         -Zob            - turn background training off (default).            \n" : "" );
    output << ( (          advanced ) ? "         -ZoB            - turn background training on (stop on interupt).    \n" : "" );
    output << ( (          advanced ) ? "         -ZoBB           - turn background training on (train to completion). \n" : "" );
#endif
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         -ZZ             - End command sequence now, even if stream nonempty. \n" : "" );
    output << ( ( basic || advanced ) ? "         -ZZZZ           - Exit now.                                          \n" : "" );
    output << ( (          advanced ) ? "         -ZZif t         - Run -ZZ if t evaluates true (non-zero integer).    \n" : "" );
    output << ( (          advanced ) ? "         -ZZZZif t       - Run -ZZZZ if t evaluates true (non-zero integer).  \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                  ** Shortcuts:                                    **         \n" : "" );
    output << ( (          advanced ) ? "                  **                                               **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 1. -Zx can be replaced by ;                   **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 2. -ZZ can be replaced by end.                **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 3. -ZZZZ can be replaced by exit.             **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 4. -ZZif can be replaced by endif.            **         \n" : "" );
    output << ( (          advanced ) ? "                  ** 5. -ZZZZif can be replaced by exitif.         **         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Training examples                                                             \n" : "" );
    output << ( ( basic || advanced ) ? "=================                                                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Example 1: svmheavyv6 -c 1 -kt 3 -kg 20 -tc 5 -AA tr1s.txt                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "SV  classifier with  C/N = 1  (-c 1),  using  a gaussian  RBF kernel  function\n" : "" );
    output << ( ( basic || advanced ) ? "(-kt 2) with gamma = 20  (-kg 20).  Validation carried  out using 5-fold cross\n" : "" );
    output << ( ( basic || advanced ) ? "validation  (-tc 5).  Training data  is contained in  the file  tr1s.txt.  The\n" : "" );
    output << ( ( basic || advanced ) ? "model file will be saved to tr1s.txt.svm and the logfile to tr1s.txt.log.     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Eg 2: svmheavyv6 -c 1 -kt 3 -kg 20 -fo 1 tr1s.txt -ANr 0 \"idiv(2*var(0,1),3)\" \n" : "" );
    output << ( (          advanced ) ? "      -1 1 -tfi 1                                                             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Similar to  example 1, except  that in this  case the training  file is opened\n" : "" );
    output << ( (          advanced ) ? "using the -fo  flag first.  2/3 are  then randomly selected  from it using the\n" : "" );
    output << ( (          advanced ) ? "-ANr flag and the equation idiv(2*var(0,1),3) (the quotes may not be needed on\n" : "" );
    output << ( (          advanced ) ? "some OSes).  The remaining 1/3 is used for testing via the -tfi flag.         \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Eg 3: svmheavyv6 -c 1 -kt 3 -kg 20 -fo 1 tr1s.txt -ANr 0 \"idiv(2*var(0,1),3)\" \n" : "" );
    output << ( (          advanced ) ? "      -1 1 -g 2 \"-c var(0,1) -kg var(0,2) -tc 5\" \"-c var(0,1) -kg var(0,2)\"   \n" : "" );
    output << ( (          advanced ) ? "       fl 1 1e-1 1e1 5 fl 2 1 1e2 5 -tfi 1                                    \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Building on  example 2, this incorporates  a grid-search to select  C and g to\n" : "" );
    output << ( (          advanced ) ? "minimise 5-fold cross-validation  error, with C ranging 0.1 to 10 over 5 steps\n" : "" );
    output << ( (          advanced ) ? "and g ranging from 1 to 100 over 5 steps (log in both cases).                 \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Eg 4: svmheavyv6 -c 1 -kt 3 -kg 20 -tc 5 -AA tr1s.txt -Zx -L trx -c 2 -tc 5   \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );
    output << ( (          advanced ) ? "Similar to  example 1, but  with an  additional  block.  The  additional block\n" : "" );
    output << ( (          advanced ) ? "(ie. after the -Zx flag)  sets C/N = 2 (-c 2).  The model  file for the second\n" : "" );
    output << ( (          advanced ) ? "block will be  written to trx.svm  (-L trx), and  the logfile to  trx.log.  As\n" : "" );
    output << ( (          advanced ) ? "for the  first block,  validation of  the second  block is  done using  5-fold\n" : "" );
    output << ( (          advanced ) ? "cross-validation.                                                             \n" : "" );
    output << ( (          advanced ) ? "                                                                              \n" : "" );

    return;
}

void printhelpkernel(std::ostream &output, int basic, int advanced)
{
    output << ( ( basic || advanced ) ? "List of available kernel functions:                                           \n" : "" );
    output << ( ( basic || advanced ) ? "===================================                                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Numbering: 0-99    are intended for ML use (default 2).                       \n" : "" );
    output << ( ( basic || advanced ) ? "           100-299 are intended for NN use (default 201).                     \n" : "" );
    output << ( ( basic || advanced ) ? "           300-399 are intended for kNN use (default 300).                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "0   = Constant kernel: K(x,y) = r1                                            \n" : "" );
    output << ( ( basic || advanced ) ? "1   = Linear kernel: K(x,y) = <x,y>/(r0.r0)                                   \n" : "" );
    output << ( ( basic || advanced ) ? "2   = Polynomial kernel: K(x,y) = ( r1 + <x,y>/(r0.r0) )^i0                   \n" : "" );
    output << ( ( basic || advanced ) ? "3   = Gaussian kernel: K(x,y) = exp(-||x-y||^2/(2*r0*r0))                     \n" : "" );
    output << ( ( basic || advanced ) ? "4   = Laplacian kernel: K(x,y) = exp(-||x-y||/r0)                             \n" : "" );
    output << ( ( basic || advanced ) ? "5   = Polynoise kernel: K(x,y) = exp(-||x-y||^r1/(r1*r0^r1))                  \n" : "" );
    output << ( ( basic || advanced ) ? "6   = ANOVA kernel: K(x,y) = sum_k (-r4*((xk/r0)^r1-(yk/r0)^r1)^r2)^r3        \n" : "" );
    output << ( ( basic || advanced ) ? "7   = Sigmoid kernel (CPD): K(x,y) = tanh( <x,y>/(r0.r0) + r1 )               \n" : "" );
    output << ( ( basic || advanced ) ? "8   = Rational quadratic kernel: K(x,y) = 1-(||x-y||/r0)^2/((||x-y||/r0)^2+r1)\n" : "" );
    output << ( ( basic || advanced ) ? "9   = Multiquadric kernel (NM): K(x,y) = sqrt(||x-y||^2/(r0.r0)+r1^2)         \n" : "" );
    output << ( ( basic || advanced ) ? "10  = Inverse multiquadric kernel: K(x,y) = 1/sqrt(||x-y||^2/(r0.r0)+r1^2)    \n" : "" );
    output << ( ( basic || advanced ) ? "11  = Circular kernel (MR2): K(x,y) = 2/pi * ( arccos(-||x-y||/r0)            \n" : "" );
    output << ( ( basic || advanced ) ? "                                         - ||x-y||*sqrt(1-||x-y||^2/r0^2)/r0 )\n" : "" );
    output << ( ( basic || advanced ) ? "12  = Spherical kernel (MR3): K(x,y) = 1 - 1.5*||x-y||/r0 + 0.5*||x-y||^3/r0^3\n" : "" );
    output << ( ( basic || advanced ) ? "13  = Wave kernel: K(x,y) = (r0/||x-y||).sin(||x-y||/r0)                      \n" : "" );
    output << ( ( basic || advanced ) ? "14  = Power kernel: K(x,y) = -(||x-y||/r0)^r1                                 \n" : "" );
    output << ( ( basic || advanced ) ? "15  = Log kernel (CPD): K(x,y) = -log((||x-y||/r0)^r1 + 1)                    \n" : "" );
    output << ( ( basic || advanced ) ? "16  = Spline kernel: prod_k ( 1 + x_k.y_k + x_k.y_k.min(x_k,y_k) ...          \n" : "" );
    output << ( ( basic || advanced ) ? "                        - ((x_k+y_k).min(x_k,y_k)^2)/2 + (min(x_k,y_k)^3)/3 ) \n" : "" );
    output << ( ( basic || advanced ) ? "17  = B-Spline kernel: sum_k B_(2i0+1)(x_k-y_k)                               \n" : "" );
    output << ( ( basic || advanced ) ? "19  = Cauchy kernel: K(x,y) = 1/(1+((||x-y||^2/(r0.r0))))                     \n" : "" );
    output << ( ( basic || advanced ) ? "20  = Chi-Square kernel: K(x,y) = 1 - sum_k (2.xk.yk/(r0.r0))/(xk/r0+yk/r0)   \n" : "" );
    output << ( ( basic || advanced ) ? "21  = Histogram kernel: K(x,y) = sum_k min(xk,yk)                             \n" : "" );
    output << ( ( basic || advanced ) ? "22  = Generalised histogram kernel: K(x,y) = sum_k min(|xk|^r0,|yk|^r1)       \n" : "" );
    output << ( ( basic || advanced ) ? "23  = Generalised T-Student kernel: K(x,y) = 1/(1+||x-y||^r0)                 \n" : "" );
    output << ( ( basic || advanced ) ? "24  = Vovk's real polynomial: K(x,y)= (1-((<x,y>/(r0^2))^i0))/(1-<x,y>/(r0^2))\n" : "" );
    output << ( ( basic || advanced ) ? "25  = Weak fourier kernel: K(x,y) = pi.cosh(pi-(||x-y||/r0))                  \n" : "" );
    output << ( ( basic || advanced ) ? "26  = Thin spline (1): K(x,y) = ((||x-y||/r0)^(r1+0.5))                       \n" : "" );
    output << ( ( basic || advanced ) ? "27  = Thin spline (2): K(x,y) = ((||x-y||/r0)^r1)*ln(sqrt(||x-y||/r0))        \n" : "" );
    output << ( ( basic || advanced ) ? "28  = Generic kernel (CPD): K(x,y) = r10(varxy)                               \n" : "" );
    output << ( ( basic || advanced ) ? "29  = Arc-cosine kernel: K(x,y) = 1/pi ||x||^i0 ||y||^i0 J_i0(theta)          \n" : "" );
    output << ( ( basic || advanced ) ? "      where: theta = <x,y>/(||x||.||y||)                                      \n" : "" );
    output << ( ( basic || advanced ) ? "             Jn(t) = sin^(2n+1) (t) (-1/sin(t) d/dt)^n ((pi-t)/sin(t))        \n" : "" );
    output << ( ( basic || advanced ) ? "30  = Chaotic logistic kernel: K(x,y) = <phi_r0^i0(x),phi_r0^i0(y)>           \n" : "" );
    output << ( ( basic || advanced ) ? "      where: - phi_r0^i0(x) = phi_r0^{i0-1}(phi_r0^1(x))                      \n" : "" );
    output << ( ( basic || advanced ) ? "             - phi_r0^1 (x) = [ r0.x_0.(2-x_0) r0.x_1.(1-x_1) ... ]           \n" : "" );
    output << ( ( basic || advanced ) ? "             - phi_r0^0 (x) = x                                               \n" : "" );
    output << ( ( basic || advanced ) ? "             - x is assumed to be ranged from 0 to 1 for each element         \n" : "" );
    output << ( ( basic || advanced ) ? "             - x is actually pre-scaled to xi+r1/(1+2.r1) for each element    \n" : "" );
    output << ( ( basic || advanced ) ? "               where r1 is assumed small positive real.                       \n" : "" );
    output << ( ( basic || advanced ) ? "             - r0 should range from 0 to 1, where the chaotic threshold       \n" : "" );
    output << ( ( basic || advanced ) ? "               1.784975 and above (approx).  Be default, r0 = 1.8             \n" : "" );
    output << ( ( basic || advanced ) ? "31  = Summed Chaotic logistic kernel: K(x,y) = sum_i=0,i0 K30(i0=i;x,y)       \n" : "" );
    output << ( ( basic || advanced ) ? "32  = Diagonal offset kernel: r1 if diagonal Hessian, 0 otherwise             \n" : "" );
    output << ( ( basic || advanced ) ? "33  = Uniform kernel: K(x,y) = 1/2r0 if |||x-y||| < r0, 0 otherwise           \n" : "" );
    output << ( ( basic || advanced ) ? "34  = Triang kernel: K(x,y) = 1/r0 (1-|||x-y|||/r0) if |||x-y||| < r0, 0 other\n" : "" );
    output << ( ( basic || advanced ) ? "36  = Weiner kernel: prod_i min(x_i/r0,y_i/r0)                                \n" : "" );
    output << ( ( basic || advanced ) ? "37  = Half-Integer Matern kernel: of degree nu = i0+1/2, lengthscale r0       \n" : "" );
    output << ( ( basic || advanced ) ? "38  = 1/2-Matern kernel: K(x,y) = exp(-||x-y||/r0)                            \n" : "" );
    output << ( ( basic || advanced ) ? "39  = 3/2-Matern kernel: K(x,y) = (1+ sq(3)*||x-y||/r0).exp(-sq(3)*||x-y||/r0)\n" : "" );
    output << ( ( basic || advanced ) ? "40  = 5/2-Matern kernel: K = (1 + sqrt(5)*||x-y||/r0 + 5*||x-y||^2/r0^2).     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                      exp(-sqrt(5)*||x-y||/r0)\n" : "" );
    output << ( ( basic || advanced ) ? "41  = RBF rescale kernel: K(x,y) = z^(1/(2*r0*r0)) = exp(log(z)/(2*r0*r0))    \n" : "" );
    output << ( ( basic || advanced ) ? "42  = Inverse gudermannian kernel: K(x,y) = igd(<x,y>/(r0.r0))                \n" : "" );
    output << ( ( basic || advanced ) ? "43  = Log ratio kernel: K(x,y) = log((1+<x,y>/(r0.r0))/(1-<x,y>/(r0.r0)))     \n" : "" );
    output << ( ( basic || advanced ) ? "44  = Exponential kernel: K(x,y) = exp(<x,y>/(r0.r0))                         \n" : "" );
    output << ( ( basic || advanced ) ? "45  = Hyperbolic sine kernel: K(x,y) = sinh(<x,y>/(r0.r0))                    \n" : "" );
    output << ( ( basic || advanced ) ? "46  = Hyperbolic cosine kernel: K(x,y) = cosh(<x,y>/(r0.r0))                  \n" : "" );
    output << ( ( basic || advanced ) ? "47  = Sinc kernel: K(x,y) = sinc(||x-y||/r0).cos(2*pi*||x-y||/(r0.r1))        \n" : "" );
    output << ( ( basic || advanced ) ? "48  = LUT kernel: K(x,y) = r1((int) x, (int) y) if r1 is a matrix.            \n" : "" );
    output << ( ( basic || advanced ) ? "                         = r1 if (int) x != (int) y and r1 not a matrix.      \n" : "" );
    output << ( ( basic || advanced ) ? "                         = 1  if (int) x == (int) y and r1 not a matrix.      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "100 = Linear 0/1 neuron:    K(z) = z                                          \n" : "" );
    output << ( ( basic || advanced ) ? "101 = Logistic 0/1 neuron:  K(z) = 1/(1+exp(-r0.z))                           \n" : "" );
    output << ( ( basic || advanced ) ? "102 = Gen. logistic 0/1:    K(z) = 1/(1+r1.exp(-r0.r2.(z-r3)))^(1/r2)         \n" : "" );
    output << ( ( basic || advanced ) ? "103 = Heavyside 0/1 neuron: K(z) = 1 if real(z) > 0, 0 otherwise              \n" : "" );
    output << ( ( basic || advanced ) ? "104 = Rectifier 0/1 neuron: K(z) = z if real(z) > 0, 0 otherwise              \n" : "" );
    output << ( ( basic || advanced ) ? "105 = Softplus 0/1 neuron:  K(z) = ln(r1+exp(r0.z))                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "200 = Linear -1/+1 neuron:    K(z) = z-1                                      \n" : "" );
    output << ( ( basic || advanced ) ? "201 = Logistic -1/+1 neuron:  K(z) = 2/(1+exp(-r0.z)) -1                      \n" : "" );
    output << ( ( basic || advanced ) ? "202 = Gen. logistic -1/+1:    K(z) = 2/(1+r1.exp(-r0.r2.(z-r3)))^(1/r2) - 1   \n" : "" );
    output << ( ( basic || advanced ) ? "203 = Heavyside -1/+1 neuron: K(z) = 1   if real(z) > 0, -1 otherwise         \n" : "" );
    output << ( ( basic || advanced ) ? "204 = Rectifier -1/+1 neuron: K(z) = z-1 if real(z) > 0, -1 otherwise         \n" : "" );
    output << ( ( basic || advanced ) ? "205 = Softplus -1/+1 neuron:  K(z) = 2.ln(r1+exp(r0.z)) -1                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "300 = Euclidean distance: K(x,y) = -1/2 ||x-y||_2^2                           \n" : "" );
    output << ( ( basic || advanced ) ? "301 = 1-norm distance:    K(x,y) = -1/2 ||x-y||_1^2                           \n" : "" );
    output << ( ( basic || advanced ) ? "302 = inf-norm distance:  K(x,y) = -1/2 ||x-y||_inf^2                         \n" : "" );
    output << ( ( basic || advanced ) ? "303 = 0-norm distance:    K(x,y) = -1/2 ||x-y||_0^2                           \n" : "" );
    output << ( ( basic || advanced ) ? "304 = p-norm distance:    K(x,y) = -1/2 ||x-y||_r0^2                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "400 = Logistic (NM):       prod_k 1/(1+exp(-r0*(x_k-y_k)))                    \n" : "" );
    output << ( ( basic || advanced ) ? "401 = Error function (NM): prod_k (1+erf(r0*(x_k-y_k))/2                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "500 = Logist intg: prod_k r0.exp(-r0*(x_k-y_k))/((1+exp(-r0*(x_k-y_k)))^2)    \n" : "" );
    output << ( ( basic || advanced ) ? "501 = Error integ: (1/r0/sqrt(pi))^k exp(-r0*||x-y||^2)                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "600 = Logistic (NM):       prod_k 2/(1+exp(-r0*(x_k-y_k))) -1                 \n" : "" );
    output << ( ( basic || advanced ) ? "601 = Error function (NM): prod_k erf(r0*(x_k-y_k))                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "700 = Logist intg: pd_k 2.r0.exp(-r0*(x_k-y_k))/((1+exp(-r0*(x_k-y_k)))^2)    \n" : "" );
    output << ( ( basic || advanced ) ? "701 = Error integ: (2/r0/sqrt(pi))^k exp(-r0*||x-y||^2)                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "800 = Kernel transfer                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Key: NM  = non-mercer kernel                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "     MRx = positive definite only in R^x                                      \n" : "" );
    output << ( ( basic || advanced ) ? "     CPD = conditionally positive definite                                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Notes: - kernels 300-399 defined so K(x,x) + K(y,y) - 2K(x,y) = ||x-y||_...^2 \n" : "" );
    output << ( ( basic || advanced ) ? "       - kernel 4xx is the dense integral of kernel 5xx                       \n" : "" );
    output << ( ( basic || advanced ) ? "         (ie K4xx = int_x0 int_x1 ... K5xx)                                   \n" : "" );
    output << ( ( basic || advanced ) ? "       - kernel 5xx is the dense derivative of kernel 4xx                     \n" : "" );
    output << ( ( basic || advanced ) ? "         (ie K5xx = d/dx0 d/dx1 ... K4xx)                                     \n" : "" );
    output << ( ( basic || advanced ) ? "       - kernel 6xx is the dense integral of kernel 7xx                       \n" : "" );
    output << ( ( basic || advanced ) ? "         (ie K6xx = int_x0 int_x1 ... K7xx)                                   \n" : "" );
    output << ( ( basic || advanced ) ? "       - kernel 7xx is the dense derivative of kernel 6xx                     \n" : "" );
    output << ( ( basic || advanced ) ? "         (ie K7xx = d/dx0 d/dx1 ... K6xx)                                     \n" : "" );
    output << ( ( basic || advanced ) ? "       - For kernel 28:                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(0,0) = m                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(0,1) = x'y                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(0,2) = conj(x)'conj(y)                                        \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(0,3) = ||x-y||^2                                              \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(0,4) = ||x||^2                                                \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(0,5) = ||y||^2                                                \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(1,0) = ri (as set, not evaluated, including r10)              \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(2,i) = Ki (evaluated only if var(2,i) explicitly included)    \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(3,.) = x  (evaluated only if var(3,.) explicitly included)    \n" : "" );
    output << ( ( basic || advanced ) ? "          varxy(4,.) = y  (evaluated only if var(4,.) explicitly included)    \n" : "" );
    output << ( ( basic || advanced ) ? "       - kernel 28 is slow to evaluate: consider adding kernel to code instead\n" : "" );
    output << ( ( basic || advanced ) ? "       - not all kernels are (conditionally) positive definite (C)PD.         \n" : "" );
    output << ( ( basic || advanced ) ? "       - ' indicates conjugate transpose                                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - ||x||^2   = conj(x)'.x      (not the norm if (hyper-)complex)        \n" : "" );
    output << ( ( basic || advanced ) ? "       - ||x-y||^2 = (x-y').(x-y') = ||x||^2 + ||y||^2 - 2<x,y>               \n" : "" );
    output << ( ( basic || advanced ) ? "       - <x,y> = ( x'y + conj(x)'conj(y) )/2                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - for the m-norm (m>=4):                                               \n" : "" );
    output << ( ( basic || advanced ) ? "                  <x,y>     -> <<xa,xb,xc,...>>_m                             \n" : "" );
    output << ( ( basic || advanced ) ? "                  ||x-y||^2 -> <<xa,xa,xa,...>>_m + <<xb,xb,xb,...>>_m +      \n" : "" );
    output << ( ( basic || advanced ) ? "                             + <<xc,xc,xc,...>>_m + ... - m.<<xa,xb,xc,...>>_m\n" : "" );
    output << ( ( basic || advanced ) ? "                  ||x||^2   -> <<xa,xa,xa,xa>>_m                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  ||y||^2   -> <<xb,xb,xb,xb>>_m if m == 4                    \n" : "" );
    output << ( ( basic || advanced ) ? "                               <<xa,xa,xa,xa>>_m if m == 6,8,...              \n" : "" );
    output << ( ( basic || advanced ) ? "       - m-norms and m-inner products do not mix well with (hyper-) complex   \n" : "" );
    output << ( ( basic || advanced ) ? "         values.  No conjugation is done for m-norms and m-inner products, so \n" : "" );
    output << ( ( basic || advanced ) ? "         results are unlikely to satisfy any relevant extensions in the       \n" : "" );
    output << ( ( basic || advanced ) ? "         m-norm (hyper-)complex case.                                         \n" : "" );
    output << ( ( basic || advanced ) ? "       - m-norms and m-inner products on symbolic features do procucts        \n" : "" );
    output << ( ( basic || advanced ) ? "         pairwise, so eg \"a\".\"a\".\"b\".\"b\" = (\"a\".\"a\").(\"b\".\"b\") = 1.           \n" : "" );
    output << ( ( basic || advanced ) ? "       - kernels 6,16,17,20,21,22 actually work directly on x,y rather than   \n" : "" );
    output << ( ( basic || advanced ) ? "         via the norms and inner products and so no extension to m-norms and  \n" : "" );
    output << ( ( basic || advanced ) ? "         m-inner products is defined in this case.  Also note that the inv,   \n" : "" );
    output << ( ( basic || advanced ) ? "         max, min etc of symbolic features is ill-defined.                    \n" : "" );
    output << ( ( basic || advanced ) ? "       - the arc-cosine kernel for i0 >= 3 is calculated on the fly - v. slow.\n" : "" );
    output << ( ( basic || advanced ) ? "       - for neural kernels, z=<x,y>                                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );

    return;
}

void printhelpvars(std::ostream &output, int basic, int advanced)
{
    output << ( ( basic || advanced ) ? "Available constants                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "===================                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "The following vars may be used in variable evaluation (note that other vars   \n" : "" );
    output << ( ( basic || advanced ) ? "may also be defined, but these are typically transient and unreliable):       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "var(0,n)   = if set by -fo or -foe, this is the number of vectors remaining in\n" : "" );
    output << ( ( basic || advanced ) ? "             that file.  May also be set directly by -fV.                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,0)   = used during grid-search to store the distance from the optimal   \n" : "" );
    output << ( ( basic || advanced ) ? "             gridpoint to the grid centre.                                    \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,1)   = the result found by the most recent performance estimation -t....\n" : "" );
    output << ( ( basic || advanced ) ? "var(1,2)   = the unprocessed result of the most recent -t...                  \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,3)   = the unprocessed count vector of the most recent -t...            \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,4)   = the unprocessed confusion matrix of the most recent -t...        \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,5)   = result of most recent -t... in terms of h(g(x)).                 \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,6)   = result of most recent -t... in terms of g(x).                    \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,8)   = result of most recent -hU in terms of h(g(x)).                   \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,9)   = result of most recent -hU in terms of g(x).                      \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,11)  = filename of most recently added data.                            \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,12)  = log filename.                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,13)  = verbosity level.                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,14)  = name of ML file if loaded.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,15)  = name of alpha file if loaded.                                    \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,16)  = name of bias file if loaded.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,17)  = name of ML file if saved.                                        \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,18)  = name of alpha file if saved.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,19)  = name of bias file if saved.                                      \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,37)  = Accuracy of most recent performance estimation -t....            \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,38)  = Precision recorded by most recent performance estimation -t....  \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,39)  = Recall recorded by most recent performance estimation -t....     \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,40)  = F1 score recorded by most recent performance estimation -t....   \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,41)  = AUC recorded by most recent performance estimation -t....        \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,42)  = Sparsity recorded by most recent performance estimation -t....   \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,44)  = the unprocessed classwise error vector of the most recent -t...  \n" : "" );
    output << ( ( basic || advanced ) ? "var(1,46)  = variance of var(1,37) if most recent was repeated cross-fold.    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "var(0,1000)= counter for -Zrep (counts 0,1,...).                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "var(42,42) = current ML index.                                                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  ** var(1,0)-var(1,34) only defined if requested  **         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Global functions                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "================                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "In the  following we  use h (h = var(0,2) >= 0)  to access the  current ML. To\n" : "" );
    output << ( ( basic || advanced ) ? "access a  different ML replace  h with the  relevant  index.  Derivatives  are\n" : "" );
    output << ( ( basic || advanced ) ? "available as:                                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "dfnB(h,k,a,i)     = d^i/da^i fnB(h,k,a)                                       \n" : "" );
    output << ( ( basic || advanced ) ? "dfnC(h,k,a,i,b,j) = d^i/da^i d^j/db^j fnB(h,k,a,b)                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "See code for more information about obscure details.                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,0)       = C                                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1)       = epsilon                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,2)       = sigma (1/C)                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,3)       = beta rank (-mvb)                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,4)       = output (target) space dimension.                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,5)       = output (target) order.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,6)       = solution sparsity.                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,7)       = input space dimension                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8)       = is target sparse?                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9)       = is input sparse?                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,10)      = N                                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,11)      = ML type                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,12)      = ML subtype                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,13)      = number of classes                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,14)      = is ML trained?                                               \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,15)      = is ML mutable?                                               \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,16)      = is ML pool?                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,17)      = is ML target treated as scalar?                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,18)      = is ML target treated as vector?                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,19)      = is ML target treated as anion?                               \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,20)      = is ML classifier?                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,21)      = is ML regressor?                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,22)      = number of (internal) classes.                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,23)      = number of basis vectors.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,24)      = basis type.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,25)      = defProj().                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,26)      = g(x) output type.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,27)      = h(x) output type.                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,28)      = target type.                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,29)      = class labels (vector).                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,30)      = zero tolerance.                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,31)      = optimality tolerance.                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,32)      = max training time.                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,33)      = mv-rank learning rate.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,34)      = mv-rank zero tolerance.                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,35)      = kernel cache max size.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,36)      = max iteration count.                                         \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,37)      = mv-rank max iteration count.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,38)      = y vector.                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,39)      = d vector.                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,40)      = C weight vector.                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,41)      = epsilon weight vector.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,42)      = alpha state vector.                                          \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,43)      = C weight fuzzy vector.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,44)      = sigma weight vector.                                         \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,45)      = target basis.                                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,46)      = index key.                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,47)      = index key count.                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,48)      = data type key.                                               \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,49)      = data type key breakdown.                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,50)      = x (training vector set) sum.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,51)      = x (training vector set) mean.                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,52)      = x (training vector set) mean squared.                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,53)      = x (training vector set) squared sum.                         \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,54)      = x (training vector set) squared mean.                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,55)      = x (training vector set) median.                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,56)      = x (training vector set) variance.                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,57)      = x (training vector set) standard deviation.                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,58)      = x (training vector set) inverse standard deviation.          \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,59)      = x (training vector set) max.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,60)      = x (training vector set) min.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,61)      = NbasisVV.                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,62)      = basisTypeVV.                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,63)      = defProjVV.                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,65)      = VbasisVV.                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,66)      = training end time.                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,67)      = current ML as an RKHS vector.                                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,100,i)   = C weight for class i.                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,101,i)   = epsilon weight for class i.                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,102,i)   = d(i) (non-zero if this training vector enabled).             \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,103,i)   = number of unconstrained vectors in class i.                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,104,i)   = d(i) (non-zero if this training vector enabled).             \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,105,i)   = label associated with (internal) class i.                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,106,i)   = training vector i (in vector/set representation).            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,200,a,b) = distance between a and b, assuming a = g(xa), b = g(xb).     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,300,i)   = g(xi).                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,301,i)   = h(xi).                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,302,i)   = var(xi).                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,303,i)   = dg(xi)/dxi.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,304,i)   = dg(xi)/dxi weights wrt training set vectors.                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,305,i)   = dg(xi)/dxi weights wrt xi.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,306,i)   = s(xi) (stability probability).                               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,400,i,j) = cov(xi,xj).                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,401,i,j) = K2(xi,xj).                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,402,i,j) = <xi,xj>.                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,403,i,j) = ||xi-xj||_K2.                                                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,499)     = K in equational form.                                        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,500,x)   = g(x).#                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,501,x)   = h(x).#                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,502,x)   = var(x).#                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,503,x)   = dg(x)/dx.#                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,504,x)   = dg(x)/dx weights wrt training set vectors.#                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,505,x)   = dg(x)/dx weights wrt x.#                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,506,x)   = s(x) (stability probability).#                               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,600,x,z) = cov(x,z).#                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,601,x,z) = K2(x,z).#                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,602,x,z) = <x,z>.#                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,603,x,z) = ||x-z||_K2.#                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,701,m,x) = Km(x).##                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,8xx), fnB(h,8xx), fnC(h,8xx) all refer to the kernel K, function xx.    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9xx), fnB(h,9xx), fnC(h,9xx) like 7xx, but for the output kernel.       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "xx = 0,...,30: see mercer.cc MercerKernel::getparam function.                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x50,8,i) = weight of kernel function i.                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x51,8,i) = type of kernel function i.                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x52,8,i) = is kernel function i normalised?                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x53,8,i) = is kernel function i un-normalised?                          \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x54,8,i) = is kernel function i chained?                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x55,8,i) = is kernel function i un-chained?                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x56,8,i) = vector of real constants for kernel function i.              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x57,8,i) = vector of integer constants for kernel function i.           \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x60,8,i) = first real constant for kernel function i.                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,x61,8,i) = first integer constant for kernel function i.                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Notes: # x (and z) must be in non-sparse vector/set format. If x is a set then\n" : "" );
    output << ( ( basic || advanced ) ? "         it is converted as { s0 s1 s2 ... } -> [ s0 : s1 :: s2 :::: ... ]. If\n" : "" );
    output << ( ( basic || advanced ) ? "         nullptrs are included then these  components are skipped, so for example\n" : "" );
    output << ( ( basic || advanced ) ? "         {  [ 1 2 3 ] null  null  [ null  null null  null null  null 3 ]  } ->\n" : "" );
    output << ( ( basic || advanced ) ? "         [ 1 2 3 :::: 6:3 ] (that is, vector [ 1 2 3 ], want 3rd derivative). \n" : "" );
    output << ( ( basic || advanced ) ? "      ## For Km evaluation, x is a vector of m vectors (see # for format).    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- SVM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9000)    = NZ                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9001)    = NF                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9002)    = NS                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9003)    = NC                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9004)    = NLB                                                          \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9005)    = NLF                                                          \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9006)    = NUF                                                          \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9007)    = NUB                                                          \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9008)    = isLinearCost                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9009)    = isQuadraticCost                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9010)    = is1NormCost                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9011)    = isVarBias                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9012)    = isPosBias                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9013)    = isNegBias                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9014)    = isFixedBias                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9015)    = isOptActive                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9016)    = isOptSMO                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9017)    = isOptD2C                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9018)    = isOptGrad                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9019)    = isFixedTube                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9020)    = isShrinkTube                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9021)    = isRestrictEpsPos                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9022)    = isRestrictEpsNeg                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9023)    = isClassifyViaSVR                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9024)    = isClassifyViaSVM                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9025)    = is1vsA                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9026)    = is1vs1                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9027)    = isDAGSVM                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9028)    = isMOC                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9029)    = ismaxwins                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9030)    = isrecdiv                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9031)    = isatonce                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9032)    = isredbin                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9033)    = isKreal                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9034)    = isKunreal                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9035)    = isanomalyOn                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9036)    = isanomalyOff                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9037)    = isautosetOff                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9038)    = isautosetCscaled                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9039)    = isautosetCKmean                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9040)    = isautosetCKmedian                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9041)    = isautosetCNKmean                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9042)    = isautosetCNKmedian                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9043)    = isautosetLinBiasForce                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9044)    = outerlr                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9045)    = outermom                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9046)    = outermethod                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9047)    = outertol                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9048)    = outerovsc                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9049)    = outermaxitcnt                                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9050)    = outermaxcache                                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9051)    = maxiterfuzzt                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9052)    = usefuzzt                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9053)    = lrfuzzt                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9054)    = ztfuzzt                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9055)    = costfnfuzzt                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9056)    = m                                                            \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9057)    = LinBiasForce                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9058)    = QuadBiasForce                                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9059)    = nu                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9060)    = nuQuad                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9061)    = theta                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9062)    = simnorm                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9063)    = anomalyNu                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9064)    = anomalyClass                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9065)    = autosetCval                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9066)    = autosetnuval                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9067)    = anomclass                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9068)    = singmethod                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9069)    = rejectThreshold                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9070)    = Gp                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9071)    = XX                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9072)    = kerndiag                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9073)    = bias                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9074)    = alpha                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9075)    = quasiloglikelihood                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9076)    = weight vector for linear SVM.#                               \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,9077)    = bias for linear SVM.                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9100,i)  = NF(i)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9101,i)  = NZ(i)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9102,i)  = NS(i)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9103,i)  = NC(i)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9104,i)  = NLB(i)                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9105,i)  = NLF(i)                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9106,i)  = NUF(i)                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9107,i)  = NUB(i)                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9108,i)  = ClassRep()(i)                                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9109,i)  = findID(i)                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9110,i)  = getu()(i)                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9111,i)  = isVarBias(i)                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9112,i)  = isPosBias(i)                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9113,i)  = isNegBias(i)                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9114,i)  = isFixedBias(i)                                               \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9115,i)  = LinBiasForce(i)                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,9116,i)  = QuadBiasForce(i)                                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,9200,i,j)= Gp(i,j)                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(h,9201,i,j)= XX(i,j)                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- LSV specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,5000)    = isVardelta                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,5001)    = isZerodelta                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,5002)    = gamma                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,5003)    = delta                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,5004)    = LSV (quasi) log-likelihood.                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- GPR specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,2000)    = mu weight.                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,2001)    = mu bias.                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,2002)    = isZeromubias.                                                \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,2003)    = isVarmubias.                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,2004)    = isSampleMode.                                                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- MLM specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,6000)    = tsize.                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,6001)    = knum.                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,6004)    = mlmlr.                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,6005)    = diffstop.                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,6006)    = lsparse.                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,6100,i)  = regtype(i).                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,6101,i)  = regC(i).                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,6102,i)  = GGp(i).                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- KNN specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,4000)    = k.                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,4001)    = ktp.                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- IMP specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,3000)    = zref.                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,3001)    = EHI method.                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,3002)    = needdg.                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,3003)    = hypervolume dominated.                                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(h,3100,x)  = imp(x).#                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                  -- BLK specific options                          --         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1000)    = outfn.                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1001)    = outfngrad.                                                   \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1002)    = mex call.                                                    \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1003)    = mex call id.                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1004)    = mercer cache size.                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1005)    = mercer cache norm.                                           \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1006)    = system call.                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1007)    = Bernstein order.                                             \n" : "" );
    output << ( ( basic || advanced ) ? "fnA(h,1008)    = Bernstein index.                                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Global test functions                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "=====================                                                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "To access  test functions (as  per -fu  and -fuu)  we use fnB/C  with negative\n" : "" );
    output << ( ( basic || advanced ) ? "indexes:                                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnB(-1,i,x)    = evaluate fi(x) where fi is test function i (see -fu).        \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(-2,i,x,A)  = like fnB(-1,i,x) but with A matrix A (see -fuu).             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "fnC(-3,i,x,M)  = evaluate fi(x) for multi-objective test fn i (see -ft).      \n" : "" );

    return;
}


void printhelpgentype(std::ostream &output, int basic, int advanced)
{
    output << ( ( basic || advanced ) ? "Variable types and functions:                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "=============================                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "As well as  simple values  for arguments  you  can  enter equations,  refer to\n" : "" );
    output << ( ( basic || advanced ) ? "global variables and functions etc - for example sqrt(20) will evaluate to the\n" : "" );
    output << ( ( basic || advanced ) ? "square root  of 20, x^2 will  be evaluated  if x  is defined  (and left  as an\n" : "" );
    output << ( ( basic || advanced ) ? "equation otherwise) etc.                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "The following types are supported:                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Integers: 0, -1, 42 etc                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Reals: 1.0, 5e-3, 32.3E12 etc                                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - Complex: i, 1+3.2i, etc                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Quaternion: I, J, K, -21J etc                                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - Octonion: l, m, n, o, p, q, r, 12.3p etc                             \n" : "" );
    output << ( ( basic || advanced ) ? "       - Anion: im_anion(n,i), where 0<=i<2^n.                                \n" : "" );
    output << ( ( basic || advanced ) ? "       - Strings: a,d,'t',\"hello world\"                                       \n" : "" );
    output << ( ( basic || advanced ) ? "       - Vectors: eg. [ 1 -3.2 4I ... ]                                       \n" : "" );
    output << ( ( basic || advanced ) ? "       - Matrices: eg. M:[ 1 2 3 ; 4 5 6 ]                                    \n" : "" );
    output << ( ( basic || advanced ) ? "       - Sets: eg. { -2 14.3 \"chickens\" \"cats\" ... }                          \n" : "" );
    output << ( ( basic || advanced ) ? "       - Directed graphs: G:{ [ n1 n2 .. ] ;  M:[ w11 w12 .. ; w21 w22 .. ] },\n" : "" );
    output << ( ( basic || advanced ) ? "         where n1 n2 ... are nodes, w11 w12 ... are edge weights.             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "The following operators are expanded, in order, with given direction (e- means\n" : "" );
    output << ( ( basic || advanced ) ? "elementwise operation), and replaced with the functional form:                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Left to right: negation:           -a    -> neg(a)                   \n" : "" );
    output << ( ( basic || advanced ) ? "                        posation (null):    +a    -> pos(a)                   \n" : "" );
    output << ( ( basic || advanced ) ? "                        logical not:        ~a    -> lnot(a)                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - Right to left: power:              a^b   -> pow(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-power:            a.^b  -> epow(a,b)                \n" : "" );
    output << ( ( basic || advanced ) ? "       - Left to right: multiplication:     a*b   -> mul(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        division:           a/b   -> div(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        right division:     a\b   -> rdiv(a,b)                \n" : "" );
    output << ( ( basic || advanced ) ? "                        modulus:            a%b   -> mod(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-multiplication:   a.*b  -> emul(a,b)                \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-division:         a./b  -> ediv(a,b)                \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-right division:   a.\b  -> erdiv(a,b)               \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-modulus:          a.%b  -> emod(a,b)                \n" : "" );
    output << ( ( basic || advanced ) ? "       - Left to right: addition:           a+b   -> add(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        subtraction:        a-b   -> sub(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - Left to right: C-D construct:      a|b   -> cayleyDickson(a,b)       \n" : "" );
    output << ( ( basic || advanced ) ? "       - Left to right: equality:           a==b  -> eq(a,b)                  \n" : "" );
    output << ( ( basic || advanced ) ? "                        inequality:         a~=b  -> ne(a,b)                  \n" : "" );
    output << ( ( basic || advanced ) ? "                        greater than:       a>b   -> gt(a,b)                  \n" : "" );
    output << ( ( basic || advanced ) ? "                        not less than:      a>=b  -> ge(a,b)                  \n" : "" );
    output << ( ( basic || advanced ) ? "                        not greater than:   a<=b  -> le(a,b)                  \n" : "" );
    output << ( ( basic || advanced ) ? "                        less than:          a<b   -> lt(a,b)                  \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-equality:         a.==b -> eeq(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-inequality:       a.~=b -> ene(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-greater than:     a.>b  -> egt(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-not less than:    a.>=b -> ege(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-not greater than: a.<=b -> ele(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        e-less than:        a.<b  -> elt(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "       - Left to right: logical or:         a||b  -> lor(a,b)                 \n" : "" );
    output << ( ( basic || advanced ) ? "                        logical and:        a&&b  -> land(a,b)                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "where we note that:                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - The product of two vectors is the inner-product of the vectors.      \n" : "" );
    output << ( ( basic || advanced ) ? "       - The product of two strings is 1 if they are identical, 0 otherwise.  \n" : "" );
    output << ( ( basic || advanced ) ? "       - The product of two sets is the number of elements in common.         \n" : "" );
    output << ( ( basic || advanced ) ? "       - The product of two directed graphs A = { An ; Aw }, B = { Bn ; Bw} is\n" : "" );
    output << ( ( basic || advanced ) ? "         A*B = { Cn ; Cw }. where:                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         Cn = [ [An0,Bn0], [An0,Bn1], ..., [An1,Bn0], [An1,Bn1], ... ]        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "         Cw = [ Aw00*Bw00  Aw00*Bw01  ...  Aw01*Bw00  Aw01*Bw01  ... ]        \n" : "" );
    output << ( ( basic || advanced ) ? "              [ Aw00*Bw10  Aw00*Bw11  ...  Aw01*Bw10  Aw01*Bw11  ... ]        \n" : "" );
    output << ( ( basic || advanced ) ? "              [  ...        ...             ...        ...           ]        \n" : "" );
    output << ( ( basic || advanced ) ? "              [ Aw10*Bw00  Aw10*Bw01  ...  Aw11*Bw00  Aw11*Bw01  ... ]        \n" : "" );
    output << ( ( basic || advanced ) ? "              [ Aw10*Bw10  Aw10*Bw11  ...  Aw11*Bw10  Aw11*Bw11  ... ]        \n" : "" );
    output << ( ( basic || advanced ) ? "              [  ...        ...             ...        ...           ]        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - The sum of two strings is the concatenation of those strings.        \n" : "" );
    output << ( ( basic || advanced ) ? "       - The sum of two sets is the union, the difference the intersection.   \n" : "" );
    output << ( ( basic || advanced ) ? "       - The sum of two directed graphs is undefined (returns type error).    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - The negation of a string is that string in reverse order.            \n" : "" );
    output << ( ( basic || advanced ) ? "       - The conjugate of a string is that string with reversed case.         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "and the following string expansions are completed:                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - pi -> pi()                                                           \n" : "" );
    output << ( ( basic || advanced ) ? "       - euler -> euler()                                                     \n" : "" );
    output << ( ( basic || advanced ) ? "       - x -> var(0,0)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - y -> var(0,1)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - z -> var(0,2)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - v -> var(0,3)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - w -> var(0,4)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - g -> var(0,5)                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "       - h -> var(42,42) (this is used to hold the index of the current ML).  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Scalar functions are a special type of  function that behave more like vectors\n" : "" );
    output << ( ( basic || advanced ) ? "and are written @(i,j,n):fn where fn is a function, (i,j) defines the variable\n" : "" );
    output << ( ( basic || advanced ) ? "in the function being treated as a (continuous) index ranging [0,1] and n sets\n" : "" );
    output << ( ( basic || advanced ) ? "the number of  steps used in various approximations.  If i,j are  vectors then\n" : "" );
    output << ( ( basic || advanced ) ? "multiple variables are treated as continuous indexes.                         \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "The result of the product of two scalar functions is the inner product:       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "@(if,jf,n):f*@(ig,jg,n):g = int_0^1 conj(f(var(if,jf)=x)).g(var(ig,jg)=x) dx  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "which is  approximated using  n steps.  The product  of a vector  and a scalar\n" : "" );
    output << ( ( basic || advanced ) ? "function is similar,  where the vector is  treated as a step  function ranging\n" : "" );
    output << ( ( basic || advanced ) ? "over [0,1].  The shorthand @():f assuming i,j = 0 and n = 100.                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "The following functions may be used in equations:                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Variables: var(i,j).                                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Unary operators: pos(a), neg(a), inv(a).                             \n" : "" );
    output << ( ( basic || advanced ) ? "       - Binary operators: add(a,b),  sub(a,b), mul(a,b),  div(a,b), idiv(a,b)\n" : "" );
    output << ( ( basic || advanced ) ? "         (integer division),  rdiv(a,b) (right division),  mod(a,b), pow(a,b),\n" : "" );
    output << ( ( basic || advanced ) ? "         powl(a,b) (left power), powr(a,b) (right power).  For non-commutative\n" : "" );
    output << ( ( basic || advanced ) ? "         variables  left power  is powl(a,b) =  exp(b*log(a)), right  power is\n" : "" );
    output << ( ( basic || advanced ) ? "         powr(a,b) = exp(log(a)*b), and pow(a,b) = (powl(a,b)+powr(a,b))/2.   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Comparative: eq(a,b), ne(a,b), gt(a,b), ge(a,b), le(a,b), lt(a,b).   \n" : "" );
    output << ( ( basic || advanced ) ? "       - Logical: lnot(a), lor(a,b), land(a,b).                               \n" : "" );
    output << ( ( basic || advanced ) ? "       - Ternary: ifthenelse(a,b,c).                                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Type   testing:    isnull(a),   isint(a),   isreal(a),    isanion(a),\n" : "" );
    output << ( ( basic || advanced ) ? "         isvector(a),   ismatrix(a),   isset(a),   isdgraph(a),   isstring(a),\n" : "" );
    output << ( ( basic || advanced ) ? "         iserror(a).                                                          \n" : "" );
    output << ( ( basic || advanced ) ? "       - Type information: size(a), numRows(a), numCols(a).                   \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Constants:  null(), pi(), euler(),  pinf(), ninf(), vnan(),  eye(i,j)\n" : "" );
    output << ( ( basic || advanced ) ? "         (i*j identity matrix).                                               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Operators: conj(a),  realDeriv(i,j,a) (derivative of  function a with\n" : "" );
    output << ( ( basic || advanced ) ? "         respect to var(i,j)).                                                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Vector constructs:  vect_const(n,x)  (vector of size n  with elements\n" : "" );
    output << ( ( basic || advanced ) ? "         x),  vect_unit(n,i) (vector  of size  n, all  zero except  element i,\n" : "" );
    output << ( ( basic || advanced ) ? "         which is 1), ivect(a,b,c) (equivalent of a:b:c in matlab).           \n" : "" );
    output << ( ( basic || advanced ) ? "       - Anion constructs: im_complex(a)  (1 if a = 0, i if a = 1), im_quat(a)\n" : "" );
    output << ( ( basic || advanced ) ? "         (1 if a = 0, I if a = 1, J if  a = 2, K if a = 3), im_octo(a) (1 if a\n" : "" );
    output << ( ( basic || advanced ) ? "         = 0, l if a = 1, m if a = 2, ..., r if l = 7).                       \n" : "" );
    output << ( ( basic || advanced ) ? "       - Cayley-dickson construction: cayleyDickson(a,b) (a|b).               \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Commutators    and    associators:     commutate(a,b)    ((ab-ba)/2),\n" : "" );
    output << ( ( basic || advanced ) ? "         associate(a,b,c)  ((a(bc)-(ab)c)/2), anticommutate(a,b)  ((ab+ba)/2),\n" : "" );
    output << ( ( basic || advanced ) ? "         antiassociate(a,b,c) ((a(bc)+(ab)c)/2).                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Algebraic constants:  eps_comm(n,q,r,s) (commutator  structure const,\n" : "" );
    output << ( ( basic || advanced ) ? "         order n, element q,r,s (0 real, 1,2,...  imag)), eps_assoc(n,q,r,s,t)\n" : "" );
    output << ( ( basic || advanced ) ? "         (associator  structure constant,  order n,  element q,r,s,t  (0 real,\n" : "" );
    output << ( ( basic || advanced ) ? "         1,2,... imag)).                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Combinatorics: kronDelta(a,b), diracDelta(a,b), perm(a,b), comb(a,b),\n" : "" );
    output << ( ( basic || advanced ) ? "         fact(a).                                                             \n" : "" );
    output << ( ( basic || advanced ) ? "       - Elementwise combinatorics: ekronDelta(a,b), ediracDelta(a,b).        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Magnitude and angle: abs1(a)  (||a||_1), abs2(a) (||a||_2), absp(a,p)\n" : "" );
    output << ( ( basic || advanced ) ? "         (||a||_p),  absinf(a)   (||a||_inf),  norm1(a)   (||a||_1),  norm2(a)\n" : "" );
    output << ( ( basic || advanced ) ? "         (||a||_2^2), normp(a) (||a||_p^p), angle(a) (a/||a||_2).             \n" : "" );
    output << ( ( basic || advanced ) ? "       - Real, imaginary,  argument: real(a), imagx(a)  (imaginary part of a),\n" : "" );
    output << ( ( basic || advanced ) ? "         imagd(a)  (unit  imaginary  part   of a,  imagx(a)/||a||_2),  imag(a)\n" : "" );
    output << ( ( basic || advanced ) ? "         (sign-corrected imaginary  magnitude of a),  argx(a) (imagx(log(a))),\n" : "" );
    output << ( ( basic || advanced ) ? "         argd(a) (imagd(log(a))), arg(a) (imag(log(a))).                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - Polar anions:  polarx(a,b)  (a*exp(b)),  polard(a,b,q)  (a*exp(b*q)),\n" : "" );
    output << ( ( basic || advanced ) ? "         polar(a,b,q) (a*exp(b*q)).                                           \n" : "" );
    output << ( ( basic || advanced ) ? "       - Sign: sgn(a).                                                        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Exponentials and similar: sqrt(a),  exp(a), tenup(a)  (10^a), log(a),\n" : "" );
    output << ( ( basic || advanced ) ? "         log10(a),  logb(a,b),  logbl(a,b),    logbr(a,b)  (here  logbl(a,b) =\n" : "" );
    output << ( ( basic || advanced ) ? "         log(a)*inv(log(b)),  logbr(a,b)   =  inv(log(b))*log(a),  logb(a,b) =\n" : "" );
    output << ( ( basic || advanced ) ? "         (logbl(a,b)+logbr(a,b))/2).                                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Trigonometric: sin(a), cos(a), tan(a), cosec(a), sec(a), cot(a).     \n" : "" );
    output << ( ( basic || advanced ) ? "       - Inverse trigonometric: asin(a), acos(a), atan(a), acosec(a), asec(a),\n" : "" );
    output << ( ( basic || advanced ) ? "         acot(a).                                                             \n" : "" );
    output << ( ( basic || advanced ) ? "       - Related: sinc(a), cosc(a), tanc(a).                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - Versed: vers(a), covers(a), hav(a).                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - Inverse versed: avers(a), acovers(a), ahav(a).                       \n" : "" );
    output << ( ( basic || advanced ) ? "       - External: excosec(a), exsec(a).                                      \n" : "" );
    output << ( ( basic || advanced ) ? "       - Inverse external: aexcosec(a), aexsec(a).                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Hyperbolic: sinh(a), cosh(a), tanh(a), cosech(a), sech(a), coth(a).  \n" : "" );
    output << ( ( basic || advanced ) ? "       - Inverse   hyperbolic:  asinh(a),   acosh(a),  atanh(a),   acosech(a),\n" : "" );
    output << ( ( basic || advanced ) ? "         asech(a), acoth(a).                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "       - Related: sinhc(a), coshc(a), tanhc(a).                               \n" : "" );
    output << ( ( basic || advanced ) ? "       - Versed: versh(a), coversh(a), havh(a).                               \n" : "" );
    output << ( ( basic || advanced ) ? "       - Inverse versed: aversh(a), acovrsh(a), ahavh(a).                     \n" : "" );
    output << ( ( basic || advanced ) ? "       - External: excosech(a), exsech(a).                                    \n" : "" );
    output << ( ( basic || advanced ) ? "       - Inverse external: aexcosech(a), aexsech(a).                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Miscellaneous:  sigm(a)  (sigmoid),  gd(a)  (gudermannian),  asigm(a)\n" : "" );
    output << ( ( basic || advanced ) ? "         (inverse sigmoid), agd(a) (inverse gudermannian).                    \n" : "" );
    output << ( ( basic || advanced ) ? "       - Distribution functions: normDistr(a), polyDistr(x,n).                \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "       - Special  functions:   gamma(a),  lngamma(a)   (log   gamma),   psi(a)\n" : "" );
    output << ( ( basic || advanced ) ? "         (digamma), psi_n(i,a) (polygamma of  degree i), gami(a,x) (incomplete\n" : "" );
    output << ( ( basic || advanced ) ? "         gamma), gamic(a,b) (inverse incomplete gamma function), erf(a) (error\n" : "" );
    output << ( ( basic || advanced ) ? "         function), erfc(a) (complementary  error function), dawson(a) (Dawson\n" : "" );
    output << ( ( basic || advanced ) ? "         function), zeta(a)  (Reimann zeta  function), lambertW(a)  (Lambert's\n" : "" );
    output << ( ( basic || advanced ) ? "         W  function,  main  branch  W0  (W>-1)),  lambertWx(a)  (Lambert's  W\n" : "" );
    output << ( ( basic || advanced ) ? "         function, lower branch W1 (W<-1).                                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "        - Type conversion: rint(a), ceil(a), floor(a).                        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "        - Vector  and  matrix:  outerProd(a,b),  fourProd(a,b,c,d),  trans(a),\n" : "" );
    output << ( ( basic || advanced ) ? "          det(a),  trace(a),  miner(a,i,j),  cofactor(a,i,j), adj(a),  max(a),\n" : "" );
    output << ( ( basic || advanced ) ? "          min(a), maxdiag(a), mindiag(a), argmax(a), argmin(a), argmaxdiag(a),\n" : "" );
    output << ( ( basic || advanced ) ? "          argmindiag(a),    allargmax(a),   allargmin(a),    allargmaxdiag(a),\n" : "" );
    output << ( ( basic || advanced ) ? "          allargmindiag(a),      maxabs(a),     minabs(a),      maxabsdiag(a),\n" : "" );
    output << ( ( basic || advanced ) ? "          minabsdiag(a),    argmaxabs(a),   argminabs(a),    argmaxabsdiag(a),\n" : "" );
    output << ( ( basic || advanced ) ? "          argminabsdiag(a),         allargmaxabs(a),          allargminabs(a),\n" : "" );
    output << ( ( basic || advanced ) ? "          allargmaxabsdiag(a), allargminabsdiag(a).                           \n" : "" );
    output << ( ( basic || advanced ) ? "        - Sums, products, means  etc:  sum(a),  prod(a),  mean(a),  median(a),\n" : "" );
    output << ( ( basic || advanced ) ? "          argmedian(a).                                                       \n" : "" );
    output << ( ( basic || advanced ) ? "        - Element reference: deref(a,x),  derefv(a,i) (element i of vector a),\n" : "" );
    output << ( ( basic || advanced ) ? "          derefm(a,i,j) (element i,j of matrix a), derefa(a,i) (anions).      \n" : "" );
    output << ( ( basic || advanced ) ? "        - Compound matrix/vector collapse: collapse(a).                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "        - External functions: fnA(i), fnB(i,a), fnC(i,a,b).                   \n" : "" );
    output << ( ( basic || advanced ) ? "        - External function derivatives: dfnB(i,j,a), dfnC(i,j,a,b).          \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "        - Draws from distributions: irand(i) (uniform random integer [0,i-1]),\n" : "" );
    output << ( ( basic || advanced ) ? "          urand(a,b)  (uniform  random  real on  [a,b]), grand(a,b)  (Gaussian\n" : "" );
    output << ( ( basic || advanced ) ? "          random from N(a,b)).  These include vector-valued draws.            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "        - Test functions:  testfn(i,a) (evaluate  test function  i with vector\n" : "" );
    output << ( ( basic || advanced ) ? "          a),  testfnA(i,a,A) (like  before, with matrix  A), partestfn(i,M,a)\n" : "" );
    output << ( ( basic || advanced ) ? "          (multi-objective test  function i, target dimension  M, evaluated on\n" : "" );
    output << ( ( basic || advanced ) ? "          a), partestfnA(i,M,a,alpha) (like before, with constant alpha).     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "+-------------------------------+---+------------------------+---------------+\n" : "" );
    output << ( ( basic || advanced ) ? "|  i: Function name             | d | Range                  | Minimum       |\n" : "" );
    output << ( ( basic || advanced ) ? "+-------------------------------+---+------------------------+---------------+\n" : "" );
    output << ( ( basic || advanced ) ? "|  1: Rastrigin function        | d | -5.12   <= x_i <= 5.12 | f(0)     = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "|  2: Ackley's function         | d | -5      <= x_i <= 5    | f(0)     = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "|  3: Sphere function           | d | -inf    <= x_i <= inf  | f(0)     = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "|  4: Rosenbrock function       | d | -inf    <= x_i <= inf  | f(1)     = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "|  5: Beale's function          | 2 | -4.5    <= x,y <= 4.5  | f(3,0.5) = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "|  6: Goldstein-Price function  | 2 | -2      <= x,y <= 2    | f(0,-1)  = 3  |\n" : "" );
    output << ( ( basic || advanced ) ? "|  7: Booth's function          | 2 | -10     <= x,y <= 10   | f(1,3)   = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "|  8: Bukin function N.6        | 2 | -15,-3  <= x,y <= -5,3 | f(-10,1) = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "|  9: Matyas function           | 2 | -10     <= x,y <= 10   | f(0,0)   = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 10: Levi function N.13        | 2 | -10     <= x,y <= 10   | f(1,1)   = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 11: Himmelblau's function     | 2 | -5      <= x,y <= 5    | f(s,t)   = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 12: Three-hump camel function | 2 | -5      <= x,y <= 5    | f(0,0)   = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 13: Easom function            | 2 | -100    <= x,y <= 100  | f(pi,pi) = -1 |\n" : "" );
    output << ( ( basic || advanced ) ? "| 14: Cross-in-tray function    | 2 | -10     <= x,y <= 10   | f(a,a)   = b  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 15: Eggholder function        | 2 | -512    <= x,y <= 512  | f(c,d)   = e  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 16: Holder table function     | 2 | -10     <= x,y <= 10   | f(f,f)   = g  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 17: McCormick function        | 2 | -1.5,-3 <= x,y <= 4,4  | f(h,j)   = k  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 18: Schaffer function N. 2    | 2 | -100    <= x,y <= 100  | f(0,0)   = 0  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 19: Schaffer function N. 4    | 2 | -100    <= x,y <= 100  | f(0,l)   = m  |\n" : "" );
    output << ( ( basic || advanced ) ? "| 20: Styblinski-Tang function  | d | -5      <= x_i <= 5    | q <= f(p) <= r|\n" : "" );
    output << ( ( basic || advanced ) ? "| 21: Stability test function 1 | 1 | 0       <= x   <= 1    | f(0.2)   = 1.3|\n" : "" );
    output << ( ( basic || advanced ) ? "|     (also has unstable max at |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "|     f(0.5) = 1.65 (2nd order) |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "|     and f(1) = 1.5 (1st))     |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "| 22: Stability test function 2 | 1 | 0       <= x   <= 1    | f(1)     = 4  |\n" : "" );
    output << ( ( basic || advanced ) ? "|     Sum  of   two  gaussians, |   |                        | f(0.5)   = 1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|     gamma  =   1/(5.sqrt(2)), |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "|     centres  at  1  (alpha 4) |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "|     and 0.5  (alpha  1).  Use |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "|     A = 0.2, B = 0.05, pmax=2 |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "| 23: Sum   of  RBFs   function | d | 0       <= x   <= 1    | depends       |\n" : "" );
    output << ( ( basic || advanced ) ? "|     sum_i a_{i,0} exp(-(||x-a_{i,2:...}||_2^2)/(2*a_{i,1}*a_{i,1}))        |\n" : "" );
    output << ( ( basic || advanced ) ? "| 10xx: function xx, normalised | d | -1      <= x   <= 1    | depends       |\n" : "" );
    output << ( ( basic || advanced ) ? "|     (nominally) so  -1<=x<=1, |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "|     0<=f(x)<=1                |   |                        |               |\n" : "" );
    output << ( ( basic || advanced ) ? "+-------------------------------+---+------------------------+---------------+\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           Stability test function:                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           f(x) = exp(-20*(x-0.2)*(x-0.2))                    \n" : "" );
    output << ( ( basic || advanced ) ? "                                + exp(-20*sqrt(0.00001+((x-0.5)*(x-0.5))))    \n" : "" );
    output << ( ( basic || advanced ) ? "                                + exp(2*(x-0.8))                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           Constants: a = +-1.34941                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                      b = -2.06261                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                      c = 512                                 \n" : "" );
    output << ( ( basic || advanced ) ? "                                      d = 404.2319                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                      e = -959.6407                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                      f = +-8.05502                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                      g = -19.2085                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                      h = -0.54719                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                      j = -1.54719                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                      k = -1.9133                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                      l = 1.25313                             \n" : "" );
    output << ( ( basic || advanced ) ? "                                      m = 0.292579                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                      p = -2.903534                           \n" : "" );
    output << ( ( basic || advanced ) ? "                                      q = -39.16617n                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                      r = -39.16616n                          \n" : "" );
    output << ( ( basic || advanced ) ? "                                      (s,t) = (3,2), (-2.805,3.131),          \n" : "" );
    output << ( ( basic || advanced ) ? "                                          (-3.779,-3.283), (3.584,-1.848)     \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "+-------------+---+-----+-----------------------------------------+----------+\n" : "" );
    output << ( ( basic || advanced ) ? "|  i: Fn name | n | M   | Function                                | Range    |\n" : "" );
    output << ( ( basic || advanced ) ? "+-------------+---+-----+-----------------------------------------+----------+\n" : "" );
    output << ( ( basic || advanced ) ? "|  1: DTLZ1   | n | <=n | [ x0...xM-2.(1+g(z))/2          ; ]     | 0<=x<=1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ x0...xM-3.(1-xM-2).(1+g(z))/2 ; ]     |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [   ...                         ; ]     |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ x0...(1-x1).(1+g(z))/2        ; ]     |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1-x0).(1+g(z))/2               ]     |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | g(z) = 100.( #(z) + sum_i ( (zi-0.5)^2  |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     |               - cos(20*pi*(zi-0.5)) ) ) |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|  2: DTLZ2   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x<=1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [   ...                               ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).s(x0)                      ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | g(z) = sum_i (zi-0.5)^2                 |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | s(x) = sin( x.pi/2 )                    |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | c(x) = cos( x.pi/2 )                    |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|  3: DTLZ3   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x<=1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [   ...                               ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).s(x0)                      ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | g(z) = 100.( #(z) + sum_i ( (zi-0.5)^2  |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     |               - cos(20*pi*(zi-0.5)) ) ) |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | s(x) = sin( x.pi/2 )                    |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | c(x) = cos( x.pi/2 )                    |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|  4: DTLZ4   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x1<=1 |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] | -5<=xi<=5|\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [   ...                               ] | 2<=i<=n  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).s(x0)                      ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | g(z) = sum_i (zi-0.5)^2                 |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | s(x) = sin( (x^alpha).pi/2 )            |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | c(x) = cos( (x^alpha).pi/2 )            |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|  5: DTLZ5   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x<=1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [   ...                               ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).s(x0)                      ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | g(z) = sum_i (zi-0.5)^2                 |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | s(x) = sin( theta.pi/2 )                |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | c(x) = cos( theta.pi/2 )                |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | theta = (pi/(4.(1+g(z)))).(1+2.g(z).xi) |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|  6: DTLZ6   | n | <=n | [ (1+g(z)).c(x0)....c(xM-2).c(xM-1) ; ] | 0<=x<=1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0)....c(xM-2).s(xM-1) ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [   ...                               ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).c(x0).s(x1)              ; ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).s(x0)                      ] |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | g(z) = sum_i zi^0.1                     |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | s(x) = sin( theta.pi/2 )                |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | c(x) = cos( theta.pi/2 )                |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | theta = (pi/(4.(1+g(z)))).(1+2.g(z).xi) |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|  7: DTLZ7   | n | <=n | [ x0                              ; ]   | 0<=x<=1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ x1                              ; ]   |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [   ...                             ]   |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ xM-2                            ; ]   |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (1+g(z)).h(f1,f2,...,fM-2,g(z))   ]   |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | g(z) = 1 + (9/#(z)) sum_i zi            |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | h = M-sum_i((fi/(1+g)).(1+sin(3pi.fi))) |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|  8: DTLZ8   | n | <n  | [ sum_ib^is xi ], i = 0,1,...,M-1       | 0<=x<=1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     |        ib = floor(i*n/M)-1              |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     |        is = floor((i+1)*n/M)-1          |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | **constraints are not implemented yet.  |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|  9: DTLZ9   | n | <n  | [ sum_ib^is xi^0.1 ], i = 0,1,...,M-1   | 0<=x<=1  |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | where: z = [ xM xM+1 ... xn-1 ]         |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     |        ib = floor(i*n/M)-1              |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     |        is = floor((i+1)*n/M)-1          |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | **constraints are not implemented yet.  |          |\n" : "" );
    output << ( ( basic || advanced ) ? "| 10: FON1    | n | 2   | [ 1-exp(-|| x - 1/sqrt(n) ||^2) ; ]     | -4<=x<=4 |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ 1-exp(-|| x + 1/sqrt(n) ||^2)   ]     |          |\n" : "" );
    output << ( ( basic || advanced ) ? "| 11: SCH1    | 1 | 2   | [ x^2     ; ]                           | free     |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (x-2)^2   ]                           |          |\n" : "" );
    output << ( ( basic || advanced ) ? "| 12: SCH2    | 1 | 2   | [ { -x     if     x <= 1 } ]            | -5<=x<=10|\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ { x-2    if 1 < x <= 3 } ]            |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ { 4-x    if 3 < x <= 4 } ]            |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ { x-4    if 4 < x      } ]            |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [                          ]            |          |\n" : "" );
    output << ( ( basic || advanced ) ? "|             |   |     | [ (x-5)^2                  ]            |          |\n" : "" );
    output << ( ( basic || advanced ) ? "+-------------+---+-----------------------------------------------+----------+\n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "                           SCH1: Schaffer, J. D.:  Some Experiments in Machine\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Learning   using  Vector   Evaluated  Genetic\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Algorithms - PhD Thesis, 1984.               \n" : "" );
    output << ( ( basic || advanced ) ? "                           DTLZ: Deb,   Kalyanmoy  and   Thiele,  Lothar   and\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Laumanns,   Marco    and   Zitzler,   Eckart:\n" : "" );
    output << ( ( basic || advanced ) ? "                                 \"Scalable  Test  Problems   for  Evolutionary\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Multiobjective Optimization\".                \n" : "" );
    output << ( ( basic || advanced ) ? "                           FON1: Fonseca,  C.  M.  and   Fleming,  P.  J.:  An\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Overview of Evolutionary Algorithms in Multi-\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Objective     Optimisation.      Evolutionary\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Multiobjective   Optimisation,    Theoretical\n" : "" );
    output << ( ( basic || advanced ) ? "                                 Advances and Applications, pg. 105-145, 2005.\n" : "" );
    output << ( ( basic || advanced ) ? "                                 (as re-interpretted in DTLZ for n-dim).      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Elementwise versions of  functions are indicated by a leading e (or  E for the\n" : "" );
    output << ( ( basic || advanced ) ? "conjugated version).  The following elementwise functions are defined:        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "einv, emul, ediv, eidiv, erdiv, emod,  epow, epowl, epowr, eeq, ene, egt, ege,\n" : "" );
    output << ( ( basic || advanced ) ? "ele, elt, eabs1, eabs2, eabsp, eabsinf,  enorm1, enorm2, enormp, eangle, efnB,\n" : "" );
    output << ( ( basic || advanced ) ? " efnC,edfnB,edfnC.                                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Conjugated versions of functions are  indicated by capitalisation of the first\n" : "" );
    output << ( ( basic || advanced ) ? "letter (eg Acosh(a) = conj(acosh(a))).  The following conjugated functions are\n" : "" );
    output << ( ( basic || advanced ) ? "defined:                                                                      \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "Var,  Pow,  Powl, Powr,  Epow,  Epowl, Epowr,  Im_complex,  Im_quat,  Im_octo,\n" : "" );
    output << ( ( basic || advanced ) ? "Im_anion, CayleyDickson,  Imagd, Argd,  Argx, Sqrt,  Log, Log10,  Logb, Logbl,\n" : "" );
    output << ( ( basic || advanced ) ? "Logbr,  Asin,  Acos, Acosec,  Asec, Avers,  Acovers, Ahav,  Aexcosec,  Aexsec,\n" : "" );
    output << ( ( basic || advanced ) ? "Acosh, Atanh, Asech, Acoth, Aversh, Ahavh, Aexsech, Asigm, Agd, PolyDistr.    \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "With regard to anionic extensions of  complex functions, these are extended to\n" : "" );
    output << ( ( basic || advanced ) ? "the general case by noting that the  set of numbers of the form R + q.I, where\n" : "" );
    output << ( ( basic || advanced ) ? "R and I are real and q  is an imaginary unit anion, are  isomorphic to complex\n" : "" );
    output << ( ( basic || advanced ) ? "numbers by simple  interchange of q and i (noting that q  commutes with reals,\n" : "" );
    output << ( ( basic || advanced ) ? "qq = -1, and the conjugate of q is -q).                                       \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "So, given a quaternion  a, write a = R + M, where  R is real and M is entirely\n" : "" );
    output << ( ( basic || advanced ) ? "imaginary (ie. R = real(a) and M = imagx(a)), and let I = abs2(M), q= angle(M)\n" : "" );
    output << ( ( basic || advanced ) ? "we may write a = R + q.I.   Then, if f is an  analytic complex valued function\n" : "" );
    output << ( ( basic || advanced ) ? "of a single complex variable, ie.:                                            \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "f(z) = f_R(real(z),imagx(z)) + i.f_I(real(z),imagx(z))                        \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "then, given that q is indistinguishable  from the complex imaginery element i,\n" : "" );
    output << ( ( basic || advanced ) ? "the natural extension of f to the anions is:                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "f(a) = f_R(R,I) + q.f_I(R,I)                                                  \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "which  is how  we  extend  most functions.  For functions  of  more  than  one\n" : "" );
    output << ( ( basic || advanced ) ? "variable, however (e.g.  logb), there are difficulties  due to the presence of\n" : "" );
    output << ( ( basic || advanced ) ? "multiple unit imaginary anions  which do not in general commute or multiply to\n" : "" );
    output << ( ( basic || advanced ) ? "give -1 (see above for implementation of these).                              \n" : "" );
    output << ( ( basic || advanced ) ? "                                                                              \n" : "" );
    output << ( ( basic || advanced ) ? "In  some cases  where functions  are well  defined for  the complex  case, for\n" : "" );
    output << ( ( basic || advanced ) ? "example acosh(-2),  we could define  an infinite number of  results, as we are\n" : "" );
    output << ( ( basic || advanced ) ? "going from real  to complex without  a well defined  unit anion q to  base our\n" : "" );
    output << ( ( basic || advanced ) ? "result.  If we assume  q is complex (and we do)  then q is defined  up to sign\n" : "" );
    output << ( ( basic || advanced ) ? "and the sign can be  taken care of.  If, however, we  don't so restrict q then\n" : "" );
    output << ( ( basic || advanced ) ? "we have an infinite family to chose from, and they all give valid answers.  To\n" : "" );
    output << ( ( basic || advanced ) ? "get around this we arbitrarily set q = i whenever ambiguity occurs (and q = -i\n" : "" );
    output << ( ( basic || advanced ) ? "if for conjugated versions - eg Acosh(-2) = conj(acosh(-2)).                  \n" : "" );

    return;
}
